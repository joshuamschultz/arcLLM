[provider]
api_format = "openai-chat"
base_url = "https://api.mistral.ai"
api_key_env = "MISTRAL_API_KEY"
api_key_required = true
default_model = "mistral-large-latest"
default_temperature = 0.7
vault_path = ""

[models.mistral-large-latest]
context_window = 128000
max_output_tokens = 8192
supports_tools = true
supports_vision = true
supports_thinking = false
input_modalities = ["text", "image"]
cost_input_per_1m = 2.00
cost_output_per_1m = 6.00
cost_cache_read_per_1m = 0.0
cost_cache_write_per_1m = 0.0

[models.mistral-small-latest]
context_window = 128000
max_output_tokens = 8192
supports_tools = true
supports_vision = true
supports_thinking = false
input_modalities = ["text", "image"]
cost_input_per_1m = 0.10
cost_output_per_1m = 0.30
cost_cache_read_per_1m = 0.0
cost_cache_write_per_1m = 0.0

[models.open-mistral-nemo]
context_window = 128000
max_output_tokens = 4096
supports_tools = true
supports_vision = false
supports_thinking = false
input_modalities = ["text"]
cost_input_per_1m = 0.15
cost_output_per_1m = 0.15
cost_cache_read_per_1m = 0.0
cost_cache_write_per_1m = 0.0

[models.codestral-latest]
context_window = 32000
max_output_tokens = 8192
supports_tools = false
supports_vision = false
supports_thinking = false
input_modalities = ["text"]
cost_input_per_1m = 0.30
cost_output_per_1m = 0.90
cost_cache_read_per_1m = 0.0
cost_cache_write_per_1m = 0.0
