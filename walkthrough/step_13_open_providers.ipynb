{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Step 13 — Open Model Providers\n",
    "\n",
    "**What we built**: 9 new provider adapters covering local inference (Ollama, vLLM), cloud inference (Together, Groq, Fireworks, DeepSeek, Mistral, HuggingFace), and self-hosted inference (HuggingFace TGI). Each gets a TOML config with model metadata and a thin adapter file.\n",
    "\n",
    "**Why it matters**: ArcLLM started with Anthropic and OpenAI. Production agents need model diversity — local models for air-gapped federal environments, fast inference (Groq) for latency-sensitive loops, cost-optimized endpoints (Together, Fireworks) for high-volume work, and frontier open models (DeepSeek, Mistral) for specialized tasks.\n",
    "\n",
    "**Key decisions**:\n",
    "- **D-100**: Thin alias adapters — 8 of 9 providers are OpenAI-compatible, so each adapter is ~10 lines inheriting `OpenaiAdapter` with just a `name` override\n",
    "- **D-101**: `api_key_required` flag in `ProviderSettings` — local providers (Ollama, vLLM) don't need API keys\n",
    "- **D-102**: All 10 providers in one step (Ollama, vLLM, Together, Groq, Fireworks, DeepSeek, Mistral, HuggingFace, HuggingFace TGI)\n",
    "- **D-103**: Common models pre-populated with metadata; unknown models use adapter defaults\n",
    "- **D-104**: Zero cost defaults for local providers (`cost_*_per_1m = 0.0`)\n",
    "- **D-105**: Mistral gets quirk overrides (tool_choice \"required\" → \"any\", extra stop reason \"model_length\")\n",
    "- **D-106**: Separate `huggingface` (cloud Inference API) and `huggingface_tgi` (self-hosted) providers\n",
    "\n",
    "**Key learning**: 8 of 9 providers are pure OpenAI-compatible aliases (~10 lines each). Only Mistral needed quirk overrides. The thin alias pattern proves the architecture's extensibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: ensure arcllm is importable\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Provider Landscape\n",
    "\n",
    "| Provider | Type | API Key | Base URL | Use Case |\n",
    "|----------|------|---------|----------|----------|\n",
    "| **Ollama** | Local | Optional | `http://localhost:11434` | Air-gapped, dev, privacy |\n",
    "| **vLLM** | Local | Optional | `http://localhost:8000` | High-perf GPU serving |\n",
    "| **Together** | Cloud | Required | `https://api.together.xyz` | Cost-optimized open models |\n",
    "| **Groq** | Cloud | Required | `https://api.groq.com/openai` | Ultra-fast inference |\n",
    "| **Fireworks** | Cloud | Required | `https://api.fireworks.ai/inference` | Fast + function calling |\n",
    "| **DeepSeek** | Cloud | Required | `https://api.deepseek.com` | Frontier reasoning (R1) |\n",
    "| **Mistral** | Cloud | Required | `https://api.mistral.ai` | EU sovereignty, vision |\n",
    "| **HuggingFace** | Cloud | Required | `https://api-inference.huggingface.co/models` | Model hub inference |\n",
    "| **HuggingFace TGI** | Self-hosted | Optional | `http://localhost:8080` | Self-hosted TGI server |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. The Thin Alias Pattern\n",
    "\n",
    "Since 8 of 9 providers use the OpenAI Chat Completions API format, each adapter is a minimal alias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from arcllm.adapters.ollama import OllamaAdapter\n",
    "from arcllm.adapters.groq import GroqAdapter\n",
    "from arcllm.adapters.together import TogetherAdapter\n",
    "from arcllm.adapters.deepseek import DeepseekAdapter\n",
    "from arcllm.adapters.fireworks import FireworksAdapter\n",
    "from arcllm.adapters.vllm import VllmAdapter\n",
    "from arcllm.adapters.huggingface import HuggingfaceAdapter\n",
    "from arcllm.adapters.huggingface_tgi import Huggingface_TgiAdapter\n",
    "\n",
    "# Show how small each adapter is\n",
    "for name, cls in [\n",
    "    (\"Ollama\", OllamaAdapter),\n",
    "    (\"Groq\", GroqAdapter),\n",
    "    (\"Together\", TogetherAdapter),\n",
    "    (\"DeepSeek\", DeepseekAdapter),\n",
    "    (\"Fireworks\", FireworksAdapter),\n",
    "    (\"vLLM\", VllmAdapter),\n",
    "    (\"HuggingFace\", HuggingfaceAdapter),\n",
    "    (\"HuggingFace TGI\", Huggingface_TgiAdapter),\n",
    "]:\n",
    "    source = inspect.getsource(cls)\n",
    "    lines = len(source.strip().split('\\n'))\n",
    "    print(f\"{name:20s} → {lines} lines (inherits OpenaiAdapter)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see a full adapter file\n",
    "print(\"=== OllamaAdapter (complete file) ===\")\n",
    "print(inspect.getsource(OllamaAdapter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "That's it. The `name` property override is the only difference — everything else (request building, response parsing, tool call handling) is inherited from `OpenaiAdapter`.\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "The OpenAI Chat Completions API has become the de facto standard. All these providers expose the same:\n",
    "- Endpoint: `POST /v1/chat/completions`\n",
    "- Auth: `Authorization: Bearer <key>` header\n",
    "- Request body: `{model, messages, tools, temperature, max_tokens}`\n",
    "- Response: `{choices[0].message, usage, model}`\n",
    "\n",
    "The TOML config handles the differences (base_url, api_key_env, model names, pricing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Convention-Based Registry (D-041, D-042)\n",
    "\n",
    "The registry discovers adapters by naming convention — no registration needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.registry import _get_adapter_class, clear_cache\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "# The convention: provider_name → arcllm.adapters.{name} → {Name.title()}Adapter\n",
    "providers = [\n",
    "    \"ollama\", \"vllm\", \"together\", \"groq\", \"fireworks\",\n",
    "    \"deepseek\", \"mistral\", \"huggingface\", \"huggingface_tgi\",\n",
    "]\n",
    "\n",
    "print(f\"{'Provider':20s} {'Module':40s} {'Class':30s}\")\n",
    "print(\"-\" * 90)\n",
    "for p in providers:\n",
    "    cls = _get_adapter_class(p)\n",
    "    print(f\"{p:20s} arcllm.adapters.{p:23s} {cls.__name__:30s}\")\n",
    "\n",
    "print(f\"\\nAll {len(providers)} providers discovered by convention. Zero registration code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "Note: `huggingface_tgi` → `\"huggingface_tgi\".title()` = `\"Huggingface_Tgi\"` → `Huggingface_TgiAdapter`. Valid Python class name, works with the convention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. `api_key_required` Flag (D-101)\n",
    "\n",
    "Local providers (Ollama, vLLM, HuggingFace TGI) don't need API keys. The `api_key_required` flag in TOML controls validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.config import load_provider_config\n",
    "\n",
    "# Show api_key_required for each provider\n",
    "print(f\"{'Provider':20s} {'api_key_required':20s} {'base_url'}\")\n",
    "print(\"-\" * 80)\n",
    "for p in providers:\n",
    "    config = load_provider_config(p)\n",
    "    print(f\"{p:20s} {str(config.provider.api_key_required):20s} {config.provider.base_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.exceptions import ArcLLMConfigError\n",
    "\n",
    "# Cloud provider without API key → error\n",
    "old_key = os.environ.pop(\"GROQ_API_KEY\", None)\n",
    "\n",
    "try:\n",
    "    config = load_provider_config(\"groq\")\n",
    "    GroqAdapter(config, \"llama-3.3-70b-versatile\")\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"Cloud without key: {e}\")\n",
    "\n",
    "if old_key:\n",
    "    os.environ[\"GROQ_API_KEY\"] = old_key\n",
    "\n",
    "# Local provider without API key → works fine\n",
    "os.environ.pop(\"OLLAMA_API_KEY\", None)  # Remove if set\n",
    "config = load_provider_config(\"ollama\")\n",
    "adapter = OllamaAdapter(config, \"llama3.2\")\n",
    "print(f\"\\nLocal without key: {adapter.name} ({adapter.model_name}) — no error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "The implementation in `BaseAdapter.__init__`:\n",
    "```python\n",
    "if config.provider.api_key_required and not api_key:\n",
    "    raise ArcLLMConfigError(f\"Missing environment variable '{env_var}' ...\")\n",
    "```\n",
    "\n",
    "And in `OpenaiAdapter._build_headers`:\n",
    "```python\n",
    "if self._api_key:  # Only adds Authorization header when key exists\n",
    "    headers[\"Authorization\"] = f\"Bearer {self._api_key}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Provider TOML Configs\n",
    "\n",
    "Each provider has a TOML file with connection settings and model metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show models per provider\n",
    "for p in providers:\n",
    "    config = load_provider_config(p)\n",
    "    models = list(config.models.keys())\n",
    "    print(f\"{p:20s} → {len(models)} models: {', '.join(models[:3])}{'...' if len(models) > 3 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed view: Ollama config (local, no auth, zero cost)\n",
    "config = load_provider_config(\"ollama\")\n",
    "\n",
    "print(\"=== Ollama Provider Settings ===\")\n",
    "print(f\"  base_url:         {config.provider.base_url}\")\n",
    "print(f\"  api_key_required: {config.provider.api_key_required}\")\n",
    "print(f\"  default_model:    {config.provider.default_model}\")\n",
    "\n",
    "print(\"\\n=== Ollama Models ===\")\n",
    "for name, meta in config.models.items():\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    context_window:  {meta.context_window:,}\")\n",
    "    print(f\"    supports_tools:  {meta.supports_tools}\")\n",
    "    print(f\"    cost_input:      ${meta.cost_input_per_1m}/1M tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq config (cloud, fast inference, has pricing)\n",
    "config = load_provider_config(\"groq\")\n",
    "\n",
    "print(\"=== Groq Provider Settings ===\")\n",
    "print(f\"  base_url:         {config.provider.base_url}\")\n",
    "print(f\"  api_key_required: {config.provider.api_key_required}\")\n",
    "print(f\"  default_model:    {config.provider.default_model}\")\n",
    "\n",
    "print(\"\\n=== Groq Models ===\")\n",
    "for name, meta in config.models.items():\n",
    "    print(f\"  {name}: ${meta.cost_input_per_1m}/{meta.cost_output_per_1m} per 1M in/out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### D-104: Zero Cost Defaults for Local Providers\n",
    "\n",
    "Local providers set `cost_*_per_1m = 0.0`. Orgs can override for GPU cost tracking. The telemetry module still logs token counts — just the dollar cost is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. TOML Gotcha — Dotted Model Names\n",
    "\n",
    "Model names with dots (e.g., `llama3.2`) must be quoted in TOML. Unquoted dots are parsed as nested tables.\n",
    "\n",
    "```toml\n",
    "# CORRECT — quoted string key\n",
    "[models.\"llama3.2\"]\n",
    "context_window = 128000\n",
    "\n",
    "# WRONG — parsed as models.llama3.2 (nested table)\n",
    "[models.llama3.2]\n",
    "```\n",
    "\n",
    "This caught us during implementation — all Ollama models with dots need quoted keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dotted model names parse correctly\n",
    "config = load_provider_config(\"ollama\")\n",
    "dotted_models = [m for m in config.models.keys() if \".\" in m]\n",
    "print(f\"Dotted model names in Ollama: {dotted_models}\")\n",
    "\n",
    "for model_name in dotted_models:\n",
    "    meta = config.models[model_name]\n",
    "    print(f\"  {model_name}: context={meta.context_window:,}, tools={meta.supports_tools}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Mistral Quirk Overrides (D-105)\n",
    "\n",
    "Mistral is the only provider that isn't a pure OpenAI alias. It has two quirks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.adapters.mistral import MistralAdapter, _MISTRAL_STOP_REASON_MAP\n",
    "\n",
    "print(\"=== Mistral Adapter (full source) ===\")\n",
    "print(inspect.getsource(MistralAdapter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Quirk 1: `tool_choice` Mapping\n",
    "\n",
    "```\n",
    "OpenAI:  tool_choice = \"required\"  → forces tool use\n",
    "Mistral: tool_choice = \"any\"       → same behavior, different keyword\n",
    "```\n",
    "\n",
    "The adapter translates `\"required\"` → `\"any\"` in `_build_request_body()`.\n",
    "\n",
    "### Quirk 2: Extra Stop Reason\n",
    "\n",
    "```\n",
    "OpenAI:  finish_reason = \"length\"         → max tokens hit\n",
    "Mistral: finish_reason = \"model_length\"   → same thing, different name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral stop reason mapping\n",
    "print(\"Mistral stop_reason mapping:\")\n",
    "for mistral_reason, arcllm_reason in _MISTRAL_STOP_REASON_MAP.items():\n",
    "    print(f\"  {mistral_reason:15s} → {arcllm_reason}\")\n",
    "print()\n",
    "\n",
    "# Compare with OpenAI's mapping\n",
    "from arcllm.adapters.openai import _STOP_REASON_MAP\n",
    "print(\"OpenAI stop_reason mapping:\")\n",
    "for openai_reason, arcllm_reason in _STOP_REASON_MAP.items():\n",
    "    print(f\"  {openai_reason:15s} → {arcllm_reason}\")\n",
    "\n",
    "print(\"\\nOnly difference: Mistral has 'model_length' (extra) and no 'content_filter'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate tool_choice translation\n",
    "os.environ.setdefault(\"MISTRAL_API_KEY\", \"test-key\")\n",
    "config = load_provider_config(\"mistral\")\n",
    "adapter = MistralAdapter(config, \"mistral-large-latest\")\n",
    "\n",
    "from arcllm.types import Message, Tool\n",
    "\n",
    "body = adapter._build_request_body(\n",
    "    messages=[Message(role=\"user\", content=\"test\")],\n",
    "    tools=[Tool(name=\"calc\", description=\"Calculator\", parameters={\"type\": \"object\"})],\n",
    "    tool_choice=\"required\",\n",
    ")\n",
    "\n",
    "print(f\"Input tool_choice: 'required'\")\n",
    "print(f\"Output tool_choice: '{body.get('tool_choice')}'\")\n",
    "print(\"Translated to Mistral's 'any' keyword\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. HuggingFace vs HuggingFace TGI (D-106)\n",
    "\n",
    "Two separate providers for different deployment models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_config = load_provider_config(\"huggingface\")\n",
    "tgi_config = load_provider_config(\"huggingface_tgi\")\n",
    "\n",
    "print(f\"{'':20s} {'HuggingFace (cloud)':30s} {'HuggingFace TGI (self-hosted)'}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'base_url':20s} {hf_config.provider.base_url:30s} {tgi_config.provider.base_url}\")\n",
    "print(f\"{'api_key_required':20s} {str(hf_config.provider.api_key_required):30s} {str(tgi_config.provider.api_key_required)}\")\n",
    "print(f\"{'api_key_env':20s} {hf_config.provider.api_key_env:30s} {tgi_config.provider.api_key_env}\")\n",
    "print(f\"{'default_model':20s} {hf_config.provider.default_model:30s} {tgi_config.provider.default_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "| Aspect | `huggingface` | `huggingface_tgi` |\n",
    "|--------|--------------|-------------------|\n",
    "| Hosting | HuggingFace cloud Inference API | Self-hosted TGI server |\n",
    "| Auth | Required (HF token) | Optional |\n",
    "| Base URL | `https://api-inference.huggingface.co/models` | `http://localhost:8080` |\n",
    "| Model names | HF hub format (`meta-llama/...`) | Whatever you loaded |\n",
    "| Use case | Quick prototyping, HF ecosystem | Production self-hosted |\n",
    "\n",
    "Both use the OpenAI-compatible API format, so both are thin aliases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. `load_model()` for All Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.registry import load_model, clear_cache\n",
    "\n",
    "# Set dummy keys for cloud providers\n",
    "for env_var in [\"TOGETHER_API_KEY\", \"GROQ_API_KEY\", \"FIREWORKS_API_KEY\",\n",
    "                \"DEEPSEEK_API_KEY\", \"MISTRAL_API_KEY\", \"HF_API_TOKEN\"]:\n",
    "    os.environ.setdefault(env_var, \"test-key\")\n",
    "\n",
    "print(f\"{'Provider':20s} {'Adapter Class':30s} {'Model':40s}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for provider in providers:\n",
    "    clear_cache()\n",
    "    model = load_model(provider)\n",
    "    print(f\"{provider:20s} {type(model).__name__:30s} {model.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a non-default model\n",
    "clear_cache()\n",
    "model = load_model(\"ollama\", model=\"deepseek-r1:8b\")\n",
    "print(f\"Provider: {model.name}\")\n",
    "print(f\"Model:    {model.model_name}\")\n",
    "print(f\"Thinking: {model._model_meta.supports_thinking}\")\n",
    "print(f\"Tools:    {model._model_meta.supports_tools}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full stack with modules on a local provider\n",
    "clear_cache()\n",
    "os.environ[\"ARCLLM_SIGNING_KEY\"] = \"test-key\"\n",
    "\n",
    "model = load_model(\n",
    "    \"ollama\",\n",
    "    telemetry=True,\n",
    "    audit=True,\n",
    "    security=True,\n",
    "    retry=True,\n",
    "    rate_limit=True,\n",
    ")\n",
    "\n",
    "layers = []\n",
    "layer = model\n",
    "while hasattr(layer, '_inner'):\n",
    "    layers.append(type(layer).__name__)\n",
    "    layer = layer._inner\n",
    "layers.append(type(layer).__name__)\n",
    "print(f\"Stack: {' → '.join(layers)}\")\n",
    "print(f\"\\nAll modules work with local providers — same wrapping, same API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Fallback Chains Across Providers\n",
    "\n",
    "With 11 providers available, fallback chains become powerful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example fallback chains for different use cases\n",
    "\n",
    "chains = {\n",
    "    \"Cost-optimized\": [\"groq\", \"together\", \"fireworks\"],\n",
    "    \"Air-gapped federal\": [\"ollama\", \"vllm\"],\n",
    "    \"Frontier reasoning\": [\"deepseek\", \"anthropic\", \"openai\"],\n",
    "    \"EU sovereignty\": [\"mistral\", \"huggingface\"],\n",
    "    \"General purpose\": [\"anthropic\", \"openai\", \"groq\"],\n",
    "}\n",
    "\n",
    "for name, chain in chains.items():\n",
    "    print(f\"{name:25s} → {' → '.join(chain)}\")\n",
    "\n",
    "print(\"\\nUsage: load_model('groq', fallback={'chain': ['together', 'fireworks']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Model Metadata — Capabilities and Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare pricing across providers for similar models\n",
    "print(\"=== Llama 3.x Model Pricing (per 1M tokens) ===\")\n",
    "print(f\"{'Provider':15s} {'Model':40s} {'Input':>8s} {'Output':>8s}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "comparisons = [\n",
    "    (\"ollama\", \"llama3.2\"),\n",
    "    (\"groq\", \"llama-3.3-70b-versatile\"),\n",
    "    (\"groq\", \"llama-3.1-8b-instant\"),\n",
    "    (\"together\", \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"),\n",
    "    (\"together\", \"meta-llama/Llama-3.1-8B-Instruct-Turbo\"),\n",
    "    (\"vllm\", \"meta-llama/Llama-3.1-8B-Instruct\"),\n",
    "]\n",
    "\n",
    "for provider, model_name in comparisons:\n",
    "    config = load_provider_config(provider)\n",
    "    meta = config.models.get(model_name)\n",
    "    if meta:\n",
    "        print(f\"{provider:15s} {model_name:40s} ${meta.cost_input_per_1m:>6.2f}  ${meta.cost_output_per_1m:>6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capability comparison\n",
    "print(\"=== Model Capabilities ===\")\n",
    "print(f\"{'Provider':15s} {'Model':30s} {'Tools':>6s} {'Vision':>7s} {'Think':>6s} {'Context':>10s}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "caps = [\n",
    "    (\"ollama\", \"llama3.2\"),\n",
    "    (\"ollama\", \"deepseek-r1:8b\"),\n",
    "    (\"groq\", \"llama-3.3-70b-versatile\"),\n",
    "    (\"mistral\", \"mistral-large-latest\"),\n",
    "    (\"mistral\", \"codestral-latest\"),\n",
    "    (\"together\", \"deepseek-ai/DeepSeek-V3\"),\n",
    "]\n",
    "\n",
    "for provider, model_name in caps:\n",
    "    config = load_provider_config(provider)\n",
    "    meta = config.models.get(model_name)\n",
    "    if meta:\n",
    "        print(f\"{provider:15s} {model_name:30s} {str(meta.supports_tools):>6s} {str(meta.supports_vision):>7s} {str(meta.supports_thinking):>6s} {meta.context_window:>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Unknown Models — Graceful Defaults (D-103)\n",
    "\n",
    "When a model isn't in the TOML, the adapter uses defaults from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an unknown model\n",
    "clear_cache()\n",
    "model = load_model(\"ollama\", model=\"some-new-model:latest\")\n",
    "\n",
    "print(f\"Provider: {model.name}\")\n",
    "print(f\"Model:    {model.model_name}\")\n",
    "print(f\"Metadata: {model._model_meta}\")\n",
    "print(\"\\nNo metadata = adapter uses defaults (4096 max_tokens, provider temperature).\")\n",
    "print(\"No error. Unknown models just work without metadata enrichment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Implementation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== BaseAdapter.__init__ (api_key_required check) ===\")\n",
    "from arcllm.adapters.base import BaseAdapter\n",
    "print(inspect.getsource(BaseAdapter.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== OpenaiAdapter._build_headers (conditional auth) ===\")\n",
    "from arcllm.adapters.openai import OpenaiAdapter\n",
    "print(inspect.getsource(OpenaiAdapter._build_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MistralAdapter (full with quirk overrides) ===\")\n",
    "print(inspect.getsource(MistralAdapter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Component | What | Why |\n",
    "|-----------|------|-----|\n",
    "| 8 thin alias adapters | ~10 lines each, inherit OpenaiAdapter | OpenAI-compatible API is the standard |\n",
    "| MistralAdapter | Quirk overrides for tool_choice + stop_reason | Only non-pure-alias provider |\n",
    "| 9 provider TOMLs | Connection settings + model metadata | Config-driven, no code changes |\n",
    "| `api_key_required` flag | Skips auth validation for local providers | Ollama/vLLM/TGI don't need keys |\n",
    "| Zero cost defaults | `cost_*_per_1m = 0.0` for local | Local inference has no API cost |\n",
    "| Dotted model names | Quoted keys in TOML (`\"llama3.2\"`) | TOML parsing gotcha |\n",
    "| Convention-based registry | `{name}` → `arcllm.adapters.{name}` → `{Name}Adapter` | Zero registration code |\n",
    "| Unknown model support | Adapter defaults when metadata missing | Graceful handling of new models |\n",
    "\n",
    "**Usage**:\n",
    "```python\n",
    "# Local (no API key needed)\n",
    "model = load_model(\"ollama\")                    # Default: llama3.2\n",
    "model = load_model(\"ollama\", model=\"mistral\")   # Specific model\n",
    "model = load_model(\"vllm\")                      # Self-hosted vLLM\n",
    "\n",
    "# Cloud (API key required)\n",
    "model = load_model(\"groq\")                      # Fast inference\n",
    "model = load_model(\"together\")                  # Cost-optimized\n",
    "model = load_model(\"fireworks\")                 # Fast + tools\n",
    "model = load_model(\"deepseek\")                  # Reasoning\n",
    "model = load_model(\"mistral\")                   # EU sovereignty\n",
    "model = load_model(\"huggingface\")               # HF Inference API\n",
    "model = load_model(\"huggingface_tgi\")           # Self-hosted TGI\n",
    "```\n",
    "\n",
    "**Test count**: 538 passing (83 new: 69 parametrized in `test_open_providers.py` + 14 in `test_mistral.py`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}