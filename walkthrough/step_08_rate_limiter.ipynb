{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 — Rate Limiter Module\n",
    "\n",
    "**What we built**: A `RateLimitModule` using the token bucket algorithm with per-provider shared state.\n",
    "\n",
    "**Why it matters**: Provider APIs enforce rate limits (requests per minute). Without client-side throttling, thousands of concurrent agents would blast the API, get 429'd, and waste time on retries. The rate limiter sits *innermost* in the module stack — throttling *before* the call goes out — so fallback and retry don't even trigger on rate-limit waits.\n",
    "\n",
    "**Key decisions**:\n",
    "- **D-055**: Token bucket algorithm (allows bursts, enforces average rate — battle-tested in nginx, AWS)\n",
    "- **D-056**: Per-provider shared buckets (rate limits are per API key, not per model)\n",
    "- **D-058**: Async wait + WARNING log (transparent to agents, no error raised)\n",
    "- **D-060**: Innermost position: `Retry(Fallback(RateLimit(adapter)))`\n",
    "\n",
    "**Stack position**: `Telemetry → Retry → Fallback → RateLimit → Adapter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: ensure arcllm is importable\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Token Bucket Algorithm\n",
    "\n",
    "The token bucket is a classic rate-limiting algorithm:\n",
    "\n",
    "1. **Bucket starts full** — `capacity` tokens available\n",
    "2. **Each request consumes 1 token** — if available, proceed immediately\n",
    "3. **If empty, wait** — sleep until a token refills\n",
    "4. **Tokens refill continuously** — at `refill_rate` tokens/second, capped at `capacity`\n",
    "\n",
    "```\n",
    "capacity=5, refill_rate=1/sec:\n",
    "\n",
    "t=0: [●●●●●]  5 tokens (full)\n",
    "t=0: [●●●●○]  4 tokens (1 request consumed)\n",
    "t=0: [●●●○○]  3 tokens (burst: 2nd request, no wait)\n",
    "t=0: [●●○○○]  2 tokens (burst: 3rd request)\n",
    "t=1: [●●●○○]  3 tokens (1 refilled after 1 second)\n",
    "t=1: [●●○○○]  2 tokens (4th request)\n",
    "```\n",
    "\n",
    "**Why token bucket vs. other algorithms?**\n",
    "- Sliding window: complex, needs timestamp storage per request\n",
    "- Fixed window: allows 2x burst at window boundary\n",
    "- Leaky bucket: no burst ability — every request waits\n",
    "- **Token bucket: simple counter + timestamp, allows bursts, enforces average rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Constructing a TokenBucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.modules.rate_limit import TokenBucket\n",
    "\n",
    "# 60 RPM = 1 request/second refill rate, bucket holds 60 tokens\n",
    "bucket = TokenBucket(capacity=60, refill_rate=1.0)\n",
    "\n",
    "print(f\"Capacity:     {bucket._capacity}\")\n",
    "print(f\"Tokens:       {bucket._tokens}\")\n",
    "print(f\"Refill rate:  {bucket._refill_rate} tokens/sec\")\n",
    "print(f\"Lock type:    {type(bucket._lock).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice:\n",
    "- Bucket starts **full** (`tokens == capacity`)\n",
    "- Uses `asyncio.Lock` for concurrent safety (multiple agents sharing one bucket)\n",
    "- `refill_rate` is in tokens/second (RPM / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Acquiring Tokens — Immediate vs. Wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Small bucket for demo: capacity=3, refill 10/sec (fast for demo)\n",
    "bucket = TokenBucket(capacity=3, refill_rate=10.0)\n",
    "\n",
    "# Immediate acquires (tokens available)\n",
    "for i in range(3):\n",
    "    wait = await bucket.acquire()\n",
    "    print(f\"Acquire {i+1}: wait={wait:.4f}s, tokens_remaining={bucket._tokens:.1f}\")\n",
    "\n",
    "print(\"\\n--- Bucket empty! Next acquire will wait ---\\n\")\n",
    "\n",
    "# This one has to wait for refill\n",
    "wait = await bucket.acquire()\n",
    "print(f\"Acquire 4: wait={wait:.4f}s (waited for refill)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key behaviors:\n",
    "- Returns `0.0` when a token is immediately available\n",
    "- Returns `> 0.0` when it had to wait (the actual wait time in seconds)\n",
    "- The caller sees **no error** — just a transparent delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Refill Mechanics\n",
    "\n",
    "Tokens refill based on elapsed time since last refill, capped at capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from unittest.mock import patch\n",
    "\n",
    "# Demonstrate refill with mocked time\n",
    "with patch(\"arcllm.modules.rate_limit.time.monotonic\") as mock_mono:\n",
    "    mock_mono.return_value = 1000.0\n",
    "    bucket = TokenBucket(capacity=10, refill_rate=2.0)  # 2 tokens/sec\n",
    "    \n",
    "    # Drain all tokens\n",
    "    for _ in range(10):\n",
    "        # Direct internal drain (bypassing async for demo)\n",
    "        bucket._tokens -= 1.0\n",
    "    print(f\"After draining: {bucket._tokens} tokens\")\n",
    "    \n",
    "    # Advance time by 3 seconds → should refill 6 tokens\n",
    "    mock_mono.return_value = 1003.0\n",
    "    bucket._refill()\n",
    "    print(f\"After 3s refill: {bucket._tokens} tokens (2/sec × 3s = 6)\")\n",
    "    \n",
    "    # Advance 100 more seconds → would be 200, but capped at 10\n",
    "    mock_mono.return_value = 1103.0\n",
    "    bucket._refill()\n",
    "    print(f\"After 100s more: {bucket._tokens} tokens (capped at capacity={bucket._capacity})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The refill formula:\n",
    "```python\n",
    "elapsed = now - last_refill\n",
    "tokens = min(capacity, tokens + elapsed * refill_rate)\n",
    "```\n",
    "\n",
    "- `time.monotonic()` — immune to system clock adjustments\n",
    "- Cap prevents unbounded token accumulation during idle periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Burst Capacity\n",
    "\n",
    "The `capacity` parameter controls **burst allowance** — how many requests can fire immediately before throttling kicks in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# burst_capacity=5 allows 5 immediate requests\n",
    "bucket = TokenBucket(capacity=5, refill_rate=1.0)\n",
    "\n",
    "waits = []\n",
    "for i in range(5):\n",
    "    w = await bucket.acquire()\n",
    "    waits.append(w)\n",
    "\n",
    "print(f\"5 burst acquires: {waits}\")\n",
    "print(f\"All immediate:    {all(w == 0.0 for w in waits)}\")\n",
    "print(f\"Tokens remaining: {bucket._tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why separate burst_capacity from RPM?**\n",
    "\n",
    "| Config | RPM | Burst | Behavior |\n",
    "|--------|-----|-------|----------|\n",
    "| Default | 60 | 60 | Fire 60 immediately, then 1/sec sustained |\n",
    "| Conservative | 60 | 10 | Fire 10 immediately, then 1/sec sustained |\n",
    "| Aggressive | 60 | 120 | Fire 120 immediately, then 1/sec sustained |\n",
    "\n",
    "Default: `burst_capacity = requests_per_minute` (sensible for most APIs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. The RateLimitModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import AsyncMock, MagicMock\n",
    "from arcllm.modules.rate_limit import RateLimitModule, clear_buckets\n",
    "from arcllm.types import LLMProvider, LLMResponse, Message, Usage\n",
    "\n",
    "clear_buckets()  # Clean state\n",
    "\n",
    "# Create a mock inner adapter\n",
    "inner = MagicMock(spec=LLMProvider)\n",
    "inner.name = \"anthropic\"\n",
    "inner.model_name = \"claude-sonnet-4-20250514\"\n",
    "inner.invoke = AsyncMock(return_value=LLMResponse(\n",
    "    content=\"Hello!\",\n",
    "    usage=Usage(input_tokens=10, output_tokens=5, total_tokens=15),\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    stop_reason=\"end_turn\",\n",
    "))\n",
    "\n",
    "# Wrap with rate limiting: 60 RPM, burst=5\n",
    "config = {\"requests_per_minute\": 60, \"burst_capacity\": 5}\n",
    "module = RateLimitModule(config, inner)\n",
    "\n",
    "print(f\"Module type:    {type(module).__name__}\")\n",
    "print(f\"Provider name:  {module._provider_name}\")\n",
    "print(f\"Bucket capacity: {module._bucket._capacity}\")\n",
    "print(f\"Bucket refill:  {module._bucket._refill_rate} tokens/sec\")\n",
    "print(f\"  (60 RPM / 60 = 1.0 tokens/sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke — acquires token, then delegates to inner\n",
    "messages = [Message(role=\"user\", content=\"hi\")]\n",
    "result = await module.invoke(messages)\n",
    "\n",
    "print(f\"Response: {result.content}\")\n",
    "print(f\"Inner called: {inner.invoke.await_count} time(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow:\n",
    "```\n",
    "module.invoke(messages)\n",
    "  → bucket.acquire()          # Get a token (wait if empty)\n",
    "  → if waited: log WARNING    # \"Rate limited for 'anthropic'. Waited 0.42s\"\n",
    "  → inner.invoke(messages)    # Delegate to actual adapter\n",
    "  → return response\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Per-Provider Shared Buckets\n",
    "\n",
    "Rate limits are per API key, not per model. So all agents using the same provider share one bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.modules.rate_limit import _bucket_registry\n",
    "\n",
    "clear_buckets()\n",
    "\n",
    "# Two modules for same provider\n",
    "inner1 = MagicMock(spec=LLMProvider)\n",
    "inner1.name = \"anthropic\"\n",
    "inner2 = MagicMock(spec=LLMProvider)\n",
    "inner2.name = \"anthropic\"\n",
    "\n",
    "config = {\"requests_per_minute\": 60}\n",
    "m1 = RateLimitModule(config, inner1)\n",
    "m2 = RateLimitModule(config, inner2)\n",
    "\n",
    "print(f\"Same bucket?     {m1._bucket is m2._bucket}\")\n",
    "print(f\"Registry entries: {list(_bucket_registry.keys())}\")\n",
    "\n",
    "# Different provider → different bucket\n",
    "inner3 = MagicMock(spec=LLMProvider)\n",
    "inner3.name = \"openai\"\n",
    "m3 = RateLimitModule(config, inner3)\n",
    "\n",
    "print(f\"\\nAfter OpenAI module:\")\n",
    "print(f\"Registry entries: {list(_bucket_registry.keys())}\")\n",
    "print(f\"Same as m1?      {m1._bucket is m3._bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_buckets() resets shared state — used by tests and registry.clear_cache()\n",
    "print(f\"Before clear: {len(_bucket_registry)} buckets\")\n",
    "clear_buckets()\n",
    "print(f\"After clear:  {len(_bucket_registry)} buckets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Config Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.exceptions import ArcLLMConfigError\n",
    "\n",
    "inner = MagicMock(spec=LLMProvider)\n",
    "inner.name = \"test\"\n",
    "\n",
    "# Zero RPM → rejected (would mean no requests ever)\n",
    "try:\n",
    "    RateLimitModule({\"requests_per_minute\": 0}, inner)\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"Zero RPM:      {e}\")\n",
    "\n",
    "# Negative RPM → rejected\n",
    "try:\n",
    "    RateLimitModule({\"requests_per_minute\": -10}, inner)\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"Negative RPM:  {e}\")\n",
    "\n",
    "# Zero burst → rejected (can't make any request)\n",
    "try:\n",
    "    RateLimitModule({\"requests_per_minute\": 60, \"burst_capacity\": 0}, inner)\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"Zero burst:    {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# burst_capacity defaults to RPM when not specified\n",
    "clear_buckets()\n",
    "module = RateLimitModule({\"requests_per_minute\": 120}, inner)\n",
    "print(f\"RPM=120, no burst specified → burst_capacity={module._bucket._capacity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Throttle Logging\n",
    "\n",
    "When the bucket is empty and a caller waits, a WARNING is logged with the provider name and wait duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "clear_buckets()\n",
    "\n",
    "# Set up logging capture\n",
    "handler = logging.StreamHandler()\n",
    "handler.setLevel(logging.WARNING)\n",
    "rl_logger = logging.getLogger(\"arcllm.modules.rate_limit\")\n",
    "rl_logger.addHandler(handler)\n",
    "rl_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Tiny burst so we hit the limit fast\n",
    "inner = MagicMock(spec=LLMProvider)\n",
    "inner.name = \"anthropic\"\n",
    "inner.invoke = AsyncMock(return_value=LLMResponse(\n",
    "    content=\"ok\",\n",
    "    usage=Usage(input_tokens=10, output_tokens=5, total_tokens=15),\n",
    "    model=\"test\",\n",
    "    stop_reason=\"end_turn\",\n",
    "))\n",
    "\n",
    "module = RateLimitModule({\"requests_per_minute\": 60, \"burst_capacity\": 1}, inner)\n",
    "\n",
    "# First call: immediate (has 1 token)\n",
    "await module.invoke([Message(role=\"user\", content=\"first\")])\n",
    "print(\"First call: immediate (no log)\")\n",
    "\n",
    "# Second call: must wait → WARNING logged\n",
    "print(\"\\nSecond call (will trigger WARNING):\")\n",
    "await module.invoke([Message(role=\"user\", content=\"second\")])\n",
    "\n",
    "# Cleanup\n",
    "rl_logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Concurrent Safety\n",
    "\n",
    "The `asyncio.Lock` ensures multiple concurrent agents sharing one bucket don't corrupt token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate 5 concurrent agents competing for 3 tokens\n",
    "bucket = TokenBucket(capacity=3, refill_rate=100.0)  # Fast refill for demo\n",
    "\n",
    "# Drain all tokens first\n",
    "for _ in range(3):\n",
    "    await bucket.acquire()\n",
    "\n",
    "print(f\"Tokens after drain: {bucket._tokens}\")\n",
    "print(\"Launching 5 concurrent acquires...\\n\")\n",
    "\n",
    "# All 5 compete for tokens simultaneously\n",
    "results = await asyncio.gather(\n",
    "    bucket.acquire(),\n",
    "    bucket.acquire(),\n",
    "    bucket.acquire(),\n",
    "    bucket.acquire(),\n",
    "    bucket.acquire(),\n",
    ")\n",
    "\n",
    "print(f\"Wait times: {[f'{w:.4f}s' for w in results]}\")\n",
    "print(f\"Tokens after: {bucket._tokens:.1f} (>= 0 guaranteed)\")\n",
    "print(f\"All floats: {all(isinstance(w, float) for w in results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lock is held **only** during the refill+check. Sleep happens **outside** the lock:\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    async with self._lock:        # Hold lock briefly\n",
    "        self._refill()\n",
    "        if self._tokens >= 1.0:\n",
    "            self._tokens -= 1.0\n",
    "            return total_wait\n",
    "        wait_seconds = ...        # Calculate wait\n",
    "    \n",
    "    await asyncio.sleep(wait)     # Sleep OUTSIDE lock\n",
    "    total_wait += wait_seconds    # Loop back to re-check\n",
    "```\n",
    "\n",
    "This prevents one sleeping caller from blocking others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Registry Integration\n",
    "\n",
    "`load_model()` now accepts `rate_limit` kwarg with the same 4-level config resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.registry import load_model, clear_cache\n",
    "import os\n",
    "\n",
    "os.environ.setdefault(\"ANTHROPIC_API_KEY\", \"test-key\")\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", \"test-key\")\n",
    "clear_cache()\n",
    "\n",
    "# Enable rate limiting\n",
    "model = load_model(\"anthropic\", rate_limit=True)\n",
    "print(f\"Type: {type(model).__name__}\")\n",
    "print(f\"Inner: {type(model._inner).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cache()\n",
    "\n",
    "# Custom RPM\n",
    "model = load_model(\"anthropic\", rate_limit={\"requests_per_minute\": 120})\n",
    "print(f\"Custom RPM: bucket capacity = {model._bucket._capacity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cache()\n",
    "\n",
    "# Full stack: Retry(Fallback(RateLimit(adapter)))\n",
    "model = load_model(\"anthropic\", retry=True, fallback=True, rate_limit=True)\n",
    "print(f\"Layer 1 (outer): {type(model).__name__}\")\n",
    "print(f\"Layer 2:         {type(model._inner).__name__}\")\n",
    "print(f\"Layer 3:         {type(model._inner._inner).__name__}\")\n",
    "print(f\"Layer 4 (inner): {type(model._inner._inner._inner).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking order (innermost first):\n",
    "1. **Adapter** — makes the actual HTTP call\n",
    "2. **RateLimit** — throttles before call goes out\n",
    "3. **Fallback** — switches provider on failure\n",
    "4. **Retry** — retries transient failures\n",
    "\n",
    "Why RateLimit is innermost:\n",
    "- Throttles *before* the call → prevents hitting the API too fast\n",
    "- Fallback doesn't trigger on rate-limit waits (it's just a delay, not an error)\n",
    "- Retry covers failures *after* the rate-limited call succeeds in getting through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Agent Code Unchanged\n",
    "\n",
    "The agent never knows rate limiting exists — same `model.invoke()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent code is identical with or without rate limiting:\n",
    "\n",
    "# Without rate limiting\n",
    "# model = load_model(\"anthropic\")\n",
    "\n",
    "# With rate limiting\n",
    "# model = load_model(\"anthropic\", rate_limit=True)\n",
    "\n",
    "# Agent code (unchanged either way):\n",
    "# response = await model.invoke(messages, tools)\n",
    "# if response.stop_reason == \"tool_use\":\n",
    "#     ...\n",
    "\n",
    "print(\"Agent code is the same with or without rate_limit=True\")\n",
    "print(\"The only difference: calls may be slightly delayed when throttled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Mocked Rate-Limited Cycle\n",
    "\n",
    "Simulate what happens when an agent hits the rate limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import patch as mock_patch\n",
    "\n",
    "clear_buckets()\n",
    "\n",
    "inner = MagicMock(spec=LLMProvider)\n",
    "inner.name = \"anthropic\"\n",
    "inner.invoke = AsyncMock(return_value=LLMResponse(\n",
    "    content=\"I can help with that!\",\n",
    "    usage=Usage(input_tokens=20, output_tokens=15, total_tokens=35),\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    stop_reason=\"end_turn\",\n",
    "))\n",
    "\n",
    "# burst_capacity=2, so 3rd request must wait\n",
    "module = RateLimitModule({\"requests_per_minute\": 60, \"burst_capacity\": 2}, inner)\n",
    "\n",
    "for i in range(4):\n",
    "    msg = [Message(role=\"user\", content=f\"Agent request #{i+1}\")]\n",
    "    result = await module.invoke(msg)\n",
    "    print(f\"Request {i+1}: '{result.content}' (bucket: {module._bucket._tokens:.1f} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Implementation Details\n",
    "\n",
    "Complete source: `src/arcllm/modules/rate_limit.py` (124 lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from arcllm.modules.rate_limit import TokenBucket, RateLimitModule\n",
    "\n",
    "# TokenBucket acquire method\n",
    "print(\"=== TokenBucket.acquire ===\")\n",
    "print(inspect.getsource(TokenBucket.acquire))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RateLimitModule invoke method\n",
    "print(\"=== RateLimitModule.invoke ===\")\n",
    "print(inspect.getsource(RateLimitModule.invoke))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Live API Call with Rate Limiting\n",
    "\n",
    "Real Anthropic API call with `rate_limit=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(os.getcwd(), '..', '.env'))\n",
    "\n",
    "has_key = bool(os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "print(f\"Anthropic API key: {'found' if has_key else 'not found (skip live tests)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_key:\n",
    "    clear_cache()\n",
    "    \n",
    "    # Load with rate limiting (60 RPM default)\n",
    "    model = load_model(\"anthropic\", rate_limit=True)\n",
    "    print(f\"Model type: {type(model).__name__}\")\n",
    "    print(f\"Inner type: {type(model._inner).__name__}\")\n",
    "    \n",
    "    response = await model.invoke([\n",
    "        Message(role=\"user\", content=\"What is a token bucket? One sentence.\")\n",
    "    ])\n",
    "    print(f\"\\nResponse: {response.content}\")\n",
    "    print(f\"Tokens: {response.usage.input_tokens} in / {response.usage.output_tokens} out\")\n",
    "    print(f\"Bucket tokens remaining: {model._bucket._tokens:.1f}\")\n",
    "    \n",
    "    await model.close()\n",
    "else:\n",
    "    print(\"Skipped — no API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_key:\n",
    "    clear_cache()\n",
    "    \n",
    "    # Full stack: rate limit + retry\n",
    "    model = load_model(\"anthropic\", rate_limit=True, retry=True)\n",
    "    stack = type(model).__name__\n",
    "    inner = type(model._inner).__name__\n",
    "    innermost = type(model._inner._inner).__name__\n",
    "    print(f\"Stack: {stack}({inner}({innermost}))\")\n",
    "    \n",
    "    response = await model.invoke([\n",
    "        Message(role=\"user\", content=\"Say 'rate limited and retried' in exactly those words.\")\n",
    "    ])\n",
    "    print(f\"Response: {response.content}\")\n",
    "    \n",
    "    await model._inner._inner.close()  # Close the adapter\n",
    "else:\n",
    "    print(\"Skipped — no API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Component | What | Why |\n",
    "|-----------|------|-----|\n",
    "| `TokenBucket` | Counter + timestamp + async lock | Classic algorithm, allows bursts, enforces average rate |\n",
    "| `RateLimitModule` | Acquires token before each `invoke()` | Transparent throttling, no agent code changes |\n",
    "| Shared buckets | Per-provider registry dict | Matches provider reality: limits are per API key |\n",
    "| `clear_buckets()` | Resets shared state | Test isolation, hooked into `registry.clear_cache()` |\n",
    "| WARNING log | Emitted when caller waits | Operators see why calls are slower |\n",
    "| Stack position | Innermost (before Fallback/Retry) | Throttle before call, not after failure |\n",
    "\n",
    "**Config**:\n",
    "```python\n",
    "load_model(\"anthropic\", rate_limit=True)                        # 60 RPM default\n",
    "load_model(\"anthropic\", rate_limit={\"requests_per_minute\": 120}) # Custom RPM\n",
    "load_model(\"anthropic\", rate_limit=False)                        # Disable\n",
    "```\n",
    "\n",
    "**Test count**: 244 passed, 1 skipped (25 new rate limiter tests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}