{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArcLLM Step 3: Anthropic Adapter + Tool Support\n",
    "\n",
    "This notebook walks through everything built in Step 3 — the **adapter layer** that translates between ArcLLM's universal types and the Anthropic Messages API.\n",
    "\n",
    "**What was built:**\n",
    "- `ArcLLMAPIError` — new exception for HTTP errors from providers\n",
    "- `BaseAdapter` — shared plumbing all adapters inherit (config, API key, httpx client, context manager)\n",
    "- `AnthropicAdapter` — request building + response parsing for Anthropic's API\n",
    "\n",
    "**Why it matters:** This is where the abstraction earns its keep. Agents write `model.invoke(messages, tools)` using universal types. The adapter handles everything provider-specific — headers, system message extraction, `parameters` → `input_schema`, `role=\"tool\"` → `role=\"user\"`, parsing tool calls, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from unittest.mock import AsyncMock\n",
    "import httpx\n",
    "\n",
    "from arcllm import (\n",
    "    AnthropicAdapter, BaseAdapter,\n",
    "    ArcLLMAPIError, ArcLLMConfigError, ArcLLMError, ArcLLMParseError,\n",
    "    ProviderConfig, ProviderSettings, ModelMetadata,\n",
    "    Message, TextBlock, ToolUseBlock, ToolResultBlock, ImageBlock,\n",
    "    Tool, ToolCall, Usage, LLMResponse, LLMProvider,\n",
    ")\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: create a fake config and set a test API key\n",
    "# We'll use these throughout the notebook\n",
    "os.environ[\"ARCLLM_TEST_KEY\"] = \"sk-test-key-for-walkthrough\"\n",
    "\n",
    "FAKE_MODEL = \"claude-test-1\"\n",
    "\n",
    "fake_config = ProviderConfig(\n",
    "    provider=ProviderSettings(\n",
    "        api_format=\"anthropic-messages\",\n",
    "        base_url=\"https://api.anthropic.com\",\n",
    "        api_key_env=\"ARCLLM_TEST_KEY\",\n",
    "        default_model=FAKE_MODEL,\n",
    "        default_temperature=0.7,\n",
    "    ),\n",
    "    models={\n",
    "        FAKE_MODEL: ModelMetadata(\n",
    "            context_window=200000,\n",
    "            max_output_tokens=8192,\n",
    "            supports_tools=True,\n",
    "            supports_vision=True,\n",
    "            supports_thinking=True,\n",
    "            input_modalities=[\"text\", \"image\"],\n",
    "            cost_input_per_1m=3.0,\n",
    "            cost_output_per_1m=15.0,\n",
    "            cost_cache_read_per_1m=0.3,\n",
    "            cost_cache_write_per_1m=3.75,\n",
    "        )\n",
    "    },\n",
    ")\n",
    "print(f\"Fake config ready: model={FAKE_MODEL}, key_env=ARCLLM_TEST_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ArcLLMAPIError\n",
    "\n",
    "Step 3 added a new exception to the hierarchy:\n",
    "\n",
    "```\n",
    "ArcLLMError (base)\n",
    "├── ArcLLMParseError   (tool call JSON couldn't be parsed)\n",
    "├── ArcLLMConfigError  (config validation failed)\n",
    "└── ArcLLMAPIError     (provider returned HTTP error)  ← NEW\n",
    "```\n",
    "\n",
    "`ArcLLMAPIError` carries three pieces of info:\n",
    "- `status_code` — HTTP status (429, 401, 500, etc.)\n",
    "- `body` — raw response body from the provider\n",
    "- `provider` — which provider returned the error\n",
    "\n",
    "This lets agents and the retry module make smart decisions (e.g., 429 → retry, 401 → don't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an API error\n",
    "err = ArcLLMAPIError(status_code=429, body=\"rate limited\", provider=\"anthropic\")\n",
    "print(f\"Error message: {err}\")\n",
    "print(f\"Status code:   {err.status_code}\")\n",
    "print(f\"Body:          {err.body}\")\n",
    "print(f\"Provider:      {err.provider}\")\n",
    "print(f\"Is ArcLLMError? {isinstance(err, ArcLLMError)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart retry logic based on status code\n",
    "def should_retry(error: ArcLLMAPIError) -> bool:\n",
    "    \"\"\"Example: retry on transient errors, don't on auth/validation.\"\"\"\n",
    "    if error.status_code == 429:  # Rate limited\n",
    "        return True\n",
    "    if error.status_code >= 500:  # Server error\n",
    "        return True\n",
    "    return False  # 401, 403, 400 — don't retry\n",
    "\n",
    "errors = [\n",
    "    ArcLLMAPIError(429, \"rate limited\", \"anthropic\"),\n",
    "    ArcLLMAPIError(500, \"internal error\", \"anthropic\"),\n",
    "    ArcLLMAPIError(401, \"invalid key\", \"anthropic\"),\n",
    "    ArcLLMAPIError(400, \"bad request\", \"anthropic\"),\n",
    "]\n",
    "\n",
    "for e in errors:\n",
    "    print(f\"  HTTP {e.status_code}: retry={should_retry(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. BaseAdapter — Shared Plumbing\n",
    "\n",
    "`BaseAdapter` is a concrete class that all provider adapters inherit from. It handles:\n",
    "\n",
    "| Responsibility | How |\n",
    "|----------------|-----|\n",
    "| Config storage | `_config`, `_model_name`, `_model_meta` |\n",
    "| API key resolution | Reads from `os.environ` at init, fails fast |\n",
    "| HTTP client | Creates `httpx.AsyncClient` with 60s timeout |\n",
    "| Lifecycle | `async with adapter:` context manager, `.close()` |\n",
    "| Interface | Inherits from `LLMProvider` ABC |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BaseAdapter — it stores config and resolves the API key\n",
    "adapter = BaseAdapter(fake_config, FAKE_MODEL)\n",
    "print(f\"Config stored:   {adapter._config.provider.api_format}\")\n",
    "print(f\"Model name:      {adapter._model_name}\")\n",
    "print(f\"Model meta:      {adapter._model_meta is not None}\")\n",
    "print(f\"API key resolved: {adapter._api_key[:10]}...\")\n",
    "print(f\"HTTP client:     {type(adapter._client).__name__}\")\n",
    "print(f\"Is LLMProvider?  {isinstance(adapter, LLMProvider)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model metadata is looked up automatically from config\n",
    "print(f\"Model metadata for '{FAKE_MODEL}':\")\n",
    "print(f\"  Context window: {adapter._model_meta.context_window:,}\")\n",
    "print(f\"  Max output:     {adapter._model_meta.max_output_tokens:,}\")\n",
    "print(f\"  Supports tools: {adapter._model_meta.supports_tools}\")\n",
    "\n",
    "# Unknown model name → _model_meta is None (not an error)\n",
    "adapter_unknown = BaseAdapter(fake_config, \"nonexistent-model\")\n",
    "print(f\"\\nUnknown model meta: {adapter_unknown._model_meta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security: missing API key → fail fast at init\n",
    "# (Not during an LLM call 3 hours into a batch job)\n",
    "saved_key = os.environ.pop(\"ARCLLM_TEST_KEY\")\n",
    "try:\n",
    "    BaseAdapter(fake_config, FAKE_MODEL)\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"BLOCKED at init: {e}\")\n",
    "finally:\n",
    "    os.environ[\"ARCLLM_TEST_KEY\"] = saved_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty API key is also rejected\n",
    "saved_key = os.environ[\"ARCLLM_TEST_KEY\"]\n",
    "os.environ[\"ARCLLM_TEST_KEY\"] = \"\"\n",
    "try:\n",
    "    BaseAdapter(fake_config, FAKE_MODEL)\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"BLOCKED (empty key): {e}\")\n",
    "finally:\n",
    "    os.environ[\"ARCLLM_TEST_KEY\"] = saved_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async context manager — clean resource lifecycle\n",
    "import asyncio\n",
    "\n",
    "async def demo_context_manager():\n",
    "    async with BaseAdapter(fake_config, FAKE_MODEL) as adapter:\n",
    "        print(f\"Inside context: client={adapter._client is not None}\")\n",
    "    print(f\"After context:  client={adapter._client}  (closed!)\")\n",
    "\n",
    "await demo_context_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close() is idempotent — safe to call multiple times\n",
    "async def demo_close_twice():\n",
    "    adapter = BaseAdapter(fake_config, FAKE_MODEL)\n",
    "    await adapter.close()\n",
    "    await adapter.close()  # no error\n",
    "    print(\"close() called twice — no error\")\n",
    "\n",
    "await demo_close_twice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. AnthropicAdapter — Request Building\n",
    "\n",
    "The `AnthropicAdapter` translates ArcLLM's universal types into Anthropic's specific API format. Let's see each translation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = AnthropicAdapter(fake_config, FAKE_MODEL)\n",
    "print(f\"Adapter name: {adapter.name}\")\n",
    "print(f\"Is LLMProvider? {isinstance(adapter, LLMProvider)}\")\n",
    "print(f\"Is BaseAdapter? {isinstance(adapter, BaseAdapter)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Headers\n",
    "\n",
    "Anthropic requires specific headers: API key, API version, content type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = adapter._build_headers()\n",
    "print(\"Request headers:\")\n",
    "for key, value in headers.items():\n",
    "    display_val = value[:15] + \"...\" if key == \"x-api-key\" else value\n",
    "    print(f\"  {key}: {display_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. System Message Extraction\n",
    "\n",
    "Anthropic's API takes `system` as a **top-level parameter**, not inside the messages array. ArcLLM uses `role=\"system\"` in the universal Message type, so the adapter must extract it.\n",
    "\n",
    "Multiple system messages are concatenated with newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single system message → extracted as top-level param\n",
    "messages = [\n",
    "    Message(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    Message(role=\"user\", content=\"Hello!\"),\n",
    "]\n",
    "\n",
    "system_text, remaining = adapter._extract_system(messages)\n",
    "print(f\"System text: {system_text!r}\")\n",
    "print(f\"Remaining messages: {len(remaining)}\")\n",
    "print(f\"  [0] role={remaining[0].role}, content={remaining[0].content!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple system messages → concatenated\n",
    "messages = [\n",
    "    Message(role=\"system\", content=\"Be concise.\"),\n",
    "    Message(role=\"system\", content=\"Use tools when needed.\"),\n",
    "    Message(role=\"user\", content=\"Hi\"),\n",
    "]\n",
    "\n",
    "system_text, remaining = adapter._extract_system(messages)\n",
    "print(f\"Concatenated system: {system_text!r}\")\n",
    "print(f\"Remaining: {len(remaining)} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No system message → None (won't be included in request)\n",
    "messages = [Message(role=\"user\", content=\"Hi\")]\n",
    "system_text, remaining = adapter._extract_system(messages)\n",
    "print(f\"No system message: {system_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Content Block Formatting\n",
    "\n",
    "Each ArcLLM content block type maps to a specific Anthropic API format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlock → simple passthrough\n",
    "text_formatted = adapter._format_content_block(TextBlock(text=\"Hello!\"))\n",
    "print(f\"TextBlock ->\")\n",
    "print(f\"  {json.dumps(text_formatted, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageBlock → nested source object with base64\n",
    "img_formatted = adapter._format_content_block(\n",
    "    ImageBlock(source=\"base64data...\", media_type=\"image/png\")\n",
    ")\n",
    "print(f\"ImageBlock ->\")\n",
    "print(f\"  {json.dumps(img_formatted, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToolUseBlock → 'arguments' becomes 'input' (Anthropic's naming)\n",
    "tool_formatted = adapter._format_content_block(\n",
    "    ToolUseBlock(id=\"toolu_01\", name=\"search\", arguments={\"query\": \"cats\"})\n",
    ")\n",
    "print(f\"ToolUseBlock ->\")\n",
    "print(f\"  {json.dumps(tool_formatted, indent=2)}\")\n",
    "print(f\"\\nNote: 'arguments' (ArcLLM) → 'input' (Anthropic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToolResultBlock with string content\n",
    "result_str = adapter._format_content_block(\n",
    "    ToolResultBlock(tool_use_id=\"toolu_01\", content=\"Found 3 results\")\n",
    ")\n",
    "print(f\"ToolResultBlock (string) ->\")\n",
    "print(f\"  {json.dumps(result_str, indent=2)}\")\n",
    "\n",
    "# ToolResultBlock with nested content blocks\n",
    "result_list = adapter._format_content_block(\n",
    "    ToolResultBlock(\n",
    "        tool_use_id=\"toolu_01\",\n",
    "        content=[TextBlock(text=\"Result 1\"), TextBlock(text=\"Result 2\")],\n",
    "    )\n",
    ")\n",
    "print(f\"\\nToolResultBlock (list) ->\")\n",
    "print(f\"  {json.dumps(result_list, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Role Translation\n",
    "\n",
    "ArcLLM uses `role=\"tool\"` for tool results. Anthropic expects `role=\"user\"` with tool_result content blocks. The adapter handles this mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# role=\"tool\" → role=\"user\" in the Anthropic request\n",
    "tool_msg = Message(\n",
    "    role=\"tool\",\n",
    "    content=[ToolResultBlock(tool_use_id=\"t1\", content=\"42\")],\n",
    ")\n",
    "formatted = adapter._format_message(tool_msg)\n",
    "print(f\"ArcLLM role:    {tool_msg.role!r}\")\n",
    "print(f\"Anthropic role: {formatted['role']!r}\")\n",
    "print(f\"Content:        {json.dumps(formatted['content'], indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other roles pass through unchanged\n",
    "for role in [\"user\", \"assistant\"]:\n",
    "    msg = Message(role=role, content=\"test\")\n",
    "    fmt = adapter._format_message(msg)\n",
    "    print(f\"  {role:>10} → {fmt['role']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Tool Definition Formatting\n",
    "\n",
    "ArcLLM uses `parameters` (JSON Schema). Anthropic expects `input_schema`. Same data, different key name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool definition: parameters → input_schema\n",
    "tool = Tool(\n",
    "    name=\"search_database\",\n",
    "    description=\"Search the knowledge base\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n",
    "            \"limit\": {\"type\": \"integer\", \"default\": 10},\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "formatted_tool = adapter._format_tool(tool)\n",
    "print(f\"ArcLLM key:    'parameters'\")\n",
    "print(f\"Anthropic key: 'input_schema'\")\n",
    "print(f\"\\nFormatted tool:\")\n",
    "print(json.dumps(formatted_tool, indent=2))\n",
    "print(f\"\\n'parameters' in output? {'parameters' in formatted_tool}\")\n",
    "print(f\"'input_schema' in output? {'input_schema' in formatted_tool}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3f. Full Request Body\n",
    "\n",
    "`_build_request_body()` combines everything into the final API payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text request\n",
    "messages = [Message(role=\"user\", content=\"What's the weather?\")]\n",
    "body = adapter._build_request_body(messages)\n",
    "print(\"Simple request body:\")\n",
    "print(json.dumps(body, indent=2))\n",
    "print(f\"\\nNote: no 'system' key (no system message)\")\n",
    "print(f\"Note: no 'tools' key (no tools provided)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request with system message + tools\n",
    "messages = [\n",
    "    Message(role=\"system\", content=\"You are a research assistant.\"),\n",
    "    Message(role=\"user\", content=\"Find papers about LLM agents\"),\n",
    "]\n",
    "tools = [tool]  # from above\n",
    "\n",
    "body = adapter._build_request_body(messages, tools=tools)\n",
    "print(\"Full request body:\")\n",
    "print(json.dumps(body, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwargs override config defaults\n",
    "body_default = adapter._build_request_body(messages)\n",
    "body_override = adapter._build_request_body(messages, max_tokens=1000, temperature=0.0)\n",
    "\n",
    "print(f\"Default:  max_tokens={body_default['max_tokens']}, temperature={body_default['temperature']}\")\n",
    "print(f\"Override: max_tokens={body_override['max_tokens']}, temperature={body_override['temperature']}\")\n",
    "print(f\"\\nOverride chain: kwargs > provider config > model metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. AnthropicAdapter — Response Parsing\n",
    "\n",
    "When Anthropic responds, the adapter parses the raw JSON into ArcLLM's universal `LLMResponse` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: simulate an Anthropic API response\n",
    "def make_anthropic_response(content_blocks, stop_reason=\"end_turn\",\n",
    "                            input_tokens=100, output_tokens=50, **extra_usage):\n",
    "    usage = {\"input_tokens\": input_tokens, \"output_tokens\": output_tokens}\n",
    "    usage.update(extra_usage)\n",
    "    return {\n",
    "        \"id\": \"msg_walkthrough\",\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"model\": FAKE_MODEL,\n",
    "        \"content\": content_blocks,\n",
    "        \"stop_reason\": stop_reason,\n",
    "        \"usage\": usage,\n",
    "    }\n",
    "\n",
    "print(\"Helper ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a text response\n",
    "raw = make_anthropic_response([{\"type\": \"text\", \"text\": \"The weather is sunny.\"}])\n",
    "resp = adapter._parse_response(raw)\n",
    "\n",
    "print(f\"Type:        {type(resp).__name__}\")\n",
    "print(f\"Content:     {resp.content!r}\")\n",
    "print(f\"Tool calls:  {resp.tool_calls}\")\n",
    "print(f\"Stop reason: {resp.stop_reason}\")\n",
    "print(f\"Model:       {resp.model}\")\n",
    "print(f\"Usage:       {resp.usage.input_tokens}in + {resp.usage.output_tokens}out = {resp.usage.total_tokens} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a tool use response\n",
    "raw = make_anthropic_response(\n",
    "    [\n",
    "        {\"type\": \"tool_use\", \"id\": \"toolu_01\", \"name\": \"search\", \"input\": {\"query\": \"LLM agents\"}},\n",
    "    ],\n",
    "    stop_reason=\"tool_use\",\n",
    ")\n",
    "resp = adapter._parse_response(raw)\n",
    "\n",
    "print(f\"Content:     {resp.content}  (None — pure tool call)\")\n",
    "print(f\"Stop reason: {resp.stop_reason}\")\n",
    "print(f\"Tool calls:  {len(resp.tool_calls)}\")\n",
    "print(f\"  [0] id={resp.tool_calls[0].id}, name={resp.tool_calls[0].name}\")\n",
    "print(f\"      arguments={resp.tool_calls[0].arguments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a mixed response (text + tool calls)\n",
    "raw = make_anthropic_response(\n",
    "    [\n",
    "        {\"type\": \"text\", \"text\": \"Let me search for that.\"},\n",
    "        {\"type\": \"tool_use\", \"id\": \"toolu_02\", \"name\": \"search\", \"input\": {\"query\": \"cats\"}},\n",
    "    ],\n",
    "    stop_reason=\"tool_use\",\n",
    ")\n",
    "resp = adapter._parse_response(raw)\n",
    "\n",
    "print(f\"Content:     {resp.content!r}\")\n",
    "print(f\"Tool calls:  {len(resp.tool_calls)}\")\n",
    "print(f\"Both present — text AND tool calls in same response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse extended thinking (Claude's chain-of-thought)\n",
    "raw = make_anthropic_response(\n",
    "    [\n",
    "        {\"type\": \"thinking\", \"thinking\": \"Let me reason about this step by step...\"},\n",
    "        {\"type\": \"text\", \"text\": \"The answer is 42.\"},\n",
    "    ]\n",
    ")\n",
    "resp = adapter._parse_response(raw)\n",
    "\n",
    "print(f\"Content:  {resp.content!r}\")\n",
    "print(f\"Thinking: {resp.thinking!r}\")\n",
    "print(f\"\\nThinking is separated from content — agents can log it without exposing it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage parsing with cache tokens\n",
    "raw = make_anthropic_response(\n",
    "    [{\"type\": \"text\", \"text\": \"Cached response.\"}],\n",
    "    cache_read_input_tokens=1200,\n",
    "    cache_creation_input_tokens=300,\n",
    ")\n",
    "resp = adapter._parse_response(raw)\n",
    "\n",
    "print(f\"Usage:\")\n",
    "print(f\"  Input tokens:       {resp.usage.input_tokens}\")\n",
    "print(f\"  Output tokens:      {resp.usage.output_tokens}\")\n",
    "print(f\"  Total tokens:       {resp.usage.total_tokens}\")\n",
    "print(f\"  Cache read tokens:  {resp.usage.cache_read_tokens}\")\n",
    "print(f\"  Cache write tokens: {resp.usage.cache_write_tokens}\")\n",
    "print(f\"\\nNote: Anthropic uses 'cache_read_input_tokens' → ArcLLM normalizes to 'cache_read_tokens'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop reasons pass through as-is\n",
    "for reason in [\"end_turn\", \"tool_use\", \"max_tokens\", \"stop_sequence\"]:\n",
    "    raw = make_anthropic_response([{\"type\": \"text\", \"text\": \"x\"}], stop_reason=reason)\n",
    "    resp = adapter._parse_response(raw)\n",
    "    print(f\"  {reason:<15} → resp.stop_reason = {resp.stop_reason!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw response preserved for debugging\n",
    "raw = make_anthropic_response([{\"type\": \"text\", \"text\": \"Hi\"}])\n",
    "resp = adapter._parse_response(raw)\n",
    "\n",
    "print(f\"resp.raw is the original dict: {resp.raw is raw}\")\n",
    "print(f\"resp.raw['id'] = {resp.raw['id']}\")\n",
    "print(f\"\\nUseful for debugging, but never logged in production.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Tool Call Parsing Edge Cases\n",
    "\n",
    "Tool call arguments can arrive as a dict (normal) or a string (edge case). The adapter handles both, and raises `ArcLLMParseError` on garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal case: arguments as dict (pass-through)\n",
    "tc = adapter._parse_tool_call({\n",
    "    \"type\": \"tool_use\", \"id\": \"t1\", \"name\": \"calc\",\n",
    "    \"input\": {\"expression\": \"2+2\"}\n",
    "})\n",
    "print(f\"Dict input → arguments: {tc.arguments}\")\n",
    "print(f\"Type: {type(tc.arguments).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge case: arguments as JSON string (json.loads it)\n",
    "tc = adapter._parse_tool_call({\n",
    "    \"type\": \"tool_use\", \"id\": \"t1\", \"name\": \"calc\",\n",
    "    \"input\": '{\"expression\": \"2+2\"}'\n",
    "})\n",
    "print(f\"String input → parsed to dict: {tc.arguments}\")\n",
    "print(f\"Type: {type(tc.arguments).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad JSON string → ArcLLMParseError with raw data preserved\n",
    "try:\n",
    "    adapter._parse_tool_call({\n",
    "        \"type\": \"tool_use\", \"id\": \"t1\", \"name\": \"calc\",\n",
    "        \"input\": \"not valid json {{{\"\n",
    "    })\n",
    "except ArcLLMParseError as e:\n",
    "    print(f\"ArcLLMParseError: {e}\")\n",
    "    print(f\"Raw string:      {e.raw_string!r}\")\n",
    "    print(f\"Original error:  {type(e.original_error).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unexpected type (not dict, not string) → ArcLLMParseError\n",
    "try:\n",
    "    adapter._parse_tool_call({\n",
    "        \"type\": \"tool_use\", \"id\": \"t1\", \"name\": \"calc\",\n",
    "        \"input\": 12345\n",
    "    })\n",
    "except ArcLLMParseError as e:\n",
    "    print(f\"Unexpected type caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Full invoke() Cycle (Mocked)\n",
    "\n",
    "Let's simulate the full `invoke()` flow — building the request, sending it (mocked), and parsing the response. No real API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a text conversation\n",
    "async def demo_text_invoke():\n",
    "    adapter = AnthropicAdapter(fake_config, FAKE_MODEL)\n",
    "\n",
    "    # Mock the HTTP client to return a fake response\n",
    "    response_data = make_anthropic_response(\n",
    "        [{\"type\": \"text\", \"text\": \"Austin is 75F and sunny.\"}]\n",
    "    )\n",
    "    mock_response = httpx.Response(\n",
    "        200, json=response_data,\n",
    "        request=httpx.Request(\"POST\", \"https://api.anthropic.com/v1/messages\"),\n",
    "    )\n",
    "    adapter._client = AsyncMock()\n",
    "    adapter._client.post = AsyncMock(return_value=mock_response)\n",
    "\n",
    "    # Call invoke() just like an agent would\n",
    "    resp = await adapter.invoke([\n",
    "        Message(role=\"system\", content=\"You are a weather bot.\"),\n",
    "        Message(role=\"user\", content=\"What's the weather in Austin?\"),\n",
    "    ])\n",
    "\n",
    "    print(f\"Response type: {type(resp).__name__}\")\n",
    "    print(f\"Content:       {resp.content}\")\n",
    "    print(f\"Stop reason:   {resp.stop_reason}\")\n",
    "    print(f\"Tokens:        {resp.usage.total_tokens}\")\n",
    "\n",
    "await demo_text_invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a tool use conversation\n",
    "async def demo_tool_invoke():\n",
    "    adapter = AnthropicAdapter(fake_config, FAKE_MODEL)\n",
    "\n",
    "    response_data = make_anthropic_response(\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": \"Let me look that up.\"},\n",
    "            {\"type\": \"tool_use\", \"id\": \"toolu_abc\", \"name\": \"get_weather\",\n",
    "             \"input\": {\"city\": \"Austin\", \"units\": \"fahrenheit\"}},\n",
    "        ],\n",
    "        stop_reason=\"tool_use\",\n",
    "    )\n",
    "    mock_response = httpx.Response(\n",
    "        200, json=response_data,\n",
    "        request=httpx.Request(\"POST\", \"https://api.anthropic.com/v1/messages\"),\n",
    "    )\n",
    "    adapter._client = AsyncMock()\n",
    "    adapter._client.post = AsyncMock(return_value=mock_response)\n",
    "\n",
    "    # Define tools\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"get_weather\",\n",
    "            description=\"Get current weather for a city\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\"type\": \"string\"},\n",
    "                    \"units\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"city\"],\n",
    "            },\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    resp = await adapter.invoke(\n",
    "        [Message(role=\"user\", content=\"What's the weather in Austin?\")],\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    print(f\"Content:     {resp.content!r}\")\n",
    "    print(f\"Stop reason: {resp.stop_reason}\")\n",
    "    print(f\"Tool calls:  {len(resp.tool_calls)}\")\n",
    "    for tc in resp.tool_calls:\n",
    "        print(f\"  {tc.name}({tc.arguments})\")\n",
    "\n",
    "await demo_tool_invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate an HTTP error from the API\n",
    "async def demo_error_invoke():\n",
    "    adapter = AnthropicAdapter(fake_config, FAKE_MODEL)\n",
    "\n",
    "    mock_response = httpx.Response(\n",
    "        429, text='{\"error\": {\"message\": \"Rate limit exceeded\"}}',\n",
    "        request=httpx.Request(\"POST\", \"https://api.anthropic.com/v1/messages\"),\n",
    "    )\n",
    "    adapter._client = AsyncMock()\n",
    "    adapter._client.post = AsyncMock(return_value=mock_response)\n",
    "\n",
    "    try:\n",
    "        await adapter.invoke([Message(role=\"user\", content=\"Hi\")])\n",
    "    except ArcLLMAPIError as e:\n",
    "        print(f\"Caught: {e}\")\n",
    "        print(f\"  status_code: {e.status_code}\")\n",
    "        print(f\"  provider:    {e.provider}\")\n",
    "        print(f\"  body:        {e.body}\")\n",
    "\n",
    "await demo_error_invoke()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Simulated Agentic Tool-Calling Loop\n",
    "\n",
    "This is the core pattern ArcLLM is built for. Let's walk through a complete tool-calling loop with mocked responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demo_agentic_loop():\n",
    "    \"\"\"Simulate a full agentic tool-calling loop.\"\"\"\n",
    "    adapter = AnthropicAdapter(fake_config, FAKE_MODEL)\n",
    "\n",
    "    # Define a tool\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"get_weather\",\n",
    "            description=\"Get weather for a city\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"city\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"city\"],\n",
    "            },\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Agent manages the conversation\n",
    "    messages = [\n",
    "        Message(role=\"system\", content=\"Use tools to answer questions.\"),\n",
    "        Message(role=\"user\", content=\"What's the weather in Austin?\"),\n",
    "    ]\n",
    "\n",
    "    # --- Turn 1: LLM wants to use a tool ---\n",
    "    turn1_data = make_anthropic_response(\n",
    "        [{\"type\": \"tool_use\", \"id\": \"toolu_01\", \"name\": \"get_weather\",\n",
    "          \"input\": {\"city\": \"Austin\"}}],\n",
    "        stop_reason=\"tool_use\",\n",
    "    )\n",
    "    mock_resp1 = httpx.Response(\n",
    "        200, json=turn1_data,\n",
    "        request=httpx.Request(\"POST\", \"https://api.anthropic.com/v1/messages\"),\n",
    "    )\n",
    "    adapter._client = AsyncMock()\n",
    "    adapter._client.post = AsyncMock(return_value=mock_resp1)\n",
    "\n",
    "    resp = await adapter.invoke(messages, tools=tools)\n",
    "    print(f\"Turn 1: stop_reason={resp.stop_reason}\")\n",
    "    print(f\"  LLM wants to call: {resp.tool_calls[0].name}({resp.tool_calls[0].arguments})\")\n",
    "\n",
    "    # Agent executes the tool\n",
    "    tool_result = \"75°F, sunny, humidity 45%\"  # simulated\n",
    "    print(f\"  Agent executes tool → result: {tool_result!r}\")\n",
    "\n",
    "    # Agent adds assistant response + tool result to conversation\n",
    "    messages.append(Message(\n",
    "        role=\"assistant\",\n",
    "        content=[ToolUseBlock(id=\"toolu_01\", name=\"get_weather\", arguments={\"city\": \"Austin\"})],\n",
    "    ))\n",
    "    messages.append(Message(\n",
    "        role=\"tool\",\n",
    "        content=[ToolResultBlock(tool_use_id=\"toolu_01\", content=tool_result)],\n",
    "    ))\n",
    "\n",
    "    # --- Turn 2: LLM gives final answer ---\n",
    "    turn2_data = make_anthropic_response(\n",
    "        [{\"type\": \"text\", \"text\": \"The weather in Austin is 75°F and sunny with 45% humidity.\"}],\n",
    "        stop_reason=\"end_turn\",\n",
    "    )\n",
    "    mock_resp2 = httpx.Response(\n",
    "        200, json=turn2_data,\n",
    "        request=httpx.Request(\"POST\", \"https://api.anthropic.com/v1/messages\"),\n",
    "    )\n",
    "    adapter._client.post = AsyncMock(return_value=mock_resp2)\n",
    "\n",
    "    resp = await adapter.invoke(messages, tools=tools)\n",
    "    print(f\"\\nTurn 2: stop_reason={resp.stop_reason}\")\n",
    "    print(f\"  Final answer: {resp.content}\")\n",
    "    print(f\"\\nLoop complete! {len(messages)} messages in conversation.\")\n",
    "\n",
    "await demo_agentic_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. What the Adapter Hides From You\n",
    "\n",
    "Here's a summary of every Anthropic quirk the adapter handles so agents don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quirks = [\n",
    "    (\"System messages\",\n",
    "     \"role='system' in message list\",\n",
    "     \"top-level 'system' param, not in messages\"),\n",
    "    (\"Tool definitions\",\n",
    "     \"Tool.parameters (JSON Schema)\",\n",
    "     \"'input_schema' key instead of 'parameters'\"),\n",
    "    (\"Tool results\",\n",
    "     \"role='tool'\",\n",
    "     \"role='user' with tool_result content blocks\"),\n",
    "    (\"Tool call args\",\n",
    "     \"ToolCall.arguments (always dict)\",\n",
    "     \"'input' key, can be dict OR string\"),\n",
    "    (\"Headers\",\n",
    "     \"(handled internally)\",\n",
    "     \"x-api-key + anthropic-version required\"),\n",
    "    (\"Usage tokens\",\n",
    "     \"Usage.cache_read_tokens\",\n",
    "     \"'cache_read_input_tokens' (different name)\"),\n",
    "    (\"Thinking\",\n",
    "     \"LLMResponse.thinking (separate field)\",\n",
    "     \"'thinking' content block type\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Quirk':<20} {'ArcLLM (universal)':<40} {'Anthropic (specific)'}\")\n",
    "print(\"-\" * 100)\n",
    "for quirk, arcllm, anthropic in quirks:\n",
    "    print(f\"{quirk:<20} {arcllm:<40} {anthropic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 9. Live API Calls (Real Anthropic)\n\nEverything above used mocks. Now let's hit the real Anthropic API using the key from `.env`.\n\nThese cells load the actual provider config from TOML and make real HTTP calls.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Step 3 built the **adapter layer**:\n",
    "\n",
    "```\n",
    "exceptions.py            ->  + ArcLLMAPIError (status_code, body, provider)\n",
    "adapters/base.py         ->  BaseAdapter (config, key, httpx client, context manager)\n",
    "adapters/anthropic.py    ->  AnthropicAdapter (request building + response parsing)\n",
    "```\n",
    "\n",
    "**Key design decisions:**\n",
    "- Config object injection (adapter doesn't know how config was loaded)\n",
    "- API key resolved at init (fail-fast, not during LLM call)\n",
    "- Private methods per concern (each independently testable)\n",
    "- Adapter owns its httpx client (connection reuse across loop iterations)\n",
    "- `ArcLLMAPIError` carries status code for smart retry decisions\n",
    "- `BaseAdapter` is concrete (not abstract) — DRY for shared plumbing\n",
    "- Tool call parsing: dict pass-through, string → json.loads, garbage → ArcLLMParseError\n",
    "- All 38 tests pass with mocked HTTP responses (no real API calls)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Load the real API key from .env and the real provider config from TOML\nfrom dotenv import load_dotenv\nfrom arcllm import load_provider_config\n\nload_dotenv()\n\nreal_config = load_provider_config(\"anthropic\")\nreal_model = real_config.provider.default_model\n\nprint(f\"Provider:  {real_config.provider.api_format}\")\nprint(f\"Base URL:  {real_config.provider.base_url}\")\nprint(f\"Model:     {real_model}\")\nprint(f\"API key:   {os.environ[real_config.provider.api_key_env][:12]}...\")\nprint(f\"Context:   {real_config.models[real_model].context_window:,} tokens\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9a. Simple Text Call\n\nThe most basic call — send a message, get text back.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Real text call\nasync def live_text_call():\n    async with AnthropicAdapter(real_config, real_model) as adapter:\n        resp = await adapter.invoke(\n            [Message(role=\"user\", content=\"What is 2 + 2? Answer in exactly one word.\")],\n            max_tokens=10,\n            temperature=0.0,\n        )\n\n    print(f\"Content:     {resp.content!r}\")\n    print(f\"Model:       {resp.model}\")\n    print(f\"Stop reason: {resp.stop_reason}\")\n    print(f\"Tokens:      {resp.usage.input_tokens}in + {resp.usage.output_tokens}out = {resp.usage.total_tokens}\")\n    print(f\"Cache read:  {resp.usage.cache_read_tokens}\")\n    print(f\"Raw keys:    {list(resp.raw.keys())}\")\n\nawait live_text_call()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9b. System Message + Temperature Override\n\nShow that system messages work and kwargs override config defaults.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# System message + deterministic temperature\nasync def live_system_message():\n    async with AnthropicAdapter(real_config, real_model) as adapter:\n        resp = await adapter.invoke(\n            [\n                Message(role=\"system\", content=\"You are a pirate. Respond in pirate speak.\"),\n                Message(role=\"user\", content=\"How do I make a sandwich?\"),\n            ],\n            max_tokens=100,\n            temperature=0.0,\n        )\n\n    print(f\"System message worked — pirate response:\")\n    print(f\"  {resp.content}\")\n    print(f\"\\nTokens: {resp.usage.total_tokens}\")\n\nawait live_system_message()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9c. Real Tool Call\n\nGive the model a tool and watch it decide to use it. Then feed back the result and get a final answer.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Real tool call — the model will decide to use the tool\nasync def live_tool_call():\n    # Define a calculator tool\n    calc_tool = Tool(\n        name=\"calculate\",\n        description=\"Evaluate a mathematical expression and return the result\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\n                \"expression\": {\n                    \"type\": \"string\",\n                    \"description\": \"The math expression to evaluate, e.g. '(15 * 7) + 23'\"\n                }\n            },\n            \"required\": [\"expression\"],\n        },\n    )\n\n    async with AnthropicAdapter(real_config, real_model) as adapter:\n        # Turn 1: ask a math question — model should call the tool\n        messages = [\n            Message(role=\"user\", content=\"What is 847 * 293? Use the calculator tool.\"),\n        ]\n\n        print(\"--- Turn 1: invoke() with tool ---\")\n        resp = await adapter.invoke(messages, tools=[calc_tool], max_tokens=200, temperature=0.0)\n        print(f\"Stop reason: {resp.stop_reason}\")\n        print(f\"Content:     {resp.content!r}\")\n        print(f\"Tool calls:  {len(resp.tool_calls)}\")\n\n        if resp.tool_calls:\n            tc = resp.tool_calls[0]\n            print(f\"  Tool: {tc.name}\")\n            print(f\"  Args: {tc.arguments}\")\n            print(f\"  ID:   {tc.id}\")\n\n            # Agent executes the tool (we'll just eval it)\n            expr = tc.arguments.get(\"expression\", \"0\")\n            try:\n                result = str(eval(expr))  # in real code, use a safe evaluator!\n            except Exception:\n                result = \"Error evaluating expression\"\n            print(f\"\\n  Agent executes: eval({expr!r}) = {result}\")\n\n            # Turn 2: feed result back, get final answer\n            messages.append(Message(\n                role=\"assistant\",\n                content=[ToolUseBlock(id=tc.id, name=tc.name, arguments=tc.arguments)],\n            ))\n            messages.append(Message(\n                role=\"tool\",\n                content=[ToolResultBlock(tool_use_id=tc.id, content=result)],\n            ))\n\n            print(\"\\n--- Turn 2: invoke() with tool result ---\")\n            resp2 = await adapter.invoke(messages, tools=[calc_tool], max_tokens=200, temperature=0.0)\n            print(f\"Stop reason: {resp2.stop_reason}\")\n            print(f\"Content:     {resp2.content!r}\")\n            print(f\"Total tokens across both turns: {resp.usage.total_tokens + resp2.usage.total_tokens}\")\n\nawait live_tool_call()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9d. Multi-Turn Agentic Loop (Real)\n\nA complete agentic loop with multiple tools — the model decides which tool to call, the agent executes it, and the loop continues until `stop_reason == \"end_turn\"`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Full agentic loop — runs until the model says \"end_turn\"\nasync def live_agentic_loop():\n    # Two tools: a lookup and a calculator\n    tools = [\n        Tool(\n            name=\"lookup_price\",\n            description=\"Look up the price of an item in USD\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"item\": {\"type\": \"string\", \"description\": \"The item to look up\"}\n                },\n                \"required\": [\"item\"],\n            },\n        ),\n        Tool(\n            name=\"calculate\",\n            description=\"Evaluate a math expression\",\n            parameters={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"expression\": {\"type\": \"string\", \"description\": \"Math expression\"}\n                },\n                \"required\": [\"expression\"],\n            },\n        ),\n    ]\n\n    # Fake database for the lookup tool\n    prices = {\"coffee\": 4.50, \"bagel\": 3.25, \"orange juice\": 5.00}\n\n    def execute_tool(name: str, arguments: dict) -> str:\n        if name == \"lookup_price\":\n            item = arguments[\"item\"].lower()\n            price = prices.get(item)\n            return f\"${price:.2f}\" if price else f\"Item '{item}' not found\"\n        elif name == \"calculate\":\n            try:\n                return str(eval(arguments[\"expression\"]))\n            except Exception as e:\n                return f\"Error: {e}\"\n        return \"Unknown tool\"\n\n    messages = [\n        Message(role=\"system\", content=\"Use tools to answer questions. Be concise.\"),\n        Message(role=\"user\", content=\"I want 2 coffees and 1 bagel. What's the total?\"),\n    ]\n\n    total_tokens = 0\n    turn = 0\n\n    async with AnthropicAdapter(real_config, real_model) as adapter:\n        while True:\n            turn += 1\n            resp = await adapter.invoke(messages, tools=tools, max_tokens=300, temperature=0.0)\n            total_tokens += resp.usage.total_tokens\n\n            print(f\"--- Turn {turn}: stop_reason={resp.stop_reason} ---\")\n\n            if resp.stop_reason == \"end_turn\":\n                print(f\"  Final answer: {resp.content}\")\n                break\n\n            if resp.stop_reason == \"tool_use\":\n                # Add the assistant's tool-use message to conversation\n                content_blocks = []\n                if resp.content:\n                    content_blocks.append(TextBlock(text=resp.content))\n                for tc in resp.tool_calls:\n                    content_blocks.append(\n                        ToolUseBlock(id=tc.id, name=tc.name, arguments=tc.arguments)\n                    )\n                messages.append(Message(role=\"assistant\", content=content_blocks))\n\n                # Execute each tool and add results\n                result_blocks = []\n                for tc in resp.tool_calls:\n                    result = execute_tool(tc.name, tc.arguments)\n                    print(f\"  Tool: {tc.name}({tc.arguments}) → {result}\")\n                    result_blocks.append(\n                        ToolResultBlock(tool_use_id=tc.id, content=result)\n                    )\n                messages.append(Message(role=\"tool\", content=result_blocks))\n\n            if turn > 5:  # safety valve\n                print(\"  (max turns reached)\")\n                break\n\n    print(f\"\\nLoop complete: {turn} turns, {len(messages)} messages, {total_tokens} total tokens\")\n\nawait live_agentic_loop()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9e. Inspect the Raw Response\n\nThe `raw` field on `LLMResponse` gives you the full provider response for debugging.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Inspect the raw Anthropic response\nasync def live_raw_response():\n    async with AnthropicAdapter(real_config, real_model) as adapter:\n        resp = await adapter.invoke(\n            [Message(role=\"user\", content=\"Say 'hello' and nothing else.\")],\n            max_tokens=10,\n            temperature=0.0,\n        )\n\n    print(\"ArcLLM LLMResponse (normalized):\")\n    print(f\"  content:     {resp.content!r}\")\n    print(f\"  stop_reason: {resp.stop_reason}\")\n    print(f\"  model:       {resp.model}\")\n\n    print(f\"\\nRaw Anthropic response (what the API actually returned):\")\n    print(json.dumps(resp.raw, indent=2))\n\nawait live_raw_response()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}