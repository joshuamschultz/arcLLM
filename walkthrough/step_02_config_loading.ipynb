{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArcLLM Step 2: Config Loading\n",
    "\n",
    "This notebook walks through everything built in Step 2 — the **config layer** that drives all behavior via TOML files.\n",
    "\n",
    "**What was built:**\n",
    "- Global `config.toml` (defaults + module toggles)\n",
    "- Per-provider TOML files (`providers/anthropic.toml`, `providers/openai.toml`)\n",
    "- Pydantic config models (`GlobalConfig`, `ProviderConfig`, `ModelMetadata`, etc.)\n",
    "- `load_global_config()` and `load_provider_config()` loaders\n",
    "- Path traversal prevention (NIST 800-53 AC-3)\n",
    "\n",
    "**Why it matters:** Config-driven means agents can switch providers, adjust parameters, and toggle modules without changing code. And validated-on-load means misconfiguration crashes at startup, not during a critical LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm import (\n",
    "    load_global_config, load_provider_config,\n",
    "    GlobalConfig, ProviderConfig,\n",
    "    DefaultsConfig, ProviderSettings, ModelMetadata, ModuleConfig,\n",
    "    ArcLLMConfigError,\n",
    ")\n",
    "import json\n",
    "print(\"All config imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Global Config\n",
    "\n",
    "The global `config.toml` lives at `src/arcllm/config.toml`. It holds:\n",
    "- **Defaults**: provider, temperature, max_tokens\n",
    "- **Module toggles**: which optional features are enabled\n",
    "\n",
    "Let's load it and explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load global config — validated on load, fails fast on any issue\n",
    "global_config = load_global_config()\n",
    "print(f\"Type: {type(global_config).__name__}\")\n",
    "print(f\"\\nDefaults:\")\n",
    "print(f\"  Provider:    {global_config.defaults.provider}\")\n",
    "print(f\"  Temperature: {global_config.defaults.temperature}\")\n",
    "print(f\"  Max tokens:  {global_config.defaults.max_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore modules — all disabled by default (opt-in complexity)\n",
    "print(f\"Modules ({len(global_config.modules)}):\")\n",
    "for name, module in global_config.modules.items():\n",
    "    print(f\"  {name:15s}  enabled={module.enabled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules can have extra fields — these are preserved via Pydantic's extra='allow'\n",
    "# This lets each module define its own settings without changing the config schema\n",
    "budget = global_config.modules[\"budget\"]\n",
    "print(f\"Budget module:\")\n",
    "print(f\"  enabled: {budget.enabled}\")\n",
    "print(f\"  monthly_limit_usd: {budget.monthly_limit_usd}\")\n",
    "\n",
    "retry = global_config.modules[\"retry\"]\n",
    "print(f\"\\nRetry module:\")\n",
    "print(f\"  enabled: {retry.enabled}\")\n",
    "print(f\"  max_retries: {retry.max_retries}\")\n",
    "print(f\"  backoff_base_seconds: {retry.backoff_base_seconds}\")\n",
    "\n",
    "fallback = global_config.modules[\"fallback\"]\n",
    "print(f\"\\nFallback module:\")\n",
    "print(f\"  enabled: {fallback.enabled}\")\n",
    "print(f\"  chain: {fallback.chain}\")\n",
    "\n",
    "rate_limit = global_config.modules[\"rate_limit\"]\n",
    "print(f\"\\nRate limit module:\")\n",
    "print(f\"  enabled: {rate_limit.enabled}\")\n",
    "print(f\"  requests_per_minute: {rate_limit.requests_per_minute}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dump to see everything at once\n",
    "print(json.dumps(global_config.model_dump(), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Provider Config\n",
    "\n",
    "Each provider has its own TOML file under `src/arcllm/providers/`.\n",
    "\n",
    "A provider config has:\n",
    "- **`[provider]`** section — connection settings (URL, API key env var, defaults)\n",
    "- **`[models.*]`** sections — per-model metadata (context window, costs, capabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Anthropic provider config\n",
    "anthropic = load_provider_config(\"anthropic\")\n",
    "print(f\"Type: {type(anthropic).__name__}\")\n",
    "print(f\"\\nProvider settings:\")\n",
    "print(f\"  API format:   {anthropic.provider.api_format}\")\n",
    "print(f\"  Base URL:     {anthropic.provider.base_url}\")\n",
    "print(f\"  API key env:  {anthropic.provider.api_key_env}\")\n",
    "print(f\"  Default model: {anthropic.provider.default_model}\")\n",
    "print(f\"  Default temp:  {anthropic.provider.default_temperature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore model metadata\n",
    "print(f\"Available models ({len(anthropic.models)}):\")\n",
    "for model_name, meta in anthropic.models.items():\n",
    "    print(f\"\\n  {model_name}:\")\n",
    "    print(f\"    Context window:   {meta.context_window:,} tokens\")\n",
    "    print(f\"    Max output:       {meta.max_output_tokens:,} tokens\")\n",
    "    print(f\"    Supports tools:   {meta.supports_tools}\")\n",
    "    print(f\"    Supports vision:  {meta.supports_vision}\")\n",
    "    print(f\"    Supports thinking:{meta.supports_thinking}\")\n",
    "    print(f\"    Input modalities: {meta.input_modalities}\")\n",
    "    print(f\"    Cost (input):     ${meta.cost_input_per_1m}/1M tokens\")\n",
    "    print(f\"    Cost (output):    ${meta.cost_output_per_1m}/1M tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with OpenAI provider\n",
    "openai = load_provider_config(\"openai\")\n",
    "print(f\"OpenAI provider settings:\")\n",
    "print(f\"  API format:    {openai.provider.api_format}\")\n",
    "print(f\"  Base URL:      {openai.provider.base_url}\")\n",
    "print(f\"  API key env:   {openai.provider.api_key_env}\")\n",
    "print(f\"  Default model: {openai.provider.default_model}\")\n",
    "\n",
    "print(f\"\\nModels:\")\n",
    "for model_name, meta in openai.models.items():\n",
    "    print(f\"  {model_name}: {meta.context_window:,} context, ${meta.cost_input_per_1m}/1M in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Providers\n",
    "\n",
    "Because everything is normalized to the same types, you can compare across providers easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Side-by-side model comparison\nprint(f\"{'Model':<35} {'Context':>10} {'Max Out':>10} {'$/1M In':>10} {'$/1M Out':>10} {'Tools':>6}\")\nprint(\"-\" * 85)\n\nfor provider_name, provider_config in [(\"anthropic\", anthropic), (\"openai\", openai)]:\n    for model_name, meta in provider_config.models.items():\n        tools_str = \"yes\" if meta.supports_tools else \"no\"\n        print(\n            f\"{model_name:<35} \"\n            f\"{meta.context_window:>10,} \"\n            f\"{meta.max_output_tokens:>10,} \"\n            f\"${meta.cost_input_per_1m:>8.2f} \"\n            f\"${meta.cost_output_per_1m:>8.2f} \"\n            f\"{tools_str:>6}\"\n        )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Using Model Metadata\n",
    "\n",
    "Model metadata isn't just informational — agents use it to make decisions:\n",
    "- Can this model use tools? Check `supports_tools`\n",
    "- Can I send images? Check `supports_vision` and `input_modalities`\n",
    "- How much will this cost? Check `cost_*_per_1m`\n",
    "- Will my prompt fit? Check `context_window`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate: pick the cheapest model that supports tools\n",
    "all_models = []\n",
    "for provider_name, config in [(\"anthropic\", anthropic), (\"openai\", openai)]:\n",
    "    for model_name, meta in config.models.items():\n",
    "        all_models.append((provider_name, model_name, meta))\n",
    "\n",
    "tool_capable = [(p, m, meta) for p, m, meta in all_models if meta.supports_tools]\n",
    "cheapest = min(tool_capable, key=lambda x: x[2].cost_input_per_1m)\n",
    "\n",
    "print(f\"Cheapest tool-capable model:\")\n",
    "print(f\"  Provider: {cheapest[0]}\")\n",
    "print(f\"  Model:    {cheapest[1]}\")\n",
    "print(f\"  Cost:     ${cheapest[2].cost_input_per_1m}/1M input tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate: check if a prompt fits\n",
    "sonnet = anthropic.models[\"claude-sonnet-4-20250514\"]\n",
    "\n",
    "estimated_tokens = 150_000  # big prompt\n",
    "if estimated_tokens < sonnet.context_window:\n",
    "    remaining = sonnet.context_window - estimated_tokens\n",
    "    print(f\"Prompt fits! {estimated_tokens:,} / {sonnet.context_window:,} tokens used\")\n",
    "    print(f\"Room for {remaining:,} more tokens\")\n",
    "else:\n",
    "    print(f\"Prompt too large! {estimated_tokens:,} > {sonnet.context_window:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate: estimate cost for a batch of calls\n",
    "n_calls = 1000\n",
    "avg_input = 2000   # tokens per call\n",
    "avg_output = 500   # tokens per call\n",
    "\n",
    "model_meta = anthropic.models[\"claude-sonnet-4-20250514\"]\n",
    "total_input = n_calls * avg_input\n",
    "total_output = n_calls * avg_output\n",
    "\n",
    "input_cost = (total_input / 1_000_000) * model_meta.cost_input_per_1m\n",
    "output_cost = (total_output / 1_000_000) * model_meta.cost_output_per_1m\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "print(f\"Cost estimate for {n_calls:,} calls:\")\n",
    "print(f\"  Input:  {total_input:>10,} tokens x ${model_meta.cost_input_per_1m}/1M = ${input_cost:.2f}\")\n",
    "print(f\"  Output: {total_output:>10,} tokens x ${model_meta.cost_output_per_1m}/1M = ${output_cost:.2f}\")\n",
    "print(f\"  Total:  ${total_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Security: Path Traversal Prevention\n",
    "\n",
    "Because `load_provider_config()` takes a string and turns it into a file path, we MUST validate it. Otherwise an attacker could pass `../../etc/passwd` as a provider name.\n",
    "\n",
    "**Decision D-035**: Strict regex validation — `^[a-z][a-z0-9\\-]*$`, max 64 characters.\n",
    "\n",
    "This satisfies NIST 800-53 AC-3 (access enforcement) and OWASP A01 (broken access control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path traversal attack — BLOCKED\n",
    "try:\n",
    "    load_provider_config(\"../../etc/passwd\")\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"BLOCKED: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various attack vectors — all blocked\n",
    "attacks = [\n",
    "    (\"../evil\", \"parent directory traversal\"),\n",
    "    (\"pro/vider\", \"slash in name\"),\n",
    "    (\".hidden\", \"dot-prefixed\"),\n",
    "    (\"a@b\", \"special characters\"),\n",
    "    (\"a b\", \"spaces\"),\n",
    "    (\"\", \"empty string\"),\n",
    "    (\"Anthropic\", \"uppercase (case-sensitive match)\"),\n",
    "    (\"a\" * 65, \"name too long (>64 chars)\"),\n",
    "]\n",
    "\n",
    "for attack, description in attacks:\n",
    "    try:\n",
    "        load_provider_config(attack)\n",
    "        print(f\"  FAIL - {description}: should have been blocked!\")\n",
    "    except ArcLLMConfigError:\n",
    "        display_name = attack if len(attack) <= 30 else attack[:27] + \"...\"\n",
    "        print(f\"  BLOCKED - {description}: {display_name!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid provider names that DO work\n",
    "from arcllm.config import _validate_provider_name\n",
    "\n",
    "valid_names = [\"anthropic\", \"openai\", \"ollama\", \"my-custom-provider\", \"o1\", \"gpt4\"]\n",
    "for name in valid_names:\n",
    "    try:\n",
    "        _validate_provider_name(name)\n",
    "        print(f\"  VALID: {name!r}\")\n",
    "    except ArcLLMConfigError as e:\n",
    "        print(f\"  REJECTED: {name!r} - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Error Handling: Fail-Fast on Bad Config\n",
    "\n",
    "Config errors are caught at load time, not during an LLM call. This is critical for unattended agents — you want to crash at startup, not 3 hours into a batch job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing provider file\n",
    "try:\n",
    "    load_provider_config(\"nonexistent\")\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"Missing file caught: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malformed TOML (simulated with unittest.mock)\n",
    "from unittest.mock import patch\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    tmp_path = Path(tmp)\n",
    "\n",
    "    # Write a broken TOML file\n",
    "    bad_toml = tmp_path / \"config.toml\"\n",
    "    bad_toml.write_text(\"[unclosed section\")\n",
    "\n",
    "    with patch(\"arcllm.config._get_config_dir\", return_value=tmp_path):\n",
    "        try:\n",
    "            load_global_config()\n",
    "        except ArcLLMConfigError as e:\n",
    "            print(f\"Malformed TOML caught: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong types in TOML (temperature should be float, not string)\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    tmp_path = Path(tmp)\n",
    "\n",
    "    bad_types = tmp_path / \"config.toml\"\n",
    "    bad_types.write_text('[defaults]\\nprovider = \"ok\"\\ntemperature = \"not-a-float\"\\n')\n",
    "\n",
    "    with patch(\"arcllm.config._get_config_dir\", return_value=tmp_path):\n",
    "        try:\n",
    "            load_global_config()\n",
    "        except ArcLLMConfigError as e:\n",
    "            print(f\"Type validation caught: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All config errors are ArcLLMConfigError -> ArcLLMError\n",
    "# So you can catch them at whatever granularity you need\n",
    "try:\n",
    "    load_provider_config(\"nonexistent\")\n",
    "except ArcLLMConfigError:\n",
    "    print(\"Caught with ArcLLMConfigError (specific)\")\n",
    "\n",
    "try:\n",
    "    load_provider_config(\"nonexistent\")\n",
    "except Exception as e:\n",
    "    from arcllm import ArcLLMError\n",
    "    print(f\"Is ArcLLMError? {isinstance(e, ArcLLMError)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Config Models in Detail\n",
    "\n",
    "Let's look at each Pydantic config model and how they compose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DefaultsConfig — has sensible defaults for every field\n",
    "# You can create one with no arguments and get reasonable values\n",
    "defaults_empty = DefaultsConfig()\n",
    "print(f\"DefaultsConfig (no args):\")\n",
    "print(f\"  provider: {defaults_empty.provider}\")\n",
    "print(f\"  temperature: {defaults_empty.temperature}\")\n",
    "print(f\"  max_tokens: {defaults_empty.max_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModuleConfig — extra='allow' is the magic\n",
    "# It accepts any extra fields without complaining\n",
    "basic = ModuleConfig(enabled=True)\n",
    "print(f\"Basic: enabled={basic.enabled}\")\n",
    "\n",
    "# With extra fields (module-specific settings)\n",
    "budget_mod = ModuleConfig(enabled=True, monthly_limit_usd=1000.0, alert_threshold=0.8)\n",
    "print(f\"Budget: enabled={budget_mod.enabled}, limit=${budget_mod.monthly_limit_usd}, alert={budget_mod.alert_threshold}\")\n",
    "\n",
    "# The extra fields show up in model_dump too\n",
    "print(f\"Dump: {budget_mod.model_dump()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelMetadata — everything you need to know about a model\n",
    "# All fields are required (no guessing in production)\n",
    "from pydantic import ValidationError\n",
    "\n",
    "try:\n",
    "    ModelMetadata(context_window=100000)  # missing required fields\n",
    "except ValidationError as e:\n",
    "    print(f\"Missing fields caught ({e.error_count()} errors):\")\n",
    "    for err in e.errors():\n",
    "        print(f\"  Missing: {err['loc'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProviderSettings — connection info for making API calls\n",
    "# Note: api_key_env is the ENV VAR NAME, never the actual key\n",
    "ps = anthropic.provider\n",
    "print(f\"To connect to Anthropic:\")\n",
    "print(f\"  POST to: {ps.base_url}\")\n",
    "print(f\"  Format:  {ps.api_format}\")\n",
    "print(f\"  API key from: os.environ['{ps.api_key_env}']\")\n",
    "print(f\"  Default model: {ps.default_model}\")\n",
    "print(f\"\\nThe key itself is NEVER in config files — security first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Config Discovery\n",
    "\n",
    "Config files are discovered **package-relative** via `Path(__file__).parent`.\n",
    "\n",
    "This means configs work in both:\n",
    "- Development mode (`pip install -e .`)\n",
    "- Installed mode (`pip install arcllm`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.config import _get_config_dir\n",
    "\n",
    "config_dir = _get_config_dir()\n",
    "print(f\"Config directory: {config_dir}\")\n",
    "print(f\"\\nFiles found:\")\n",
    "for f in sorted(config_dir.rglob(\"*.toml\")):\n",
    "    print(f\"  {f.relative_to(config_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the raw TOML to see what the loader parses\n",
    "import tomllib\n",
    "\n",
    "config_path = config_dir / \"config.toml\"\n",
    "with open(config_path, \"rb\") as f:\n",
    "    raw_data = tomllib.load(f)\n",
    "\n",
    "print(\"Raw config.toml data:\")\n",
    "print(json.dumps(raw_data, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a provider TOML\n",
    "provider_path = config_dir / \"providers\" / \"anthropic.toml\"\n",
    "with open(provider_path, \"rb\") as f:\n",
    "    raw_provider = tomllib.load(f)\n",
    "\n",
    "print(\"Raw anthropic.toml data:\")\n",
    "print(json.dumps(raw_provider, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Switching Providers\n",
    "\n",
    "One of the key benefits of config-driven design: switching providers is just loading a different TOML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both and compare\n",
    "providers = {}\n",
    "for name in [\"anthropic\", \"openai\"]:\n",
    "    providers[name] = load_provider_config(name)\n",
    "\n",
    "print(f\"{'Setting':<20} {'Anthropic':<30} {'OpenAI':<30}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'API format':<20} {providers['anthropic'].provider.api_format:<30} {providers['openai'].provider.api_format:<30}\")\n",
    "print(f\"{'Base URL':<20} {providers['anthropic'].provider.base_url:<30} {providers['openai'].provider.base_url:<30}\")\n",
    "print(f\"{'API key env':<20} {providers['anthropic'].provider.api_key_env:<30} {providers['openai'].provider.api_key_env:<30}\")\n",
    "print(f\"{'Default model':<20} {providers['anthropic'].provider.default_model:<30} {providers['openai'].provider.default_model:<30}\")\n",
    "print(f\"{'# of models':<20} {len(providers['anthropic'].models):<30} {len(providers['openai'].models):<30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern: agent selects provider based on global defaults\n",
    "global_cfg = load_global_config()\n",
    "default_provider = global_cfg.defaults.provider\n",
    "\n",
    "provider_cfg = load_provider_config(default_provider)\n",
    "default_model = provider_cfg.provider.default_model\n",
    "model_meta = provider_cfg.models[default_model]\n",
    "\n",
    "print(f\"Default provider: {default_provider}\")\n",
    "print(f\"Default model:    {default_model}\")\n",
    "print(f\"Context window:   {model_meta.context_window:,} tokens\")\n",
    "print(f\"Supports tools:   {model_meta.supports_tools}\")\n",
    "print(f\"\\nThis is exactly what load_model() will do in Step 6.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Override Chain Preview\n",
    "\n",
    "The merge strategy (Decision D-033) is: **args > provider TOML > global TOML**\n",
    "\n",
    "This isn't implemented yet (it will be in `load_model()`), but here's how it will work conceptually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview: how overrides will chain\n",
    "global_temp = global_cfg.defaults.temperature        # 0.7 (from config.toml)\n",
    "provider_temp = provider_cfg.provider.default_temperature  # 0.7 (from anthropic.toml)\n",
    "user_temp = 0.0  # agent passes temperature=0 for deterministic output\n",
    "\n",
    "print(f\"Override chain for 'temperature':\")\n",
    "print(f\"  Global default:   {global_temp}\")\n",
    "print(f\"  Provider default: {provider_temp}  (overrides global)\")\n",
    "print(f\"  User override:    {user_temp}  (overrides everything)\")\n",
    "print(f\"  Final value:      {user_temp}  (last writer wins)\")\n",
    "print(f\"\\nImplemented in load_model() (Step 6).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Putting It Together: What an Agent Startup Looks Like\n",
    "\n",
    "Here's the pattern agents will use once everything is wired up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_startup_preview():\n",
    "    \"\"\"What agent initialization will look like.\"\"\"\n",
    "    # 1. Load global config\n",
    "    global_cfg = load_global_config()\n",
    "    print(f\"1. Global config loaded\")\n",
    "    print(f\"   Default provider: {global_cfg.defaults.provider}\")\n",
    "    print(f\"   Modules: {', '.join(m for m, c in global_cfg.modules.items() if c.enabled) or 'none enabled'}\")\n",
    "\n",
    "    # 2. Load provider config\n",
    "    provider_name = global_cfg.defaults.provider\n",
    "    provider_cfg = load_provider_config(provider_name)\n",
    "    print(f\"\\n2. Provider config loaded: {provider_name}\")\n",
    "    print(f\"   Base URL: {provider_cfg.provider.base_url}\")\n",
    "    print(f\"   API key from: ${provider_cfg.provider.api_key_env}\")\n",
    "\n",
    "    # 3. Select model\n",
    "    model_name = provider_cfg.provider.default_model\n",
    "    model_meta = provider_cfg.models[model_name]\n",
    "    print(f\"\\n3. Model selected: {model_name}\")\n",
    "    print(f\"   Context: {model_meta.context_window:,} tokens\")\n",
    "    print(f\"   Tools:   {model_meta.supports_tools}\")\n",
    "    print(f\"   Vision:  {model_meta.supports_vision}\")\n",
    "\n",
    "    # 4. Check API key exists (security: env vars only)\n",
    "    import os\n",
    "    key_name = provider_cfg.provider.api_key_env\n",
    "    has_key = key_name in os.environ\n",
    "    print(f\"\\n4. API key check: {'FOUND' if has_key else 'MISSING'} (${key_name})\")\n",
    "\n",
    "    # 5. Ready (Step 6 will return an LLMProvider here)\n",
    "    print(f\"\\n5. Agent ready! (load_model() will return provider in Step 6)\")\n",
    "\n",
    "agent_startup_preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Step 2 built the **config layer**:\n",
    "\n",
    "```\n",
    "config.toml          ->  GlobalConfig (defaults + module toggles)\n",
    "providers/*.toml      ->  ProviderConfig (connection settings + model metadata)\n",
    "config.py            ->  Pydantic models + loaders + path traversal prevention\n",
    "```\n",
    "\n",
    "**Key design decisions:**\n",
    "- TOML format (stdlib `tomllib`, zero dependencies)\n",
    "- Pydantic validation on load (fail-fast for agents)\n",
    "- Package-relative file discovery (`Path(__file__).parent`)\n",
    "- Simple override chain: args > provider > global\n",
    "- Strict regex on provider names (path traversal prevention)\n",
    "- `extra='allow'` on ModuleConfig (extensible without schema changes)\n",
    "- API keys referenced by env var name, never stored in config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}