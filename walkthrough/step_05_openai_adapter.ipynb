{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ArcLLM Step 5: OpenAI Adapter + StopReason Normalization\n",
    "\n",
    "This notebook walks through everything built in Step 5 \u2014 the **OpenAI adapter** that translates between ArcLLM's universal types and OpenAI's Chat Completions API, plus the **StopReason** type that normalizes stop reasons across providers.\n",
    "\n",
    "**What was built:**\n",
    "- `StopReason` \u2014 a `Literal` type that normalizes stop reasons across all providers\n",
    "- `OpenaiAdapter` \u2014 request building + response parsing for OpenAI's Chat Completions API\n",
    "- Stop reason mapping: OpenAI `finish_reason` \u2192 canonical `StopReason`\n",
    "\n",
    "**Why it matters:** With two adapters, the abstraction proves itself. An agent writes the same code regardless of whether it's talking to Anthropic or OpenAI \u2014 different auth, different message formats, different tool wrapping, different stop reasons \u2014 all hidden by the adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from unittest.mock import AsyncMock\n",
    "import httpx\n",
    "\n",
    "from arcllm import (\n",
    "    AnthropicAdapter, OpenaiAdapter, BaseAdapter,\n",
    "    ArcLLMAPIError, ArcLLMConfigError, ArcLLMError, ArcLLMParseError,\n",
    "    ProviderConfig, ProviderSettings, ModelMetadata,\n",
    "    Message, TextBlock, ToolUseBlock, ToolResultBlock,\n",
    "    Tool, ToolCall, Usage, LLMResponse, StopReason,\n",
    ")\n",
    "print(\"All imports successful \u2014 including StopReason and OpenaiAdapter!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: fake OpenAI config for mocked tests\n",
    "os.environ[\"ARCLLM_TEST_KEY\"] = \"sk-test-openai-key-for-walkthrough\"\n",
    "\n",
    "FAKE_MODEL = \"gpt-4o-test\"\n",
    "\n",
    "fake_config = ProviderConfig(\n",
    "    provider=ProviderSettings(\n",
    "        api_format=\"openai-chat\",\n",
    "        base_url=\"https://api.openai.com\",\n",
    "        api_key_env=\"ARCLLM_TEST_KEY\",\n",
    "        default_model=FAKE_MODEL,\n",
    "        default_temperature=0.7,\n",
    "    ),\n",
    "    models={\n",
    "        FAKE_MODEL: ModelMetadata(\n",
    "            context_window=128000,\n",
    "            max_output_tokens=16384,\n",
    "            supports_tools=True,\n",
    "            supports_vision=True,\n",
    "            supports_thinking=False,\n",
    "            input_modalities=[\"text\", \"image\"],\n",
    "            cost_input_per_1m=2.50,\n",
    "            cost_output_per_1m=10.00,\n",
    "            cost_cache_read_per_1m=1.25,\n",
    "            cost_cache_write_per_1m=2.50,\n",
    "        )\n",
    "    },\n",
    ")\n",
    "print(f\"Fake config ready: model={FAKE_MODEL}, key_env=ARCLLM_TEST_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response helpers \u2014 simulate OpenAI API responses\n",
    "def make_openai_text_response(\n",
    "    text=\"Hello!\",\n",
    "    model=FAKE_MODEL,\n",
    "    prompt_tokens=10,\n",
    "    completion_tokens=5,\n",
    "    finish_reason=\"stop\",\n",
    "    **extra_usage,\n",
    "):\n",
    "    usage = {\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"completion_tokens\": completion_tokens,\n",
    "        \"total_tokens\": prompt_tokens + completion_tokens,\n",
    "    }\n",
    "    usage.update(extra_usage)\n",
    "    return {\n",
    "        \"id\": \"chatcmpl-walkthrough\",\n",
    "        \"object\": \"chat.completion\",\n",
    "        \"model\": model,\n",
    "        \"choices\": [\n",
    "            {\n",
    "                \"index\": 0,\n",
    "                \"message\": {\"role\": \"assistant\", \"content\": text},\n",
    "                \"finish_reason\": finish_reason,\n",
    "            }\n",
    "        ],\n",
    "        \"usage\": usage,\n",
    "    }\n",
    "\n",
    "\n",
    "def make_openai_tool_response(\n",
    "    tool_id=\"call_01\",\n",
    "    tool_name=\"search\",\n",
    "    tool_args=None,\n",
    "    text=None,\n",
    "):\n",
    "    tool_calls = [\n",
    "        {\n",
    "            \"id\": tool_id,\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool_name,\n",
    "                \"arguments\": json.dumps(tool_args or {\"query\": \"test\"}),\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    return {\n",
    "        \"id\": \"chatcmpl-walkthrough\",\n",
    "        \"object\": \"chat.completion\",\n",
    "        \"model\": FAKE_MODEL,\n",
    "        \"choices\": [\n",
    "            {\n",
    "                \"index\": 0,\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": text,\n",
    "                    \"tool_calls\": tool_calls,\n",
    "                },\n",
    "                \"finish_reason\": \"tool_calls\",\n",
    "            }\n",
    "        ],\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": 20,\n",
    "            \"completion_tokens\": 15,\n",
    "            \"total_tokens\": 35,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Response helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. StopReason \u2014 Normalized Across Providers\n",
    "\n",
    "Before Step 5, `LLMResponse.stop_reason` was a plain `str`. Different providers use different values:\n",
    "\n",
    "| What happened | Anthropic says | OpenAI says | ArcLLM canonical |\n",
    "|---------------|---------------|-------------|------------------|\n",
    "| Model finished | `\"end_turn\"` | `\"stop\"` | `\"end_turn\"` |\n",
    "| Model wants to use a tool | `\"tool_use\"` | `\"tool_calls\"` | `\"tool_use\"` |\n",
    "| Hit max tokens | `\"max_tokens\"` | `\"length\"` | `\"max_tokens\"` |\n",
    "| Stop sequence | `\"stop_sequence\"` | (N/A) | `\"stop_sequence\"` |\n",
    "| Content filter | (N/A) | `\"content_filter\"` | `\"end_turn\"` |\n",
    "\n",
    "Step 5 added `StopReason = Literal[\"end_turn\", \"tool_use\", \"max_tokens\", \"stop_sequence\"]` and updated `LLMResponse` to use it. Now agents can check `resp.stop_reason == \"tool_use\"` regardless of provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StopReason is a Literal type \u2014 pydantic validates it\n",
    "from typing import get_args\n",
    "\n",
    "valid_values = get_args(StopReason)\n",
    "print(f\"StopReason = Literal{list(valid_values)}\")\n",
    "print(f\"\\nType: {StopReason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 4 valid values work in LLMResponse\n",
    "for reason in get_args(StopReason):\n",
    "    resp = LLMResponse(\n",
    "        content=\"test\",\n",
    "        usage=Usage(input_tokens=1, output_tokens=1, total_tokens=2),\n",
    "        model=\"test\",\n",
    "        stop_reason=reason,\n",
    "    )\n",
    "    print(f\"  {reason:<15} -> accepted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invalid values are rejected by pydantic\n",
    "from pydantic import ValidationError\n",
    "\n",
    "for bad_value in [\"done\", \"finished\", \"stop\", \"length\", \"tool_calls\"]:\n",
    "    try:\n",
    "        LLMResponse(\n",
    "            content=\"test\",\n",
    "            usage=Usage(input_tokens=1, output_tokens=1, total_tokens=2),\n",
    "            model=\"test\",\n",
    "            stop_reason=bad_value,\n",
    "        )\n",
    "        print(f\"  {bad_value:<15} -> ACCEPTED (unexpected!)\")\n",
    "    except ValidationError:\n",
    "        print(f\"  {bad_value:<15} -> REJECTED (correct)\")\n",
    "\n",
    "print(\"\\nNote: raw OpenAI values like 'stop' and 'length' are rejected.\")\n",
    "print(\"The adapter maps them to canonical values before creating LLMResponse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. OpenAI vs Anthropic \u2014 The Differences\n",
    "\n",
    "Let's see every difference the adapter hides, side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = OpenaiAdapter(fake_config, FAKE_MODEL)\n",
    "print(f\"Adapter name: {adapter.name}\")\n",
    "print(f\"Is BaseAdapter? {isinstance(adapter, BaseAdapter)}\")\n",
    "print(f\"Is LLMProvider? True (inherits via BaseAdapter)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison of every adapter difference\n",
    "diffs = [\n",
    "    (\"Auth header\",\n",
    "     \"x-api-key: sk-ant-...\",\n",
    "     \"Authorization: Bearer sk-...\"),\n",
    "    (\"System messages\",\n",
    "     \"Extracted to top-level 'system' param\",\n",
    "     \"Stay in-line in messages array\"),\n",
    "    (\"Tool definition key\",\n",
    "     \"'input_schema' (renamed from parameters)\",\n",
    "     \"'parameters' (stays as-is)\"),\n",
    "    (\"Tool wrapping\",\n",
    "     \"Flat: {name, description, input_schema}\",\n",
    "     'Nested: {type: \"function\", function: {...}}'),\n",
    "    (\"Tool call location\",\n",
    "     \"Content blocks: [{type: 'tool_use', ...}]\",\n",
    "     \"Message-level: message.tool_calls[]\"),\n",
    "    (\"Tool call args\",\n",
    "     \"'input' key (dict)\",\n",
    "     \"'arguments' key (JSON string!)\"),\n",
    "    (\"Tool results\",\n",
    "     \"role='user' with tool_result blocks\",\n",
    "     \"role='tool' flattened (one msg per result)\"),\n",
    "    (\"Stop reason\",\n",
    "     \"Already canonical (end_turn, tool_use)\",\n",
    "     \"Mapped: stop->end_turn, tool_calls->tool_use\"),\n",
    "    (\"Response content\",\n",
    "     \"content: [{type: 'text', text: '...'}]\",\n",
    "     \"choices[0].message.content: '...'\"),\n",
    "    (\"Usage tokens\",\n",
    "     \"input_tokens / output_tokens\",\n",
    "     \"prompt_tokens / completion_tokens\"),\n",
    "    (\"API version\",\n",
    "     \"anthropic-version header required\",\n",
    "     \"No version header needed\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Concern':<22} {'Anthropic':<42} {'OpenAI'}\")\n",
    "print(\"-\" * 110)\n",
    "for concern, anthropic, openai in diffs:\n",
    "    print(f\"{concern:<22} {anthropic:<42} {openai}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Request Building \u2014 Headers\n",
    "\n",
    "OpenAI uses Bearer token auth (not a custom header like Anthropic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = adapter._build_headers()\n",
    "print(\"OpenAI request headers:\")\n",
    "for key, value in headers.items():\n",
    "    display_val = value[:25] + \"...\" if \"Bearer\" in value else value\n",
    "    print(f\"  {key}: {display_val}\")\n",
    "\n",
    "print(f\"\\nNo 'anthropic-version' header \u2014 OpenAI doesn't need one.\")\n",
    "print(f\"Bearer prefix: {headers['Authorization'].startswith('Bearer ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Request Building \u2014 System Messages (In-line)\n",
    "\n",
    "Unlike Anthropic (which extracts system messages to a top-level param), OpenAI keeps system messages **in the messages array**. The adapter just passes them through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System messages stay in-line \u2014 not extracted\n",
    "messages = [\n",
    "    Message(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    Message(role=\"user\", content=\"Hello!\"),\n",
    "]\n",
    "\n",
    "body = adapter._build_request_body(messages)\n",
    "print(f\"Messages in request body: {len(body['messages'])}\")\n",
    "for i, msg in enumerate(body[\"messages\"]):\n",
    "    print(f\"  [{i}] role={msg['role']!r}, content={msg['content']!r}\")\n",
    "\n",
    "print(f\"\\n'system' key in body? {'system' in body}\")\n",
    "print(\"System message stays in the messages array \u2014 NOT extracted like Anthropic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Request Building \u2014 Tool Definitions\n",
    "\n",
    "OpenAI wraps tools in `{\"type\": \"function\", \"function\": {...}}` and keeps the key as `parameters` (Anthropic renames it to `input_schema`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool definition formatting\n",
    "tool = Tool(\n",
    "    name=\"search_database\",\n",
    "    description=\"Search the knowledge base\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n",
    "            \"limit\": {\"type\": \"integer\", \"default\": 10},\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "formatted = adapter._format_tool(tool)\n",
    "print(\"OpenAI tool format:\")\n",
    "print(json.dumps(formatted, indent=2))\n",
    "\n",
    "print(f\"\\nOuter 'type' key: {formatted['type']!r}\")\n",
    "print(f\"Nested under 'function': {list(formatted['function'].keys())}\")\n",
    "print(f\"Key is 'parameters' (not 'input_schema'): {'parameters' in formatted['function']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Request Building \u2014 Message Formatting\n",
    "\n",
    "The OpenAI adapter handles three special cases:\n",
    "1. **Plain text messages** \u2014 pass through as-is\n",
    "2. **Assistant messages with ToolUseBlocks** \u2014 format as `tool_calls` array with JSON string arguments\n",
    "3. **Tool result messages** \u2014 flatten from one message with N blocks to N individual messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text message \u2014 passthrough\n",
    "msg = Message(role=\"user\", content=\"Hello!\")\n",
    "formatted = adapter._format_message(msg)\n",
    "print(\"Text message:\")\n",
    "print(f\"  {json.dumps(formatted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assistant message with ToolUseBlocks -> tool_calls array\n",
    "msg = Message(\n",
    "    role=\"assistant\",\n",
    "    content=[\n",
    "        TextBlock(text=\"Let me look that up.\"),\n",
    "        ToolUseBlock(id=\"call_01\", name=\"search\", arguments={\"query\": \"cats\"}),\n",
    "        ToolUseBlock(id=\"call_02\", name=\"lookup\", arguments={\"id\": 42}),\n",
    "    ],\n",
    ")\n",
    "formatted = adapter._format_message(msg)\n",
    "print(\"Assistant with tool calls:\")\n",
    "print(json.dumps(formatted, indent=2))\n",
    "\n",
    "print(f\"\\nArguments are JSON STRINGS (not dicts):\")\n",
    "for tc in formatted[\"tool_calls\"]:\n",
    "    args = tc[\"function\"][\"arguments\"]\n",
    "    print(f\"  {tc['function']['name']}: {args!r}  (type: {type(args).__name__})\")\n",
    "\n",
    "print(\"\\nThis is a key OpenAI quirk \u2014 arguments must be serialized to JSON strings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assistant with ONLY tool calls (no text) -> content is None\n",
    "msg = Message(\n",
    "    role=\"assistant\",\n",
    "    content=[\n",
    "        ToolUseBlock(id=\"call_01\", name=\"calc\", arguments={\"x\": 1}),\n",
    "    ],\n",
    ")\n",
    "formatted = adapter._format_message(msg)\n",
    "print(f\"content: {formatted['content']}  (None when no text blocks)\")\n",
    "print(f\"tool_calls: {len(formatted['tool_calls'])} call(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### 6a. Tool Result Flattening\n",
    "\n",
    "This is the most interesting OpenAI adapter behavior.\n",
    "\n",
    "In ArcLLM, a single `Message(role=\"tool\")` can contain **multiple** `ToolResultBlock`s (the agent executed 2 tools, here are both results).\n",
    "\n",
    "But OpenAI requires **one message per tool result**, each with its own `tool_call_id`. So one ArcLLM message becomes N OpenAI messages.\n",
    "\n",
    "```\n",
    "ArcLLM (1 message):              OpenAI (2 messages):\n",
    "Message(role=\"tool\",       -->   {\"role\": \"tool\", \"tool_call_id\": \"t1\", \"content\": \"42\"}\n",
    "  content=[                -->   {\"role\": \"tool\", \"tool_call_id\": \"t2\", \"content\": \"hello\"}\n",
    "    ToolResult(id=\"t1\"),\n",
    "    ToolResult(id=\"t2\"),\n",
    "  ])\n",
    "```\n",
    "\n",
    "This flattening happens in `_format_messages()` (not `_format_message()`) \u2014 clean separation of concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool result flattening: 1 ArcLLM message -> 2 OpenAI messages\n",
    "messages = [\n",
    "    Message(role=\"user\", content=\"Do two things.\"),\n",
    "    Message(\n",
    "        role=\"tool\",\n",
    "        content=[\n",
    "            ToolResultBlock(tool_use_id=\"call_01\", content=\"Result from tool 1\"),\n",
    "            ToolResultBlock(tool_use_id=\"call_02\", content=\"Result from tool 2\"),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "formatted = adapter._format_messages(messages)\n",
    "print(f\"ArcLLM messages: {len(messages)}\")\n",
    "print(f\"OpenAI messages: {len(formatted)}  (flattened!)\")\n",
    "print()\n",
    "for i, msg in enumerate(formatted):\n",
    "    print(f\"  [{i}] {json.dumps(msg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single tool result \u2014 no flattening needed (1 -> 1)\n",
    "messages = [\n",
    "    Message(\n",
    "        role=\"tool\",\n",
    "        content=[ToolResultBlock(tool_use_id=\"call_01\", content=\"42\")],\n",
    "    ),\n",
    "]\n",
    "\n",
    "formatted = adapter._format_messages(messages)\n",
    "print(f\"1 ArcLLM message -> {len(formatted)} OpenAI message(s)\")\n",
    "print(f\"  {json.dumps(formatted[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Request Building \u2014 Full Request Body\n",
    "\n",
    "`_build_request_body()` combines everything into the final API payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text request\n",
    "body = adapter._build_request_body(\n",
    "    [Message(role=\"user\", content=\"What's the weather?\")]\n",
    ")\n",
    "print(\"Simple request body:\")\n",
    "print(json.dumps(body, indent=2))\n",
    "print(f\"\\nNo 'tools' key (not provided).\")\n",
    "print(f\"No 'system' key (OpenAI doesn't extract system messages).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request with system message + tools\n",
    "body = adapter._build_request_body(\n",
    "    [\n",
    "        Message(role=\"system\", content=\"You are a research assistant.\"),\n",
    "        Message(role=\"user\", content=\"Find papers about LLM agents\"),\n",
    "    ],\n",
    "    tools=[tool],  # from above\n",
    ")\n",
    "print(\"Full request body with system + tools:\")\n",
    "print(json.dumps(body, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwargs override config defaults\n",
    "body_default = adapter._build_request_body(\n",
    "    [Message(role=\"user\", content=\"Hi\")]\n",
    ")\n",
    "body_override = adapter._build_request_body(\n",
    "    [Message(role=\"user\", content=\"Hi\")],\n",
    "    max_tokens=1000, temperature=0.2,\n",
    ")\n",
    "\n",
    "print(f\"Default:  max_tokens={body_default['max_tokens']}, temperature={body_default['temperature']}\")\n",
    "print(f\"Override: max_tokens={body_override['max_tokens']}, temperature={body_override['temperature']}\")\n",
    "print(f\"\\nOverride chain: kwargs > provider config > model metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Response Parsing \u2014 Text\n",
    "\n",
    "OpenAI responses come wrapped in `choices[0].message`. The adapter extracts content, stop reason, and usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a text response\n",
    "raw = make_openai_text_response(text=\"The answer is 42.\")\n",
    "print(\"Raw OpenAI response structure:\")\n",
    "print(json.dumps(raw, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse it into LLMResponse\n",
    "resp = adapter._parse_response(raw)\n",
    "print(f\"Type:        {type(resp).__name__}\")\n",
    "print(f\"Content:     {resp.content!r}\")\n",
    "print(f\"Tool calls:  {resp.tool_calls}\")\n",
    "print(f\"Stop reason: {resp.stop_reason!r}  (mapped from 'stop')\")\n",
    "print(f\"Model:       {resp.model}\")\n",
    "print(f\"Usage:       {resp.usage.input_tokens}in + {resp.usage.output_tokens}out = {resp.usage.total_tokens}\")\n",
    "print(f\"Raw stored:  {resp.raw is raw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Response Parsing \u2014 Tool Calls\n",
    "\n",
    "OpenAI puts tool calls at the **message level** (`message.tool_calls[]`), not as content blocks like Anthropic. Arguments come as JSON **strings** that need parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a tool use response\n",
    "raw = make_openai_tool_response(\n",
    "    tool_id=\"call_abc\",\n",
    "    tool_name=\"search\",\n",
    "    tool_args={\"query\": \"LLM agents\", \"limit\": 5},\n",
    ")\n",
    "\n",
    "print(\"Raw tool call in OpenAI format:\")\n",
    "raw_tc = raw[\"choices\"][0][\"message\"][\"tool_calls\"][0]\n",
    "print(f\"  id: {raw_tc['id']}\")\n",
    "print(f\"  function.name: {raw_tc['function']['name']}\")\n",
    "print(f\"  function.arguments: {raw_tc['function']['arguments']!r}\")\n",
    "print(f\"  arguments type: {type(raw_tc['function']['arguments']).__name__}  <- JSON string!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After parsing \u2014 arguments are a proper dict\n",
    "resp = adapter._parse_response(raw)\n",
    "print(f\"Parsed tool call:\")\n",
    "print(f\"  id:        {resp.tool_calls[0].id}\")\n",
    "print(f\"  name:      {resp.tool_calls[0].name}\")\n",
    "print(f\"  arguments: {resp.tool_calls[0].arguments}\")\n",
    "print(f\"  type:      {type(resp.tool_calls[0].arguments).__name__}  <- proper dict!\")\n",
    "print(f\"\\nStop reason: {resp.stop_reason!r}  (mapped from 'tool_calls')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed response: text + tool calls together\n",
    "raw = make_openai_tool_response(\n",
    "    text=\"Let me search for that.\",\n",
    "    tool_id=\"call_xyz\",\n",
    "    tool_name=\"search\",\n",
    "    tool_args={\"query\": \"cats\"},\n",
    ")\n",
    "resp = adapter._parse_response(raw)\n",
    "print(f\"Content:     {resp.content!r}\")\n",
    "print(f\"Tool calls:  {len(resp.tool_calls)}\")\n",
    "print(\"Both present \u2014 text AND tool calls in the same response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null content \u2014 when model only uses tools (no text)\n",
    "raw = make_openai_tool_response()\n",
    "raw[\"choices\"][0][\"message\"][\"content\"] = None\n",
    "resp = adapter._parse_response(raw)\n",
    "print(f\"Content: {resp.content}  (None \u2014 pure tool call, no text)\")\n",
    "print(f\"Tool calls: {len(resp.tool_calls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Stop Reason Mapping\n",
    "\n",
    "The `_STOP_REASON_MAP` dict at module level maps every OpenAI `finish_reason` to a canonical `StopReason`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the full mapping\n",
    "from arcllm.adapters.openai import _STOP_REASON_MAP\n",
    "\n",
    "print(\"OpenAI finish_reason -> ArcLLM StopReason:\")\n",
    "print(\"-\" * 45)\n",
    "for openai_val, arcllm_val in _STOP_REASON_MAP.items():\n",
    "    print(f\"  {openai_val!r:<20} -> {arcllm_val!r}\")\n",
    "\n",
    "print(f\"\\nUnknown values default to 'end_turn' (safe fallback).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each mapping through _parse_response\n",
    "test_cases = [\n",
    "    (\"stop\", \"end_turn\"),\n",
    "    (\"tool_calls\", \"tool_use\"),\n",
    "    (\"length\", \"max_tokens\"),\n",
    "    (\"content_filter\", \"end_turn\"),\n",
    "]\n",
    "\n",
    "for openai_reason, expected in test_cases:\n",
    "    if openai_reason == \"tool_calls\":\n",
    "        raw = make_openai_tool_response()\n",
    "    else:\n",
    "        raw = make_openai_text_response(finish_reason=openai_reason)\n",
    "    resp = adapter._parse_response(raw)\n",
    "    status = \"pass\" if resp.stop_reason == expected else \"FAIL\"\n",
    "    print(f\"  {openai_reason!r:<18} -> {resp.stop_reason!r:<14} (expected {expected!r}) [{status}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unknown finish_reason -> safe default\n",
    "result = adapter._map_stop_reason(\"some_future_reason\")\n",
    "print(f\"Unknown reason 'some_future_reason' -> {result!r}\")\n",
    "print(\"Safe default prevents crashes when OpenAI adds new finish reasons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Usage Parsing\n",
    "\n",
    "OpenAI uses different token field names than ArcLLM's canonical format, plus has `reasoning_tokens` for o1/o3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard usage mapping\n",
    "raw = make_openai_text_response(prompt_tokens=100, completion_tokens=50)\n",
    "resp = adapter._parse_response(raw)\n",
    "\n",
    "print(\"Token name mapping:\")\n",
    "print(f\"  OpenAI 'prompt_tokens'     -> ArcLLM 'input_tokens':  {resp.usage.input_tokens}\")\n",
    "print(f\"  OpenAI 'completion_tokens'  -> ArcLLM 'output_tokens': {resp.usage.output_tokens}\")\n",
    "print(f\"  OpenAI 'total_tokens'       -> ArcLLM 'total_tokens':  {resp.usage.total_tokens}\")\n",
    "print(f\"  reasoning_tokens:           {resp.usage.reasoning_tokens}  (None \u2014 not an o1/o3 model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning tokens (o1/o3 models include completion_tokens_details)\n",
    "raw = make_openai_text_response(prompt_tokens=200, completion_tokens=150)\n",
    "raw[\"usage\"][\"completion_tokens_details\"] = {\"reasoning_tokens\": 80}\n",
    "\n",
    "resp = adapter._parse_response(raw)\n",
    "print(\"o1/o3 model with reasoning tokens:\")\n",
    "print(f\"  input_tokens:     {resp.usage.input_tokens}\")\n",
    "print(f\"  output_tokens:    {resp.usage.output_tokens}\")\n",
    "print(f\"  reasoning_tokens: {resp.usage.reasoning_tokens}\")\n",
    "print(f\"  total_tokens:     {resp.usage.total_tokens}\")\n",
    "print(f\"\\nReasoning tokens tell agents how much 'thinking' happened.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Tool Call Parsing Edge Cases\n",
    "\n",
    "OpenAI sends tool call arguments as JSON strings. The adapter handles three cases:\n",
    "1. JSON string (normal) -> `json.loads()`\n",
    "2. Dict (defensive) -> pass-through\n",
    "3. Garbage -> `ArcLLMParseError`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal case: JSON string arguments (this is what OpenAI sends)\n",
    "tc = adapter._parse_tool_call({\n",
    "    \"id\": \"call_1\",\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"calc\",\n",
    "        \"arguments\": '{\"expression\": \"2+2\", \"precision\": 2}',\n",
    "    },\n",
    "})\n",
    "print(f\"JSON string -> parsed dict: {tc.arguments}\")\n",
    "print(f\"Type: {type(tc.arguments).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defensive case: dict arguments (pass-through)\n",
    "tc = adapter._parse_tool_call({\n",
    "    \"id\": \"call_1\",\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"calc\",\n",
    "        \"arguments\": {\"expression\": \"2+2\"},\n",
    "    },\n",
    "})\n",
    "print(f\"Dict -> pass-through: {tc.arguments}\")\n",
    "print(f\"Type: {type(tc.arguments).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad JSON string -> ArcLLMParseError with raw data preserved\n",
    "try:\n",
    "    adapter._parse_tool_call({\n",
    "        \"id\": \"call_1\",\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calc\",\n",
    "            \"arguments\": \"not valid json {{{\",\n",
    "        },\n",
    "    })\n",
    "except ArcLLMParseError as e:\n",
    "    print(f\"ArcLLMParseError: {e}\")\n",
    "    print(f\"Raw string:      {e.raw_string!r}\")\n",
    "    print(f\"Original error:  {type(e.original_error).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unexpected type (not dict, not string) -> ArcLLMParseError\n",
    "try:\n",
    "    adapter._parse_tool_call({\n",
    "        \"id\": \"call_1\",\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calc\",\n",
    "            \"arguments\": 12345,\n",
    "        },\n",
    "    })\n",
    "except ArcLLMParseError as e:\n",
    "    print(f\"Unexpected type caught: {e}\")\n",
    "    print(f\"Raw string: {e.raw_string!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-48",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Full invoke() Cycle (Mocked)\n",
    "\n",
    "Let's simulate the complete `invoke()` flow with mocked HTTP responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text conversation\n",
    "async def demo_text_invoke():\n",
    "    adapter = OpenaiAdapter(fake_config, FAKE_MODEL)\n",
    "\n",
    "    response_data = make_openai_text_response(text=\"Austin is 75F and sunny.\")\n",
    "    mock_response = httpx.Response(\n",
    "        200, json=response_data,\n",
    "        request=httpx.Request(\"POST\", \"https://api.openai.com/v1/chat/completions\"),\n",
    "    )\n",
    "    adapter._client = AsyncMock()\n",
    "    adapter._client.post = AsyncMock(return_value=mock_response)\n",
    "\n",
    "    resp = await adapter.invoke([\n",
    "        Message(role=\"system\", content=\"You are a weather bot.\"),\n",
    "        Message(role=\"user\", content=\"What's the weather in Austin?\"),\n",
    "    ])\n",
    "\n",
    "    print(f\"Response type: {type(resp).__name__}\")\n",
    "    print(f\"Content:       {resp.content}\")\n",
    "    print(f\"Stop reason:   {resp.stop_reason}\")\n",
    "    print(f\"Tokens:        {resp.usage.total_tokens}\")\n",
    "\n",
    "    # Verify the request was built correctly\n",
    "    call_kwargs = adapter._client.post.call_args\n",
    "    url = call_kwargs[0][0]\n",
    "    body = call_kwargs[1][\"json\"]\n",
    "    print(f\"\\nURL:           {url}\")\n",
    "    print(f\"Messages:      {len(body['messages'])} (system stays in-line)\")\n",
    "    print(f\"Auth header:   {call_kwargs[1]['headers']['Authorization'][:25]}...\")\n",
    "\n",
    "await demo_text_invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool use conversation\n",
    "async def demo_tool_invoke():\n",
    "    adapter = OpenaiAdapter(fake_config, FAKE_MODEL)\n",
    "\n",
    "    response_data = make_openai_tool_response(\n",
    "        tool_id=\"call_weather\",\n",
    "        tool_name=\"get_weather\",\n",
    "        tool_args={\"city\": \"Austin\", \"units\": \"fahrenheit\"},\n",
    "        text=\"Let me check the weather.\",\n",
    "    )\n",
    "    mock_response = httpx.Response(\n",
    "        200, json=response_data,\n",
    "        request=httpx.Request(\"POST\", \"https://api.openai.com/v1/chat/completions\"),\n",
    "    )\n",
    "    adapter._client = AsyncMock()\n",
    "    adapter._client.post = AsyncMock(return_value=mock_response)\n",
    "\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"get_weather\",\n",
    "            description=\"Get current weather for a city\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\"type\": \"string\"},\n",
    "                    \"units\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"city\"],\n",
    "            },\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    resp = await adapter.invoke(\n",
    "        [Message(role=\"user\", content=\"What's the weather in Austin?\")],\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    print(f\"Content:     {resp.content!r}\")\n",
    "    print(f\"Stop reason: {resp.stop_reason}\")\n",
    "    print(f\"Tool calls:  {len(resp.tool_calls)}\")\n",
    "    for tc in resp.tool_calls:\n",
    "        print(f\"  {tc.name}({tc.arguments})\")\n",
    "\n",
    "await demo_tool_invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP error handling\n",
    "async def demo_error_invoke():\n",
    "    adapter = OpenaiAdapter(fake_config, FAKE_MODEL)\n",
    "\n",
    "    for status, label in [(429, \"Rate limited\"), (401, \"Invalid key\"), (500, \"Server error\")]:\n",
    "        mock_response = httpx.Response(\n",
    "            status, text=f'{label}',\n",
    "            request=httpx.Request(\"POST\", \"https://api.openai.com/v1/chat/completions\"),\n",
    "        )\n",
    "        adapter._client = AsyncMock()\n",
    "        adapter._client.post = AsyncMock(return_value=mock_response)\n",
    "\n",
    "        try:\n",
    "            await adapter.invoke([Message(role=\"user\", content=\"Hi\")])\n",
    "        except ArcLLMAPIError as e:\n",
    "            print(f\"  HTTP {e.status_code}: provider={e.provider!r}, body={e.body!r}\")\n",
    "\n",
    "await demo_error_invoke()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Simulated Agentic Tool-Calling Loop (OpenAI)\n",
    "\n",
    "The exact same loop pattern from the Anthropic walkthrough, but using `OpenaiAdapter`. The agent code is **identical** \u2014 only the adapter changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-53",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demo_openai_agentic_loop():\n",
    "    \"\"\"Simulate a full agentic tool-calling loop with OpenAI adapter.\"\"\"\n",
    "    adapter = OpenaiAdapter(fake_config, FAKE_MODEL)\n",
    "\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"get_weather\",\n",
    "            description=\"Get weather for a city\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"city\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"city\"],\n",
    "            },\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    messages = [\n",
    "        Message(role=\"system\", content=\"Use tools to answer questions.\"),\n",
    "        Message(role=\"user\", content=\"What's the weather in Austin?\"),\n",
    "    ]\n",
    "\n",
    "    # --- Turn 1: LLM wants to use a tool ---\n",
    "    turn1_data = make_openai_tool_response(\n",
    "        tool_id=\"call_weather_01\",\n",
    "        tool_name=\"get_weather\",\n",
    "        tool_args={\"city\": \"Austin\"},\n",
    "    )\n",
    "    mock_resp1 = httpx.Response(\n",
    "        200, json=turn1_data,\n",
    "        request=httpx.Request(\"POST\", \"https://api.openai.com/v1/chat/completions\"),\n",
    "    )\n",
    "    adapter._client = AsyncMock()\n",
    "    adapter._client.post = AsyncMock(return_value=mock_resp1)\n",
    "\n",
    "    resp = await adapter.invoke(messages, tools=tools)\n",
    "    print(f\"Turn 1: stop_reason={resp.stop_reason}\")\n",
    "    print(f\"  LLM wants to call: {resp.tool_calls[0].name}({resp.tool_calls[0].arguments})\")\n",
    "\n",
    "    # Agent executes the tool\n",
    "    tool_result = \"75 deg F, sunny, humidity 45%\"\n",
    "    print(f\"  Agent executes tool -> result: {tool_result!r}\")\n",
    "\n",
    "    # Agent adds assistant response + tool result to conversation\n",
    "    tc = resp.tool_calls[0]\n",
    "    messages.append(Message(\n",
    "        role=\"assistant\",\n",
    "        content=[ToolUseBlock(id=tc.id, name=tc.name, arguments=tc.arguments)],\n",
    "    ))\n",
    "    messages.append(Message(\n",
    "        role=\"tool\",\n",
    "        content=[ToolResultBlock(tool_use_id=tc.id, content=tool_result)],\n",
    "    ))\n",
    "\n",
    "    # --- Turn 2: LLM gives final answer ---\n",
    "    turn2_data = make_openai_text_response(\n",
    "        text=\"The weather in Austin is 75 deg F and sunny with 45% humidity.\"\n",
    "    )\n",
    "    mock_resp2 = httpx.Response(\n",
    "        200, json=turn2_data,\n",
    "        request=httpx.Request(\"POST\", \"https://api.openai.com/v1/chat/completions\"),\n",
    "    )\n",
    "    adapter._client.post = AsyncMock(return_value=mock_resp2)\n",
    "\n",
    "    resp = await adapter.invoke(messages, tools=tools)\n",
    "    print(f\"\\nTurn 2: stop_reason={resp.stop_reason}\")\n",
    "    print(f\"  Final answer: {resp.content}\")\n",
    "    print(f\"\\nLoop complete! {len(messages)} messages in conversation.\")\n",
    "\n",
    "    # Verify the request \u2014 tool result was flattened correctly\n",
    "    call_kwargs = adapter._client.post.call_args\n",
    "    sent_body = call_kwargs[1][\"json\"]\n",
    "    tool_msgs = [m for m in sent_body[\"messages\"] if m[\"role\"] == \"tool\"]\n",
    "    print(f\"\\nTool result messages sent to OpenAI: {len(tool_msgs)}\")\n",
    "    for tm in tool_msgs:\n",
    "        print(f\"  tool_call_id={tm['tool_call_id']}, content={tm['content']!r}\")\n",
    "\n",
    "await demo_openai_agentic_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-54",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Cross-Provider Proof: Same Agent Code, Different Adapters\n",
    "\n",
    "This is the whole point of ArcLLM. The agent writes `adapter.invoke(messages, tools)` once, and it works with **any** provider. Let's prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup both adapters with fake configs\n",
    "os.environ[\"ARCLLM_ANTHROPIC_KEY\"] = \"sk-ant-test-key\"\n",
    "os.environ[\"ARCLLM_OPENAI_KEY\"] = \"sk-openai-test-key\"\n",
    "\n",
    "anthropic_config = ProviderConfig(\n",
    "    provider=ProviderSettings(\n",
    "        api_format=\"anthropic-messages\",\n",
    "        base_url=\"https://api.anthropic.com\",\n",
    "        api_key_env=\"ARCLLM_ANTHROPIC_KEY\",\n",
    "        default_model=\"claude-test\",\n",
    "        default_temperature=0.7,\n",
    "    ),\n",
    "    models={\n",
    "        \"claude-test\": ModelMetadata(\n",
    "            context_window=200000, max_output_tokens=8192,\n",
    "            supports_tools=True, supports_vision=True, supports_thinking=True,\n",
    "            input_modalities=[\"text\", \"image\"],\n",
    "            cost_input_per_1m=3.0, cost_output_per_1m=15.0,\n",
    "            cost_cache_read_per_1m=0.3, cost_cache_write_per_1m=3.75,\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "openai_config = ProviderConfig(\n",
    "    provider=ProviderSettings(\n",
    "        api_format=\"openai-chat\",\n",
    "        base_url=\"https://api.openai.com\",\n",
    "        api_key_env=\"ARCLLM_OPENAI_KEY\",\n",
    "        default_model=\"gpt-4o-test\",\n",
    "        default_temperature=0.7,\n",
    "    ),\n",
    "    models={\n",
    "        \"gpt-4o-test\": ModelMetadata(\n",
    "            context_window=128000, max_output_tokens=16384,\n",
    "            supports_tools=True, supports_vision=True, supports_thinking=False,\n",
    "            input_modalities=[\"text\", \"image\"],\n",
    "            cost_input_per_1m=2.50, cost_output_per_1m=10.00,\n",
    "            cost_cache_read_per_1m=1.25, cost_cache_write_per_1m=2.50,\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Both configs ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-56",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def agent_tool_loop(adapter, mock_responses):\n",
    "    \"\"\"\n",
    "    Generic agentic loop \u2014 works with ANY adapter.\n",
    "    This is the code agents actually write.\n",
    "    \"\"\"\n",
    "    adapter._client = AsyncMock()\n",
    "    response_iter = iter(mock_responses)\n",
    "\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"calculate\",\n",
    "            description=\"Evaluate a math expression\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"expression\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"expression\"],\n",
    "            },\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    messages = [Message(role=\"user\", content=\"What is 7 * 8?\")]\n",
    "    turn = 0\n",
    "\n",
    "    while True:\n",
    "        turn += 1\n",
    "        mock_resp = next(response_iter)\n",
    "        adapter._client.post = AsyncMock(return_value=mock_resp)\n",
    "\n",
    "        resp = await adapter.invoke(messages, tools=tools)\n",
    "\n",
    "        if resp.stop_reason == \"end_turn\":\n",
    "            return resp.content, turn\n",
    "\n",
    "        if resp.stop_reason == \"tool_use\":\n",
    "            tc = resp.tool_calls[0]\n",
    "            result = str(eval(tc.arguments[\"expression\"]))\n",
    "\n",
    "            messages.append(Message(\n",
    "                role=\"assistant\",\n",
    "                content=[ToolUseBlock(id=tc.id, name=tc.name, arguments=tc.arguments)],\n",
    "            ))\n",
    "            messages.append(Message(\n",
    "                role=\"tool\",\n",
    "                content=[ToolResultBlock(tool_use_id=tc.id, content=result)],\n",
    "            ))\n",
    "\n",
    "        if turn > 5:\n",
    "            return \"(max turns)\", turn\n",
    "\n",
    "\n",
    "# Mock responses for Anthropic format\n",
    "anthropic_responses = [\n",
    "    httpx.Response(200, json={\n",
    "        \"id\": \"msg_1\", \"type\": \"message\", \"role\": \"assistant\", \"model\": \"claude-test\",\n",
    "        \"content\": [{\"type\": \"tool_use\", \"id\": \"toolu_01\", \"name\": \"calculate\", \"input\": {\"expression\": \"7 * 8\"}}],\n",
    "        \"stop_reason\": \"tool_use\",\n",
    "        \"usage\": {\"input_tokens\": 50, \"output_tokens\": 20},\n",
    "    }, request=httpx.Request(\"POST\", \"https://api.anthropic.com/v1/messages\")),\n",
    "    httpx.Response(200, json={\n",
    "        \"id\": \"msg_2\", \"type\": \"message\", \"role\": \"assistant\", \"model\": \"claude-test\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"7 * 8 = 56\"}],\n",
    "        \"stop_reason\": \"end_turn\",\n",
    "        \"usage\": {\"input_tokens\": 80, \"output_tokens\": 10},\n",
    "    }, request=httpx.Request(\"POST\", \"https://api.anthropic.com/v1/messages\")),\n",
    "]\n",
    "\n",
    "# Mock responses for OpenAI format\n",
    "openai_responses = [\n",
    "    httpx.Response(200, json=make_openai_tool_response(\n",
    "        tool_id=\"call_01\", tool_name=\"calculate\", tool_args={\"expression\": \"7 * 8\"},\n",
    "    ), request=httpx.Request(\"POST\", \"https://api.openai.com/v1/chat/completions\")),\n",
    "    httpx.Response(200, json=make_openai_text_response(\n",
    "        text=\"7 * 8 = 56\",\n",
    "    ), request=httpx.Request(\"POST\", \"https://api.openai.com/v1/chat/completions\")),\n",
    "]\n",
    "\n",
    "# Run the SAME agent code with both adapters\n",
    "print(\"=\" * 60)\n",
    "print(\"Same agent code, different adapters:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "anthropic_adapter = AnthropicAdapter(anthropic_config, \"claude-test\")\n",
    "answer, turns = await agent_tool_loop(anthropic_adapter, anthropic_responses)\n",
    "print(f\"\\nAnthropic: '{answer}' (in {turns} turns)\")\n",
    "\n",
    "openai_adapter = OpenaiAdapter(openai_config, \"gpt-4o-test\")\n",
    "answer, turns = await agent_tool_loop(openai_adapter, openai_responses)\n",
    "print(f\"OpenAI:    '{answer}' (in {turns} turns)\")\n",
    "\n",
    "print(\"\\nThe agent_tool_loop() function is IDENTICAL for both.\")\n",
    "print(\"It checks resp.stop_reason == 'end_turn' and 'tool_use' \u2014 works everywhere.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-57",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Live API Calls (Real Anthropic + Cross-Provider Comparison)\n",
    "\n",
    "Let's use the real Anthropic API to run the same agent pattern we tested with mocks above \u2014 proving the full stack works end-to-end.\n",
    "\n",
    "(No OpenAI key is configured, so OpenAI calls use mocks. The agent code is the same either way.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from arcllm import load_provider_config\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "real_config = load_provider_config(\"anthropic\")\n",
    "real_model = real_config.provider.default_model\n",
    "\n",
    "print(f\"Provider: {real_config.provider.api_format}\")\n",
    "print(f\"Model:    {real_model}\")\n",
    "print(f\"API key:  {os.environ[real_config.provider.api_key_env][:12]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-59",
   "metadata": {},
   "source": [
    "### 16a. StopReason in Action (Real API)\n",
    "\n",
    "Make a real call with and without tools to see both `end_turn` and `tool_use` stop reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-60",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def live_stop_reasons():\n",
    "    calc_tool = Tool(\n",
    "        name=\"calculate\",\n",
    "        description=\"Evaluate a mathematical expression\",\n",
    "        parameters={\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"expression\": {\"type\": \"string\"}},\n",
    "            \"required\": [\"expression\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    async with AnthropicAdapter(real_config, real_model) as adapter:\n",
    "        # Text response -> end_turn\n",
    "        resp1 = await adapter.invoke(\n",
    "            [Message(role=\"user\", content=\"Say 'hello' and nothing else.\")],\n",
    "            max_tokens=10, temperature=0.0,\n",
    "        )\n",
    "        print(f\"Text response:\")\n",
    "        print(f\"  content:     {resp1.content!r}\")\n",
    "        print(f\"  stop_reason: {resp1.stop_reason!r}  <- StopReason type\")\n",
    "\n",
    "        # Tool response -> tool_use\n",
    "        resp2 = await adapter.invoke(\n",
    "            [Message(role=\"user\", content=\"What is 847 * 293? Use the calculator.\")],\n",
    "            tools=[calc_tool], max_tokens=200, temperature=0.0,\n",
    "        )\n",
    "        print(f\"\\nTool response:\")\n",
    "        print(f\"  content:     {resp2.content!r}\")\n",
    "        print(f\"  stop_reason: {resp2.stop_reason!r}  <- StopReason type\")\n",
    "        if resp2.tool_calls:\n",
    "            tc = resp2.tool_calls[0]\n",
    "            print(f\"  tool:        {tc.name}({tc.arguments})\")\n",
    "\n",
    "    print(f\"\\nBoth stop reasons are canonical StopReason values.\")\n",
    "    print(f\"An agent checks resp.stop_reason == 'tool_use' \u2014 works for Anthropic AND OpenAI.\")\n",
    "\n",
    "await live_stop_reasons()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-61",
   "metadata": {},
   "source": [
    "### 16b. Real Agentic Loop (Same Pattern as OpenAI Mock)\n",
    "\n",
    "The same multi-tool loop pattern from Section 14, but hitting the real Anthropic API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-62",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def live_agentic_loop():\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"lookup_price\",\n",
    "            description=\"Look up the price of an item in USD\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"item\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"item\"],\n",
    "            },\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"calculate\",\n",
    "            description=\"Evaluate a math expression\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"expression\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"expression\"],\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    prices = {\"coffee\": 4.50, \"bagel\": 3.25, \"orange juice\": 5.00}\n",
    "\n",
    "    def execute_tool(name, arguments):\n",
    "        if name == \"lookup_price\":\n",
    "            item = arguments[\"item\"].lower()\n",
    "            price = prices.get(item)\n",
    "            return f\"${price:.2f}\" if price else f\"Item '{item}' not found\"\n",
    "        elif name == \"calculate\":\n",
    "            try:\n",
    "                return str(eval(arguments[\"expression\"]))\n",
    "            except Exception as e:\n",
    "                return f\"Error: {e}\"\n",
    "        return \"Unknown tool\"\n",
    "\n",
    "    messages = [\n",
    "        Message(role=\"system\", content=\"Use tools to answer. Be concise.\"),\n",
    "        Message(role=\"user\", content=\"I want 2 coffees and 1 bagel. What's the total?\"),\n",
    "    ]\n",
    "\n",
    "    total_tokens = 0\n",
    "    turn = 0\n",
    "\n",
    "    async with AnthropicAdapter(real_config, real_model) as adapter:\n",
    "        while True:\n",
    "            turn += 1\n",
    "            resp = await adapter.invoke(messages, tools=tools, max_tokens=300, temperature=0.0)\n",
    "            total_tokens += resp.usage.total_tokens\n",
    "\n",
    "            print(f\"--- Turn {turn}: stop_reason={resp.stop_reason} ---\")\n",
    "\n",
    "            if resp.stop_reason == \"end_turn\":\n",
    "                print(f\"  Final answer: {resp.content}\")\n",
    "                break\n",
    "\n",
    "            if resp.stop_reason == \"tool_use\":\n",
    "                content_blocks = []\n",
    "                if resp.content:\n",
    "                    content_blocks.append(TextBlock(text=resp.content))\n",
    "                for tc in resp.tool_calls:\n",
    "                    content_blocks.append(\n",
    "                        ToolUseBlock(id=tc.id, name=tc.name, arguments=tc.arguments)\n",
    "                    )\n",
    "                messages.append(Message(role=\"assistant\", content=content_blocks))\n",
    "\n",
    "                result_blocks = []\n",
    "                for tc in resp.tool_calls:\n",
    "                    result = execute_tool(tc.name, tc.arguments)\n",
    "                    print(f\"  Tool: {tc.name}({tc.arguments}) -> {result}\")\n",
    "                    result_blocks.append(\n",
    "                        ToolResultBlock(tool_use_id=tc.id, content=result)\n",
    "                    )\n",
    "                messages.append(Message(role=\"tool\", content=result_blocks))\n",
    "\n",
    "            if turn > 6:\n",
    "                print(\"  (safety valve)\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nLoop complete: {turn} turns, {total_tokens} total tokens\")\n",
    "    print(\"\\nThis EXACT same loop works with OpenaiAdapter \u2014 just swap the adapter.\")\n",
    "\n",
    "await live_agentic_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-63",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Step 5 built the **OpenAI adapter** and the **StopReason** normalization:\n",
    "\n",
    "```\n",
    "types.py                 ->  + StopReason = Literal[\"end_turn\", \"tool_use\", \"max_tokens\", \"stop_sequence\"]\n",
    "                             + LLMResponse.stop_reason now uses StopReason (not str)\n",
    "adapters/openai.py       ->  OpenaiAdapter (request building + response parsing)\n",
    "```\n",
    "\n",
    "**Key differences from Anthropic adapter:**\n",
    "\n",
    "| Feature | Anthropic | OpenAI |\n",
    "|---------|-----------|--------|\n",
    "| Auth | `x-api-key` header | `Authorization: Bearer` |\n",
    "| System messages | Extracted to top-level | Stay in messages array |\n",
    "| Tool key | `input_schema` | `parameters` |\n",
    "| Tool wrapping | Flat | `{type: \"function\", function: {...}}` |\n",
    "| Tool call location | Content blocks | `message.tool_calls[]` |\n",
    "| Tool call args | Dict (usually) | JSON string (always) |\n",
    "| Tool results | `role=\"user\"` with blocks | `role=\"tool\"` flattened (1-to-N) |\n",
    "| Stop reasons | Already canonical | Mapped via `_STOP_REASON_MAP` |\n",
    "| Response content | `content: [{type, text}]` | `choices[0].message.content` |\n",
    "| Usage fields | `input_tokens`/`output_tokens` | `prompt_tokens`/`completion_tokens` |\n",
    "| Reasoning tokens | N/A | `completion_tokens_details.reasoning_tokens` |\n",
    "\n",
    "**The point:** Agents don't care about any of this. They write:\n",
    "```python\n",
    "resp = await adapter.invoke(messages, tools)\n",
    "if resp.stop_reason == \"tool_use\":\n",
    "    # execute tools, loop\n",
    "```\n",
    "...and it works with both providers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}