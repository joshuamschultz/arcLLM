{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9 — Telemetry Module\n",
    "\n",
    "**What we built**: A `TelemetryModule` that logs structured timing, token usage, and cost per `invoke()` call.\n",
    "\n",
    "**Why it matters**: In production with thousands of agents, operators need visibility into every LLM call — how long it took, how many tokens were used, and how much it cost. Telemetry sits *outermost* in the module stack, measuring total wall-clock duration including retries, fallback attempts, and rate-limit waits.\n",
    "\n",
    "**Key decisions**:\n",
    "- **D-064**: Structured logging only — no callback, no accumulator (simple, toggle-able)\n",
    "- **D-065**: Calculate and log `cost_usd` per call from provider pricing metadata\n",
    "- **D-066**: `load_model()` injects pricing from provider TOML via `setdefault()`\n",
    "- **D-067**: Outermost position: `Telemetry(Retry(Fallback(RateLimit(adapter))))`\n",
    "- **D-068**: INFO by default, configurable via `log_level`\n",
    "- **D-069**: Fields: provider, model, duration_ms, tokens, cache (conditional), cost_usd, stop_reason\n",
    "\n",
    "**Stack position**: `Telemetry → Retry → Fallback → RateLimit → Adapter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: ensure arcllm is importable\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What Gets Logged\n",
    "\n",
    "Every `invoke()` call produces one structured log line:\n",
    "\n",
    "```\n",
    "INFO  LLM call | provider=anthropic model=claude-sonnet-4-20250514 duration_ms=1234.5\n",
    "               input_tokens=100 output_tokens=50 total_tokens=150\n",
    "               cache_read_tokens=80 cache_write_tokens=20\n",
    "               cost_usd=0.001149 stop_reason=end_turn\n",
    "```\n",
    "\n",
    "**Always present**: provider, model, duration_ms, input/output/total tokens, cost_usd, stop_reason\n",
    "\n",
    "**Conditional**: cache_read_tokens, cache_write_tokens (omitted when None — reduces log noise for providers that don't use caching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Constructing TelemetryModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import AsyncMock, MagicMock\n",
    "from arcllm.modules.telemetry import TelemetryModule\n",
    "from arcllm.types import LLMProvider, LLMResponse, Message, Usage\n",
    "\n",
    "# Create a mock inner adapter\n",
    "inner = MagicMock(spec=LLMProvider)\n",
    "inner.name = \"anthropic\"\n",
    "inner.model_name = \"claude-sonnet-4-20250514\"\n",
    "inner.invoke = AsyncMock(return_value=LLMResponse(\n",
    "    content=\"Hello!\",\n",
    "    usage=Usage(input_tokens=100, output_tokens=50, total_tokens=150),\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    stop_reason=\"end_turn\",\n",
    "))\n",
    "\n",
    "# Config with Anthropic Sonnet pricing\n",
    "config = {\n",
    "    \"cost_input_per_1m\": 3.00,\n",
    "    \"cost_output_per_1m\": 15.00,\n",
    "    \"cost_cache_read_per_1m\": 0.30,\n",
    "    \"cost_cache_write_per_1m\": 3.75,\n",
    "}\n",
    "\n",
    "module = TelemetryModule(config, inner)\n",
    "\n",
    "print(f\"Module type:      {type(module).__name__}\")\n",
    "print(f\"Provider (inner): {module.name}\")\n",
    "print(f\"Model (inner):    {module.model_name}\")\n",
    "print(f\"Cost input/1M:    ${module._cost_input:.2f}\")\n",
    "print(f\"Cost output/1M:   ${module._cost_output:.2f}\")\n",
    "print(f\"Cost cache rd/1M: ${module._cost_cache_read:.2f}\")\n",
    "print(f\"Cost cache wr/1M: ${module._cost_cache_write:.2f}\")\n",
    "print(f\"Log level:        {module._log_level} (10=DEBUG, 20=INFO)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Cost Calculation\n",
    "\n",
    "Formula: `cost_usd = (input_tokens × cost_input / 1M) + (output_tokens × cost_output / 1M) + cache costs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cost: input + output only\n",
    "usage_basic = Usage(input_tokens=1000, output_tokens=500, total_tokens=1500)\n",
    "cost = module._calculate_cost(usage_basic)\n",
    "print(f\"Basic cost (1K in, 500 out):\")\n",
    "print(f\"  Input:  1000 × $3.00 / 1M = ${1000 * 3.00 / 1_000_000:.6f}\")\n",
    "print(f\"  Output: 500 × $15.00 / 1M = ${500 * 15.00 / 1_000_000:.6f}\")\n",
    "print(f\"  Total:  ${cost:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost with all 4 token types\n",
    "usage_full = Usage(\n",
    "    input_tokens=1000,\n",
    "    output_tokens=500,\n",
    "    total_tokens=1500,\n",
    "    cache_read_tokens=800,\n",
    "    cache_write_tokens=200,\n",
    ")\n",
    "cost_full = module._calculate_cost(usage_full)\n",
    "print(f\"Full cost (with cache):\")\n",
    "print(f\"  Input:       1000 × $3.00 / 1M = ${1000 * 3.00 / 1_000_000:.6f}\")\n",
    "print(f\"  Output:       500 × $15.00 / 1M = ${500 * 15.00 / 1_000_000:.6f}\")\n",
    "print(f\"  Cache read:   800 × $0.30 / 1M = ${800 * 0.30 / 1_000_000:.6f}\")\n",
    "print(f\"  Cache write:  200 × $3.75 / 1M = ${200 * 3.75 / 1_000_000:.6f}\")\n",
    "print(f\"  Total: ${cost_full:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At exactly 1M tokens — cost should match the per-1M prices\n",
    "usage_1m = Usage(input_tokens=1_000_000, output_tokens=1_000_000, total_tokens=2_000_000)\n",
    "cost_1m = module._calculate_cost(usage_1m)\n",
    "print(f\"At 1M tokens each: ${cost_1m:.2f} (= $3.00 input + $15.00 output)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero pricing → zero cost (module created with empty config)\n",
    "module_free = TelemetryModule({}, inner)\n",
    "cost_free = module_free._calculate_cost(usage_basic)\n",
    "print(f\"No pricing configured: ${cost_free:.6f}\")\n",
    "print(f\"All cost fields default to 0.0: input={module_free._cost_input}, output={module_free._cost_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Timing with time.monotonic()\n",
    "\n",
    "Wall-clock duration is measured around the entire `inner.invoke()` call, including any retries/fallback/rate-limit waits that happen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from unittest.mock import patch\n",
    "\n",
    "# Mock time to control exact duration\n",
    "with patch(\"arcllm.modules.telemetry.time.monotonic\") as mock_mono:\n",
    "    mock_mono.side_effect = [1000.0, 1000.5]  # 500ms elapsed\n",
    "    \n",
    "    inner = MagicMock(spec=LLMProvider)\n",
    "    inner.name = \"anthropic\"\n",
    "    inner.invoke = AsyncMock(return_value=LLMResponse(\n",
    "        content=\"ok\",\n",
    "        usage=Usage(input_tokens=100, output_tokens=50, total_tokens=150),\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        stop_reason=\"end_turn\",\n",
    "    ))\n",
    "    \n",
    "    module = TelemetryModule(config, inner)\n",
    "    \n",
    "    # Capture log output\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setLevel(logging.INFO)\n",
    "    tel_logger = logging.getLogger(\"arcllm.modules.telemetry\")\n",
    "    tel_logger.addHandler(handler)\n",
    "    tel_logger.setLevel(logging.INFO)\n",
    "    \n",
    "    result = await module.invoke([Message(role=\"user\", content=\"hi\")])\n",
    "    \n",
    "    tel_logger.removeHandler(handler)\n",
    "\n",
    "print(f\"\\nduration_ms = round(0.5 * 1000, 1) = 500.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timing code:\n",
    "```python\n",
    "start = time.monotonic()\n",
    "response = await self._inner.invoke(messages, tools, **kwargs)\n",
    "elapsed = time.monotonic() - start\n",
    "duration_ms = round(elapsed * 1000, 1)\n",
    "```\n",
    "\n",
    "`time.monotonic()` — immune to system clock adjustments (NTP corrections, DST changes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Conditional Cache Token Fields\n",
    "\n",
    "Cache tokens are only logged when present (not None). This reduces log noise for providers/calls that don't use prompt caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response WITHOUT cache tokens\n",
    "resp_no_cache = LLMResponse(\n",
    "    content=\"ok\",\n",
    "    usage=Usage(input_tokens=100, output_tokens=50, total_tokens=150),\n",
    "    model=\"test\",\n",
    "    stop_reason=\"end_turn\",\n",
    ")\n",
    "\n",
    "# Response WITH cache tokens\n",
    "resp_cached = LLMResponse(\n",
    "    content=\"cached\",\n",
    "    usage=Usage(\n",
    "        input_tokens=100, output_tokens=50, total_tokens=150,\n",
    "        cache_read_tokens=80, cache_write_tokens=20,\n",
    "    ),\n",
    "    model=\"test\",\n",
    "    stop_reason=\"end_turn\",\n",
    ")\n",
    "\n",
    "print(f\"No cache - cache_read_tokens: {resp_no_cache.usage.cache_read_tokens}\")\n",
    "print(f\"No cache - cache_write_tokens: {resp_no_cache.usage.cache_write_tokens}\")\n",
    "print(f\"Cached - cache_read_tokens: {resp_cached.usage.cache_read_tokens}\")\n",
    "print(f\"Cached - cache_write_tokens: {resp_cached.usage.cache_write_tokens}\")\n",
    "print(\"\\nWhen None → fields omitted from log line\")\n",
    "print(\"When present → fields included\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the conditional logic\n",
    "import logging\n",
    "\n",
    "# Test 1: No cache fields in log\n",
    "with patch(\"arcllm.modules.telemetry.time.monotonic\", side_effect=[0.0, 0.1]):\n",
    "    inner1 = MagicMock(spec=LLMProvider)\n",
    "    inner1.name = \"test\"\n",
    "    inner1.invoke = AsyncMock(return_value=resp_no_cache)\n",
    "    mod = TelemetryModule(config, inner1)\n",
    "    \n",
    "    handler = logging.StreamHandler()\n",
    "    tel_logger = logging.getLogger(\"arcllm.modules.telemetry\")\n",
    "    tel_logger.addHandler(handler)\n",
    "    tel_logger.setLevel(logging.INFO)\n",
    "    \n",
    "    print(\"=== Without cache tokens ===\")\n",
    "    await mod.invoke([Message(role=\"user\", content=\"hi\")])\n",
    "    print(\"(no cache_read_tokens or cache_write_tokens in output)\")\n",
    "    \n",
    "    tel_logger.removeHandler(handler)\n",
    "\n",
    "# Test 2: Cache fields present\n",
    "with patch(\"arcllm.modules.telemetry.time.monotonic\", side_effect=[0.0, 0.1]):\n",
    "    inner2 = MagicMock(spec=LLMProvider)\n",
    "    inner2.name = \"test\"\n",
    "    inner2.invoke = AsyncMock(return_value=resp_cached)\n",
    "    mod2 = TelemetryModule(config, inner2)\n",
    "    \n",
    "    tel_logger.addHandler(handler)\n",
    "    \n",
    "    print(\"\\n=== With cache tokens ===\")\n",
    "    await mod2.invoke([Message(role=\"user\", content=\"hi\")])\n",
    "    print(\"(cache_read_tokens=80 and cache_write_tokens=20 included)\")\n",
    "    \n",
    "    tel_logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Configurable Log Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.modules.telemetry import _VALID_LOG_LEVELS\n",
    "\n",
    "print(f\"Valid log levels: {sorted(_VALID_LOG_LEVELS)}\")\n",
    "print(f\"Default: INFO\")\n",
    "print()\n",
    "\n",
    "# DEBUG level — won't show at INFO\n",
    "debug_config = {**config, \"log_level\": \"DEBUG\"}\n",
    "inner_d = MagicMock(spec=LLMProvider)\n",
    "inner_d.name = \"test\"\n",
    "inner_d.invoke = AsyncMock(return_value=resp_no_cache)\n",
    "\n",
    "mod_debug = TelemetryModule(debug_config, inner_d)\n",
    "print(f\"Module log level: {mod_debug._log_level} (10=DEBUG)\")\n",
    "print(\"At INFO capture level, DEBUG messages are invisible\")\n",
    "print(\"At DEBUG capture level, they appear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Config Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.exceptions import ArcLLMConfigError\n",
    "\n",
    "inner = MagicMock(spec=LLMProvider)\n",
    "inner.name = \"test\"\n",
    "\n",
    "# Negative input cost → rejected\n",
    "try:\n",
    "    TelemetryModule({\"cost_input_per_1m\": -1.0}, inner)\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"Negative input cost:  {e}\")\n",
    "\n",
    "# Negative output cost → rejected\n",
    "try:\n",
    "    TelemetryModule({\"cost_output_per_1m\": -1.0}, inner)\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"Negative output cost: {e}\")\n",
    "\n",
    "# Invalid log level → rejected\n",
    "try:\n",
    "    TelemetryModule({\"log_level\": \"INVALID\"}, inner)\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"Invalid log level:    {e}\")\n",
    "\n",
    "# Empty config → all defaults to 0.0 (valid)\n",
    "mod_empty = TelemetryModule({}, inner)\n",
    "print(f\"\\nEmpty config (all zeros): input={mod_empty._cost_input}, output={mod_empty._cost_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Response Passthrough\n",
    "\n",
    "TelemetryModule returns the **exact same response object** — no copies, no modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with patch(\"arcllm.modules.telemetry.time.monotonic\", side_effect=[0.0, 0.1]):\n",
    "    original_response = LLMResponse(\n",
    "        content=\"original\",\n",
    "        usage=Usage(input_tokens=10, output_tokens=5, total_tokens=15),\n",
    "        model=\"test\",\n",
    "        stop_reason=\"end_turn\",\n",
    "    )\n",
    "    \n",
    "    inner = MagicMock(spec=LLMProvider)\n",
    "    inner.name = \"test\"\n",
    "    inner.invoke = AsyncMock(return_value=original_response)\n",
    "    mod = TelemetryModule(config, inner)\n",
    "    \n",
    "    result = await mod.invoke([Message(role=\"user\", content=\"hi\")])\n",
    "    \n",
    "    # Same object (is check, not equality)\n",
    "    print(f\"Same object? {result is original_response}\")\n",
    "    print(f\"Content: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Pricing Injection via load_model()\n",
    "\n",
    "The elegant part: pricing data lives in provider TOML files, and `load_model()` bridges it to TelemetryModule via `setdefault()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's in anthropic.toml:\n",
    "from arcllm.config import load_provider_config\n",
    "\n",
    "provider_config = load_provider_config(\"anthropic\")\n",
    "sonnet_meta = provider_config.models[\"claude-sonnet-4-20250514\"]\n",
    "haiku_meta = provider_config.models[\"claude-haiku-4-5-20251001\"]\n",
    "\n",
    "print(\"=== Sonnet Pricing (from anthropic.toml) ===\")\n",
    "print(f\"  Input:       ${sonnet_meta.cost_input_per_1m}/1M\")\n",
    "print(f\"  Output:      ${sonnet_meta.cost_output_per_1m}/1M\")\n",
    "print(f\"  Cache read:  ${sonnet_meta.cost_cache_read_per_1m}/1M\")\n",
    "print(f\"  Cache write: ${sonnet_meta.cost_cache_write_per_1m}/1M\")\n",
    "\n",
    "print(\"\\n=== Haiku Pricing ===\")\n",
    "print(f\"  Input:       ${haiku_meta.cost_input_per_1m}/1M\")\n",
    "print(f\"  Output:      ${haiku_meta.cost_output_per_1m}/1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.registry import load_model, clear_cache\n",
    "os.environ.setdefault(\"ANTHROPIC_API_KEY\", \"test-key\")\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", \"test-key\")\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "# load_model() auto-injects pricing from TOML metadata\n",
    "model = load_model(\"anthropic\", telemetry=True)\n",
    "\n",
    "print(\"=== Pricing injected by load_model() ===\")\n",
    "print(f\"  Input:       ${model._cost_input}/1M\")\n",
    "print(f\"  Output:      ${model._cost_output}/1M\")\n",
    "print(f\"  Cache read:  ${model._cost_cache_read}/1M\")\n",
    "print(f\"  Cache write: ${model._cost_cache_write}/1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cache()\n",
    "\n",
    "# Different model → different pricing\n",
    "model_haiku = load_model(\"anthropic\", \"claude-haiku-4-5-20251001\", telemetry=True)\n",
    "print(\"=== Haiku pricing (auto-injected) ===\")\n",
    "print(f\"  Input:  ${model_haiku._cost_input}/1M\")\n",
    "print(f\"  Output: ${model_haiku._cost_output}/1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cache()\n",
    "\n",
    "# Explicit override wins over TOML metadata (setdefault behavior)\n",
    "model_override = load_model(\"anthropic\", telemetry={\"cost_input_per_1m\": 99.0})\n",
    "print(\"=== Explicit override ===\")\n",
    "print(f\"  Input:  ${model_override._cost_input}/1M (override: 99.0)\")\n",
    "print(f\"  Output: ${model_override._cost_output}/1M (from TOML metadata)\")\n",
    "print(\"\\nsetdefault() only fills MISSING keys — explicit values are preserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The injection code in `registry.py`:\n",
    "```python\n",
    "model_meta = config.models.get(model_name)\n",
    "if model_meta is not None:\n",
    "    telemetry_config.setdefault(\"cost_input_per_1m\", model_meta.cost_input_per_1m)\n",
    "    telemetry_config.setdefault(\"cost_output_per_1m\", model_meta.cost_output_per_1m)\n",
    "    telemetry_config.setdefault(\"cost_cache_read_per_1m\", model_meta.cost_cache_read_per_1m)\n",
    "    telemetry_config.setdefault(\"cost_cache_write_per_1m\", model_meta.cost_cache_write_per_1m)\n",
    "```\n",
    "\n",
    "`setdefault()` only sets if the key doesn't already exist — so explicit overrides in the kwarg dict always win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Full Module Stack\n",
    "\n",
    "With all modules enabled: `Telemetry(Retry(Fallback(RateLimit(Adapter))))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cache()\n",
    "\n",
    "model = load_model(\n",
    "    \"anthropic\",\n",
    "    telemetry=True,\n",
    "    retry=True,\n",
    "    fallback=True,\n",
    "    rate_limit=True,\n",
    ")\n",
    "\n",
    "# Walk the stack\n",
    "layer = model\n",
    "depth = 0\n",
    "while hasattr(layer, '_inner'):\n",
    "    indent = \"  \" * depth\n",
    "    print(f\"{indent}{type(layer).__name__}\")\n",
    "    layer = layer._inner\n",
    "    depth += 1\n",
    "print(f\"{'  ' * depth}{type(layer).__name__}\")\n",
    "\n",
    "print(f\"\\n5 layers total — Telemetry measures everything below it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Telemetry is outermost:\n",
    "\n",
    "```\n",
    "Telemetry (measures total wall-clock, including:)\n",
    "  └─ Retry (adds retry wait time)\n",
    "      └─ Fallback (adds fallback attempt time)\n",
    "          └─ RateLimit (adds throttle wait time)\n",
    "              └─ Adapter (actual API call time)\n",
    "```\n",
    "\n",
    "The duration_ms you see in the log is the **total time from the agent's perspective** — the most useful operational metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Implementation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "print(\"=== TelemetryModule.invoke ===\")\n",
    "print(inspect.getsource(TelemetryModule.invoke))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TelemetryModule._calculate_cost ===\")\n",
    "print(inspect.getsource(TelemetryModule._calculate_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. config.toml Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.config import load_global_config\n",
    "\n",
    "global_config = load_global_config()\n",
    "tel_config = global_config.modules.get(\"telemetry\")\n",
    "\n",
    "print(\"=== config.toml [modules.telemetry] ===\")\n",
    "print(f\"  enabled:   {tel_config.enabled}\")\n",
    "print(f\"  log_level: {tel_config.model_dump().get('log_level', 'not set')}\")\n",
    "print()\n",
    "print(\"When enabled=true in config.toml, telemetry activates by default.\")\n",
    "print(\"kwarg=False can override to disable. kwarg=True forces enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Live API Call with Telemetry\n",
    "\n",
    "Real Anthropic API call with `telemetry=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join(os.getcwd(), '..', '.env'))\n",
    "\n",
    "has_key = bool(os.environ.get(\"ANTHROPIC_API_KEY\")) and os.environ.get(\"ANTHROPIC_API_KEY\") != \"test-key\"\n",
    "print(f\"Anthropic API key: {'found' if has_key else 'not found (skip live tests)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_key:\n",
    "    clear_cache()\n",
    "    \n",
    "    # Set up logging so we can see telemetry output\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(levelname)s %(name)s: %(message)s\"))\n",
    "    tel_logger = logging.getLogger(\"arcllm.modules.telemetry\")\n",
    "    tel_logger.addHandler(handler)\n",
    "    tel_logger.setLevel(logging.INFO)\n",
    "    \n",
    "    model = load_model(\"anthropic\", telemetry=True)\n",
    "    print(f\"Stack: {type(model).__name__}({type(model._inner).__name__})\")\n",
    "    print(f\"Pricing: ${model._cost_input}/1M input, ${model._cost_output}/1M output\")\n",
    "    print()\n",
    "    \n",
    "    response = await model.invoke([\n",
    "        Message(role=\"user\", content=\"What is 2+2? Just the number.\")\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nResponse: {response.content}\")\n",
    "    \n",
    "    tel_logger.removeHandler(handler)\n",
    "    await model._inner.close()\n",
    "else:\n",
    "    print(\"Skipped — no API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_key:\n",
    "    clear_cache()\n",
    "    \n",
    "    # Haiku model — different pricing\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(levelname)s %(name)s: %(message)s\"))\n",
    "    tel_logger = logging.getLogger(\"arcllm.modules.telemetry\")\n",
    "    tel_logger.addHandler(handler)\n",
    "    tel_logger.setLevel(logging.INFO)\n",
    "    \n",
    "    model = load_model(\"anthropic\", \"claude-haiku-4-5-20251001\", telemetry=True)\n",
    "    print(f\"Haiku pricing: ${model._cost_input}/1M input, ${model._cost_output}/1M output\")\n",
    "    print()\n",
    "    \n",
    "    response = await model.invoke([\n",
    "        Message(role=\"user\", content=\"What is the speed of light? One sentence.\")\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nResponse: {response.content}\")\n",
    "    print(f\"(Note: Haiku cost per call should be ~4x cheaper than Sonnet)\")\n",
    "    \n",
    "    tel_logger.removeHandler(handler)\n",
    "    await model._inner.close()\n",
    "else:\n",
    "    print(\"Skipped — no API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_key:\n",
    "    clear_cache()\n",
    "    \n",
    "    # Full stack: telemetry + retry + rate_limit\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(levelname)s %(name)s: %(message)s\"))\n",
    "    tel_logger = logging.getLogger(\"arcllm.modules.telemetry\")\n",
    "    tel_logger.addHandler(handler)\n",
    "    tel_logger.setLevel(logging.INFO)\n",
    "    \n",
    "    model = load_model(\"anthropic\", telemetry=True, retry=True, rate_limit=True)\n",
    "    \n",
    "    # Walk the stack\n",
    "    layer = model\n",
    "    layers = []\n",
    "    while hasattr(layer, '_inner'):\n",
    "        layers.append(type(layer).__name__)\n",
    "        layer = layer._inner\n",
    "    layers.append(type(layer).__name__)\n",
    "    print(f\"Stack: {' → '.join(layers)}\")\n",
    "    print()\n",
    "    \n",
    "    response = await model.invoke([\n",
    "        Message(role=\"user\", content=\"Say 'telemetry works' in exactly those words.\")\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nResponse: {response.content}\")\n",
    "    \n",
    "    tel_logger.removeHandler(handler)\n",
    "    # Close the innermost adapter\n",
    "    innermost = model\n",
    "    while hasattr(innermost, '_inner'):\n",
    "        innermost = innermost._inner\n",
    "    await innermost.close()\n",
    "else:\n",
    "    print(\"Skipped — no API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Component | What | Why |\n",
    "|-----------|------|-----|\n",
    "| `TelemetryModule` | Logs timing + tokens + cost per `invoke()` | Operational visibility for thousands of agents |\n",
    "| `_calculate_cost()` | `(tokens × cost_per_1M) / 1,000,000` | Per-call cost tracking |\n",
    "| Pricing injection | `load_model()` uses `setdefault()` from provider TOML | Pricing lives in TOML, module doesn't know about ProviderConfig |\n",
    "| Conditional fields | Cache tokens omitted when None | Reduces log noise |\n",
    "| `log_level` | Configurable (default INFO) | Toggle verbosity per environment |\n",
    "| Stack position | Outermost | Measures total wall-clock including retries/fallback/rate-limit |\n",
    "\n",
    "**Config**:\n",
    "```python\n",
    "load_model(\"anthropic\", telemetry=True)                              # Auto-pricing from TOML\n",
    "load_model(\"anthropic\", telemetry={\"log_level\": \"DEBUG\"})            # Custom log level\n",
    "load_model(\"anthropic\", telemetry={\"cost_input_per_1m\": 99.0})      # Override pricing\n",
    "load_model(\"anthropic\", telemetry=False)                             # Disable\n",
    "```\n",
    "\n",
    "**Log output**:\n",
    "```\n",
    "INFO  arcllm.modules.telemetry: LLM call | provider=anthropic model=claude-sonnet-4-20250514\n",
    "  duration_ms=1234.5 input_tokens=100 output_tokens=50 total_tokens=150\n",
    "  cost_usd=0.001050 stop_reason=end_turn\n",
    "```\n",
    "\n",
    "**Test count**: 272 passed, 1 skipped (21 new telemetry tests + 7 new registry tests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}