{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ArcLLM Step 6: Provider Registry + load_model()\n",
    "\n",
    "This notebook walks through everything built in Step 6 — the **provider registry** that lets agents get a configured model object with a single function call.\n",
    "\n",
    "**What was built:**\n",
    "- `load_model()` — the public API entry point. One call, ready to invoke.\n",
    "- `_get_adapter_class()` — convention-based adapter discovery (no mapping dict)\n",
    "- `clear_cache()` — reset the config cache for testing\n",
    "- Module-level config caching for performance at scale\n",
    "- `OpenAIAdapter` renamed to `OpenaiAdapter` (naming convention compliance)\n",
    "\n",
    "**Why it matters:** Before this step, agents had to manually load configs, resolve API keys, pick adapter classes, and wire everything together. Now it's:\n",
    "\n",
    "```python\n",
    "from arcllm import load_model\n",
    "\n",
    "model = load_model(\"anthropic\")\n",
    "resp = await model.invoke(messages, tools)\n",
    "```\n",
    "\n",
    "That's it. Config loaded from TOML, API key resolved from env, adapter class discovered by convention, model object ready to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from unittest.mock import AsyncMock, patch\n",
    "import httpx\n",
    "\n",
    "from arcllm import (\n",
    "    load_model, clear_cache,\n",
    "    AnthropicAdapter, OpenaiAdapter, BaseAdapter,\n",
    "    ArcLLMAPIError, ArcLLMConfigError,\n",
    "    ProviderConfig, ProviderSettings, ModelMetadata,\n",
    "    Message, TextBlock, ToolUseBlock, ToolResultBlock,\n",
    "    Tool, ToolCall, Usage, LLMResponse, LLMProvider, StopReason,\n",
    "    load_provider_config,\n",
    ")\n",
    "print(\"All imports successful — including load_model and clear_cache!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Convention: How load_model() Finds Everything\n",
    "\n",
    "The registry is **convention-based** — the provider name string drives everything:\n",
    "\n",
    "```\n",
    "load_model(\"anthropic\")\n",
    "         ↓\n",
    "   provider name: \"anthropic\"\n",
    "         ↓\n",
    "   ┌─ TOML config:  providers/anthropic.toml\n",
    "   ├─ Module path:  arcllm.adapters.anthropic\n",
    "   └─ Class name:   AnthropicAdapter  (\"anthropic\".title() + \"Adapter\")\n",
    "```\n",
    "\n",
    "No mapping dictionary. No registration step. The **file structure IS the registry**. Add a new provider? Drop a `.toml` and a `.py` — it works automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The naming convention in action\n",
    "for provider in [\"anthropic\", \"openai\"]:\n",
    "    toml_path = f\"providers/{provider}.toml\"\n",
    "    module_path = f\"arcllm.adapters.{provider}\"\n",
    "    class_name = f\"{provider.title()}Adapter\"\n",
    "    print(f\"  '{provider}':\")\n",
    "    print(f\"    TOML:   {toml_path}\")\n",
    "    print(f\"    Module: {module_path}\")\n",
    "    print(f\"    Class:  {class_name}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### 1a. Why OpenAIAdapter Was Renamed to OpenaiAdapter\n",
    "\n",
    "The convention uses `provider_name.title()` to build the class name:\n",
    "- `\"anthropic\".title()` = `\"Anthropic\"` + `\"Adapter\"` = `AnthropicAdapter`\n",
    "- `\"openai\".title()` = `\"Openai\"` + `\"Adapter\"` = `OpenaiAdapter`\n",
    "\n",
    "The old name `OpenAIAdapter` wouldn't match the convention. Rather than add a special-case mapping dict, we renamed the class. Convention over configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the rename\n",
    "print(f\"'openai'.title() = {repr('openai'.title())}\")\n",
    "print(f\"Class name:        {'openai'.title()}Adapter\")\n",
    "print(f\"\\nOpenaiAdapter exists: {OpenaiAdapter is not None}\")\n",
    "print(f\"OpenaiAdapter name:  {OpenaiAdapter.__name__}\")\n",
    "\n",
    "# The old name no longer exists\n",
    "try:\n",
    "    from arcllm import OpenAIAdapter\n",
    "    print(\"\\nOpenAIAdapter still importable (backward compat)\")\n",
    "except ImportError:\n",
    "    print(\"\\nOpenAIAdapter removed — use OpenaiAdapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. load_model() — The Public API\n",
    "\n",
    "This is the function agents use. Let's trace through everything it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set up API keys (load_model resolves them from env)\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-test-key-for-walkthrough\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-openai-test-key-for-walkthrough\"\n",
    "\n",
    "# Clear cache to start fresh\n",
    "clear_cache()\n",
    "print(\"API keys set, cache cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an Anthropic model — one call does everything\n",
    "model = load_model(\"anthropic\")\n",
    "\n",
    "print(f\"Type:           {type(model).__name__}\")\n",
    "print(f\"Is LLMProvider: {isinstance(model, LLMProvider)}\")\n",
    "print(f\"Is BaseAdapter: {isinstance(model, BaseAdapter)}\")\n",
    "print(f\"Adapter name:   {model.name}\")\n",
    "print(f\"Model name:     {model._model_name}\")\n",
    "print(f\"Has API key:    {bool(model._api_key)}\")\n",
    "print(f\"Has client:     {model._client is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an OpenAI model\n",
    "model = load_model(\"openai\")\n",
    "\n",
    "print(f\"Type:           {type(model).__name__}\")\n",
    "print(f\"Is LLMProvider: {isinstance(model, LLMProvider)}\")\n",
    "print(f\"Adapter name:   {model.name}\")\n",
    "print(f\"Model name:     {model._model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Default Model vs Explicit Model\n",
    "\n",
    "When you call `load_model(\"anthropic\")` without a model argument, it uses `default_model` from the provider TOML. You can override it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model — from anthropic.toml [provider].default_model\n",
    "default_model = load_model(\"anthropic\")\n",
    "print(f\"Default model: {default_model._model_name}\")\n",
    "print(f\"  (from anthropic.toml: default_model = 'claude-sonnet-4-20250514')\")\n",
    "\n",
    "# Explicit model — overrides the default\n",
    "haiku_model = load_model(\"anthropic\", \"claude-haiku-4-5-20251001\")\n",
    "print(f\"\\nExplicit model: {haiku_model._model_name}\")\n",
    "\n",
    "# Model metadata is looked up from the TOML models section\n",
    "print(f\"\\nDefault model metadata:\")\n",
    "print(f\"  context_window:   {default_model._model_meta.context_window:,}\")\n",
    "print(f\"  max_output:       {default_model._model_meta.max_output_tokens:,}\")\n",
    "print(f\"  supports_tools:   {default_model._model_meta.supports_tools}\")\n",
    "print(f\"  supports_thinking: {default_model._model_meta.supports_thinking}\")\n",
    "\n",
    "print(f\"\\nHaiku model metadata:\")\n",
    "print(f\"  context_window:   {haiku_model._model_meta.context_window:,}\")\n",
    "print(f\"  cost_input_per_1m: ${haiku_model._model_meta.cost_input_per_1m}\")\n",
    "print(f\"  supports_thinking: {haiku_model._model_meta.supports_thinking}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for OpenAI\n",
    "default_oai = load_model(\"openai\")\n",
    "print(f\"OpenAI default: {default_oai._model_name}\")\n",
    "\n",
    "mini_oai = load_model(\"openai\", \"gpt-4o-mini\")\n",
    "print(f\"OpenAI mini:    {mini_oai._model_name}\")\n",
    "print(f\"  cost_input:   ${mini_oai._model_meta.cost_input_per_1m}/1M tokens\")\n",
    "print(f\"  cost_output:  ${mini_oai._model_meta.cost_output_per_1m}/1M tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unknown model name — _model_meta is None (not an error)\n",
    "unknown = load_model(\"anthropic\", \"claude-nonexistent-99\")\n",
    "print(f\"Unknown model name: {unknown._model_name}\")\n",
    "print(f\"Model metadata:     {unknown._model_meta}\")\n",
    "print(f\"\\nNot an error — the adapter still works, it just won't have metadata.\")\n",
    "print(\"This allows using new models before updating the TOML.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. _get_adapter_class() — Convention-Based Discovery\n",
    "\n",
    "This private function is the heart of the convention:\n",
    "1. Build module path: `arcllm.adapters.{provider_name}`\n",
    "2. `importlib.import_module()` to load the module\n",
    "3. Build class name: `{provider_name.title()}Adapter`\n",
    "4. `getattr(module, class_name)` to get the class\n",
    "5. Return the class (not an instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.registry import _get_adapter_class\n",
    "\n",
    "# Get the class for each provider\n",
    "for provider in [\"anthropic\", \"openai\"]:\n",
    "    cls = _get_adapter_class(provider)\n",
    "    print(f\"  '{provider}' -> {cls.__name__}\")\n",
    "    print(f\"    module: {cls.__module__}\")\n",
    "    print(f\"    is BaseAdapter subclass: {issubclass(cls, BaseAdapter)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class is lazy-loaded via importlib — not imported at init time\n",
    "import importlib\n",
    "\n",
    "# This is what _get_adapter_class does internally:\n",
    "provider = \"anthropic\"\n",
    "module_path = f\"arcllm.adapters.{provider}\"\n",
    "module = importlib.import_module(module_path)\n",
    "\n",
    "class_name = f\"{provider.title()}Adapter\"\n",
    "adapter_class = getattr(module, class_name)\n",
    "\n",
    "print(f\"Module path:  {module_path}\")\n",
    "print(f\"Class name:   {class_name}\")\n",
    "print(f\"Found class:  {adapter_class}\")\n",
    "print(f\"\\nLazy loading means unused providers never import their adapter code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Config Caching — Performance at Scale\n",
    "\n",
    "When thousands of agents call `load_model(\"anthropic\")`, we don't want to re-parse `anthropic.toml` every time. The registry uses a module-level cache.\n",
    "\n",
    "```python\n",
    "_provider_config_cache: dict[str, ProviderConfig] = {}\n",
    "```\n",
    "\n",
    "First call loads + caches. Subsequent calls hit the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcllm.registry import _provider_config_cache\n",
    "\n",
    "# Start with a clean cache\n",
    "clear_cache()\n",
    "print(f\"Cache empty: {len(_provider_config_cache)} entries\")\n",
    "\n",
    "# First call — loads from TOML\n",
    "model1 = load_model(\"anthropic\")\n",
    "print(f\"After first load_model('anthropic'): {len(_provider_config_cache)} entry\")\n",
    "print(f\"  Cached providers: {list(_provider_config_cache.keys())}\")\n",
    "\n",
    "# Second call — hits cache (no TOML parsing)\n",
    "model2 = load_model(\"anthropic\")\n",
    "print(f\"After second load_model('anthropic'): still {len(_provider_config_cache)} entry\")\n",
    "\n",
    "# Different provider — separate cache entry\n",
    "model3 = load_model(\"openai\")\n",
    "print(f\"After load_model('openai'): {len(_provider_config_cache)} entries\")\n",
    "print(f\"  Cached providers: {list(_provider_config_cache.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prove caching works: mock load_provider_config and count calls\n",
    "clear_cache()\n",
    "\n",
    "real_loader = __import__(\"arcllm.config\", fromlist=[\"load_provider_config\"]).load_provider_config\n",
    "\n",
    "with patch(\"arcllm.registry.load_provider_config\", wraps=real_loader) as mock_load:\n",
    "    load_model(\"anthropic\")   # call 1 — loads config\n",
    "    load_model(\"anthropic\")   # call 2 — cache hit\n",
    "    load_model(\"anthropic\")   # call 3 — cache hit\n",
    "    load_model(\"openai\")      # call 4 — loads config (different provider)\n",
    "    load_model(\"openai\")      # call 5 — cache hit\n",
    "\n",
    "    print(f\"load_model() called 5 times\")\n",
    "    print(f\"load_provider_config() called {mock_load.call_count} times\")\n",
    "    print(f\"\\nCache saved {5 - mock_load.call_count} TOML parses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_cache() resets everything — next call re-parses TOML\n",
    "with patch(\"arcllm.registry.load_provider_config\", wraps=real_loader) as mock_load:\n",
    "    load_model(\"anthropic\")   # loads config\n",
    "    print(f\"Before clear: load count = {mock_load.call_count}\")\n",
    "\n",
    "    clear_cache()\n",
    "    load_model(\"anthropic\")   # re-loads config (cache was cleared)\n",
    "    print(f\"After clear:  load count = {mock_load.call_count}\")\n",
    "    print(f\"\\nclear_cache() is essential for test isolation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Error Handling — Clear Errors for Every Failure Mode\n",
    "\n",
    "When something goes wrong, the error message tells you exactly what to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing provider TOML — no file at providers/nonexistent.toml\n",
    "clear_cache()\n",
    "try:\n",
    "    load_model(\"nonexistent\")\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"Missing provider TOML:\")\n",
    "    print(f\"  {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path traversal attack — rejected by config loader's regex\n",
    "clear_cache()\n",
    "for attack in [\"../etc/passwd\", \"../../secrets\", \".hidden\", \"UPPERCASE\"]:\n",
    "    try:\n",
    "        load_model(attack)\n",
    "    except ArcLLMConfigError as e:\n",
    "        print(f\"  '{attack}' -> BLOCKED: {str(e)[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty provider name\n",
    "clear_cache()\n",
    "try:\n",
    "    load_model(\"\")\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"Empty provider name:\")\n",
    "    print(f\"  {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing adapter module — TOML exists but no adapter .py file\n",
    "clear_cache()\n",
    "with patch(\"arcllm.registry.load_provider_config\") as mock_config:\n",
    "    # Fake a successful config load, but the adapter module won't exist\n",
    "    mock_config.return_value = type(\"FakeConfig\", (), {\n",
    "        \"provider\": type(\"FakeProvider\", (), {\"default_model\": \"test\"})()\n",
    "    })()\n",
    "    try:\n",
    "        load_model(\"nosuchadapter\")\n",
    "    except ArcLLMConfigError as e:\n",
    "        print(f\"Missing adapter module:\")\n",
    "        print(f\"  {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing adapter class — module exists but convention-named class doesn't\n",
    "import types as stdlib_types\n",
    "\n",
    "with patch(\"importlib.import_module\") as mock_import:\n",
    "    fake_module = stdlib_types.ModuleType(\"arcllm.adapters.fakeprov\")\n",
    "    mock_import.return_value = fake_module\n",
    "    try:\n",
    "        _get_adapter_class(\"fakeprov\")\n",
    "    except ArcLLMConfigError as e:\n",
    "        print(f\"Missing adapter class:\")\n",
    "        print(f\"  {e}\")\n",
    "        print(f\"\\nThe error names the EXPECTED class — easy to fix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing API key — caught at adapter construction (fail-fast)\n",
    "clear_cache()\n",
    "saved_key = os.environ.pop(\"ANTHROPIC_API_KEY\")\n",
    "try:\n",
    "    load_model(\"anthropic\")\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"Missing API key:\")\n",
    "    print(f\"  {e}\")\n",
    "    print(f\"\\nCaught at init, not during an invoke() call hours later.\")\n",
    "finally:\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = saved_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. The Complete Data Flow\n",
    "\n",
    "Let's trace the entire journey from `load_model()` to a working `invoke()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cache()\n",
    "\n",
    "# Step 1: load_model() — the one-liner agents use\n",
    "model = load_model(\"anthropic\")\n",
    "\n",
    "print(\"What load_model('anthropic') did:\")\n",
    "print(f\"  1. Loaded providers/anthropic.toml -> ProviderConfig\")\n",
    "print(f\"  2. Cached the config (won't re-parse next time)\")\n",
    "print(f\"  3. Resolved model: '{model._model_name}' (from default_model)\")\n",
    "print(f\"  4. Imported arcllm.adapters.anthropic\")\n",
    "print(f\"  5. Found class: {type(model).__name__}\")\n",
    "print(f\"  6. Constructed adapter:\")\n",
    "print(f\"     - Config injected\")\n",
    "print(f\"     - API key resolved from $ANTHROPIC_API_KEY\")\n",
    "print(f\"     - httpx.AsyncClient created (60s timeout)\")\n",
    "print(f\"  7. Returned ready-to-use LLMProvider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Agent uses the model — mocked HTTP for this walkthrough\n",
    "model._client = AsyncMock()\n",
    "model._client.post = AsyncMock(return_value=httpx.Response(\n",
    "    200,\n",
    "    json={\n",
    "        \"id\": \"msg_demo\", \"type\": \"message\", \"role\": \"assistant\",\n",
    "        \"model\": \"claude-sonnet-4-20250514\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Hello from Anthropic!\"}],\n",
    "        \"stop_reason\": \"end_turn\",\n",
    "        \"usage\": {\"input_tokens\": 10, \"output_tokens\": 5},\n",
    "    },\n",
    "    request=httpx.Request(\"POST\", \"https://api.anthropic.com/v1/messages\"),\n",
    "))\n",
    "\n",
    "resp = await model.invoke([Message(role=\"user\", content=\"Hello!\")])\n",
    "print(f\"Response: {resp.content!r}\")\n",
    "print(f\"Model:    {resp.model}\")\n",
    "print(f\"Tokens:   {resp.usage.total_tokens}\")\n",
    "print(f\"\\nFrom load_model() to response — that's the whole API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Provider Switching — The Real Payoff\n",
    "\n",
    "Want to switch from Anthropic to OpenAI? Change one string. The rest of the agent code stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cache()\n",
    "\n",
    "# Same agent function, different provider strings\n",
    "async def agent_demo(provider_name):\n",
    "    model = load_model(provider_name)\n",
    "    print(f\"  Provider:    {provider_name}\")\n",
    "    print(f\"  Adapter:     {type(model).__name__}\")\n",
    "    print(f\"  Model:       {model._model_name}\")\n",
    "    print(f\"  Base URL:    {model._config.provider.base_url}\")\n",
    "    print(f\"  API key env: {model._config.provider.api_key_env}\")\n",
    "    print()\n",
    "\n",
    "print(\"Provider switching — change one string:\")\n",
    "print(\"=\" * 50)\n",
    "await agent_demo(\"anthropic\")\n",
    "await agent_demo(\"openai\")\n",
    "print(\"Everything else (messages, tools, response handling) is identical.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison of what each provider gets\n",
    "clear_cache()\n",
    "providers = {\n",
    "    \"anthropic\": load_model(\"anthropic\"),\n",
    "    \"openai\": load_model(\"openai\"),\n",
    "}\n",
    "\n",
    "print(f\"{'Property':<22} {'Anthropic':<35} {'OpenAI'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for prop, getter in [\n",
    "    (\"Adapter class\", lambda m: type(m).__name__),\n",
    "    (\"Model name\", lambda m: m._model_name),\n",
    "    (\"Context window\", lambda m: f\"{m._model_meta.context_window:,}\"),\n",
    "    (\"Max output\", lambda m: f\"{m._model_meta.max_output_tokens:,}\"),\n",
    "    (\"Supports tools\", lambda m: str(m._model_meta.supports_tools)),\n",
    "    (\"Supports thinking\", lambda m: str(m._model_meta.supports_thinking)),\n",
    "    (\"Cost (input/1M)\", lambda m: f\"${m._model_meta.cost_input_per_1m}\"),\n",
    "    (\"Cost (output/1M)\", lambda m: f\"${m._model_meta.cost_output_per_1m}\"),\n",
    "    (\"API format\", lambda m: m._config.provider.api_format),\n",
    "    (\"Base URL\", lambda m: m._config.provider.base_url),\n",
    "]:\n",
    "    a = getter(providers[\"anthropic\"])\n",
    "    o = getter(providers[\"openai\"])\n",
    "    print(f\"{prop:<22} {a:<35} {o}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. The Complete Agent Pattern\n",
    "\n",
    "Here's what an agent looks like now — from import to agentic loop. This is the pattern all ArcLLM agents follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def complete_agent_demo():\n",
    "    \"\"\"A complete agent using load_model() — the target API.\"\"\"\n",
    "\n",
    "    # === Setup (one line!) ===\n",
    "    model = load_model(\"anthropic\")\n",
    "\n",
    "    # === Define tools ===\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"get_weather\",\n",
    "            description=\"Get current weather for a city\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"city\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"city\"],\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    def execute_tool(name, arguments):\n",
    "        if name == \"get_weather\":\n",
    "            return f\"{arguments['city']}: 75F, sunny\"\n",
    "        return \"Unknown tool\"\n",
    "\n",
    "    # === Build conversation ===\n",
    "    messages = [\n",
    "        Message(role=\"system\", content=\"Use tools to answer. Be concise.\"),\n",
    "        Message(role=\"user\", content=\"What's the weather in Austin?\"),\n",
    "    ]\n",
    "\n",
    "    # === Mock HTTP (for walkthrough) ===\n",
    "    model._client = AsyncMock()\n",
    "    responses = iter([\n",
    "        httpx.Response(200, json={\n",
    "            \"id\": \"msg_1\", \"type\": \"message\", \"role\": \"assistant\",\n",
    "            \"model\": \"claude-sonnet-4-20250514\",\n",
    "            \"content\": [{\"type\": \"tool_use\", \"id\": \"t1\", \"name\": \"get_weather\", \"input\": {\"city\": \"Austin\"}}],\n",
    "            \"stop_reason\": \"tool_use\",\n",
    "            \"usage\": {\"input_tokens\": 50, \"output_tokens\": 20},\n",
    "        }, request=httpx.Request(\"POST\", \"https://api.anthropic.com/v1/messages\")),\n",
    "        httpx.Response(200, json={\n",
    "            \"id\": \"msg_2\", \"type\": \"message\", \"role\": \"assistant\",\n",
    "            \"model\": \"claude-sonnet-4-20250514\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Austin is 75F and sunny.\"}],\n",
    "            \"stop_reason\": \"end_turn\",\n",
    "            \"usage\": {\"input_tokens\": 80, \"output_tokens\": 10},\n",
    "        }, request=httpx.Request(\"POST\", \"https://api.anthropic.com/v1/messages\")),\n",
    "    ])\n",
    "\n",
    "    # === Agentic loop ===\n",
    "    turn = 0\n",
    "    while True:\n",
    "        turn += 1\n",
    "        model._client.post = AsyncMock(return_value=next(responses))\n",
    "\n",
    "        resp = await model.invoke(messages, tools=tools)\n",
    "\n",
    "        if resp.stop_reason == \"end_turn\":\n",
    "            print(f\"Turn {turn}: DONE — {resp.content}\")\n",
    "            break\n",
    "\n",
    "        if resp.stop_reason == \"tool_use\":\n",
    "            for tc in resp.tool_calls:\n",
    "                result = execute_tool(tc.name, tc.arguments)\n",
    "                print(f\"Turn {turn}: {tc.name}({tc.arguments}) -> {result}\")\n",
    "\n",
    "                messages.append(Message(\n",
    "                    role=\"assistant\",\n",
    "                    content=[ToolUseBlock(id=tc.id, name=tc.name, arguments=tc.arguments)],\n",
    "                ))\n",
    "                messages.append(Message(\n",
    "                    role=\"tool\",\n",
    "                    content=[ToolResultBlock(tool_use_id=tc.id, content=result)],\n",
    "                ))\n",
    "\n",
    "        if turn > 5:\n",
    "            break\n",
    "\n",
    "    print(f\"\\n{turn} turns, {len(messages)} messages\")\n",
    "    print(\"\\nSwap 'anthropic' for 'openai' and the ENTIRE loop works the same.\")\n",
    "\n",
    "await complete_agent_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Adding a New Provider — Zero Code in the Registry\n",
    "\n",
    "The convention-based approach means adding a provider requires zero changes to `registry.py`. Let's trace what you'd do to add (hypothetical) Ollama support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What you'd need for a new provider:\n",
    "steps = [\n",
    "    (\"1. Create TOML\",       \"src/arcllm/providers/ollama.toml\",    \"Provider config + model metadata\"),\n",
    "    (\"2. Create adapter\",    \"src/arcllm/adapters/ollama.py\",       \"class OllamaAdapter(BaseAdapter)\"),\n",
    "    (\"3. That's it!\",        \"load_model('ollama')\",                 \"Convention handles the rest\"),\n",
    "]\n",
    "\n",
    "print(\"Adding a new provider — 2 files, 0 registry changes:\")\n",
    "print(\"=\" * 65)\n",
    "for step, path, desc in steps:\n",
    "    print(f\"  {step:<20} {path:<40} {desc}\")\n",
    "\n",
    "print(f\"\\nThe convention:\")\n",
    "print(f\"  'ollama' -> providers/ollama.toml\")\n",
    "print(f\"  'ollama' -> arcllm.adapters.ollama\")\n",
    "print(f\"  'ollama' -> OllamaAdapter  ('ollama'.title() + 'Adapter')\")\n",
    "print(f\"\\nNo mapping dict. No registration call. No __init__.py changes.\")\n",
    "print(f\"The file structure IS the registry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Inspecting the Registry Internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full registry module — only 76 lines\n",
    "import arcllm.registry as registry\n",
    "import inspect\n",
    "\n",
    "# Public API\n",
    "public = [name for name in dir(registry) if not name.startswith(\"_\")]\n",
    "print(f\"Public API: {public}\")\n",
    "\n",
    "# Private internals\n",
    "private = [name for name in dir(registry) if name.startswith(\"_\") and not name.startswith(\"__\")]\n",
    "print(f\"Private:    {private}\")\n",
    "\n",
    "# Function signatures\n",
    "print(f\"\\nSignatures:\")\n",
    "for name in [\"load_model\", \"clear_cache\", \"_get_adapter_class\"]:\n",
    "    func = getattr(registry, name)\n",
    "    sig = inspect.signature(func)\n",
    "    print(f\"  {name}{sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache state inspection\n",
    "clear_cache()\n",
    "print(f\"Cache after clear_cache(): {dict(registry._provider_config_cache)}\")\n",
    "\n",
    "load_model(\"anthropic\")\n",
    "print(f\"Cache after load_model('anthropic'):\")\n",
    "for provider, config in registry._provider_config_cache.items():\n",
    "    print(f\"  '{provider}':\")\n",
    "    print(f\"    base_url:      {config.provider.base_url}\")\n",
    "    print(f\"    default_model: {config.provider.default_model}\")\n",
    "    print(f\"    models:        {list(config.models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Live API — load_model() End to End\n",
    "\n",
    "Let's use `load_model()` with the real Anthropic API key from `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "clear_cache()\n",
    "\n",
    "print(f\"API key: {os.environ.get('ANTHROPIC_API_KEY', 'NOT SET')[:12]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The one-liner: load_model() -> invoke()\n",
    "async def live_one_liner():\n",
    "    model = load_model(\"anthropic\")\n",
    "    async with model:\n",
    "        resp = await model.invoke(\n",
    "            [Message(role=\"user\", content=\"What is 2 + 2? One word.\")],\n",
    "            max_tokens=10, temperature=0.0,\n",
    "        )\n",
    "\n",
    "    print(f\"Content:     {resp.content!r}\")\n",
    "    print(f\"Model:       {resp.model}\")\n",
    "    print(f\"Stop reason: {resp.stop_reason}\")\n",
    "    print(f\"Tokens:      {resp.usage.total_tokens}\")\n",
    "    print(f\"\\nTwo lines: load_model() + model.invoke(). That's the entire API.\")\n",
    "\n",
    "await live_one_liner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicit model selection — use Haiku for cheap tasks\n",
    "async def live_model_selection():\n",
    "    sonnet = load_model(\"anthropic\")  # default: claude-sonnet-4\n",
    "    haiku = load_model(\"anthropic\", \"claude-haiku-4-5-20251001\")\n",
    "\n",
    "    print(f\"Sonnet: {sonnet._model_name}\")\n",
    "    print(f\"  cost: ${sonnet._model_meta.cost_input_per_1m}/1M input, ${sonnet._model_meta.cost_output_per_1m}/1M output\")\n",
    "    print(f\"Haiku:  {haiku._model_name}\")\n",
    "    print(f\"  cost: ${haiku._model_meta.cost_input_per_1m}/1M input, ${haiku._model_meta.cost_output_per_1m}/1M output\")\n",
    "\n",
    "    async with haiku:\n",
    "        resp = await haiku.invoke(\n",
    "            [Message(role=\"user\", content=\"Say 'hello' and nothing else.\")],\n",
    "            max_tokens=10, temperature=0.0,\n",
    "        )\n",
    "    print(f\"\\nHaiku response: {resp.content!r}\")\n",
    "    print(f\"Tokens:         {resp.usage.total_tokens}\")\n",
    "    print(f\"\\nUse Sonnet for complex reasoning, Haiku for cheap classification/routing.\")\n",
    "\n",
    "await live_model_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full agentic loop via load_model()\n",
    "async def live_agentic_loop():\n",
    "    model = load_model(\"anthropic\")\n",
    "\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"calculate\",\n",
    "            description=\"Evaluate a mathematical expression and return the result\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"expression\": {\"type\": \"string\", \"description\": \"Math expression\"}},\n",
    "                \"required\": [\"expression\"],\n",
    "            },\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    messages = [\n",
    "        Message(role=\"user\", content=\"What is (17 * 23) + (31 * 11)? Use the calculator tool.\"),\n",
    "    ]\n",
    "\n",
    "    total_tokens = 0\n",
    "    turn = 0\n",
    "\n",
    "    async with model:\n",
    "        while True:\n",
    "            turn += 1\n",
    "            resp = await model.invoke(messages, tools=tools, max_tokens=300, temperature=0.0)\n",
    "            total_tokens += resp.usage.total_tokens\n",
    "\n",
    "            print(f\"Turn {turn}: stop_reason={resp.stop_reason}\")\n",
    "\n",
    "            if resp.stop_reason == \"end_turn\":\n",
    "                print(f\"  Answer: {resp.content}\")\n",
    "                break\n",
    "\n",
    "            if resp.stop_reason == \"tool_use\":\n",
    "                content_blocks = []\n",
    "                if resp.content:\n",
    "                    content_blocks.append(TextBlock(text=resp.content))\n",
    "                for tc in resp.tool_calls:\n",
    "                    content_blocks.append(\n",
    "                        ToolUseBlock(id=tc.id, name=tc.name, arguments=tc.arguments)\n",
    "                    )\n",
    "                messages.append(Message(role=\"assistant\", content=content_blocks))\n",
    "\n",
    "                result_blocks = []\n",
    "                for tc in resp.tool_calls:\n",
    "                    try:\n",
    "                        result = str(eval(tc.arguments[\"expression\"]))\n",
    "                    except Exception:\n",
    "                        result = \"Error\"\n",
    "                    print(f\"  Tool: calculate({tc.arguments['expression']}) = {result}\")\n",
    "                    result_blocks.append(\n",
    "                        ToolResultBlock(tool_use_id=tc.id, content=result)\n",
    "                    )\n",
    "                messages.append(Message(role=\"tool\", content=result_blocks))\n",
    "\n",
    "            if turn > 5:\n",
    "                break\n",
    "\n",
    "    print(f\"\\n{turn} turns, {total_tokens} tokens\")\n",
    "    expected = (17 * 23) + (31 * 11)\n",
    "    print(f\"Expected answer: {expected}\")\n",
    "\n",
    "await live_agentic_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "metadata": {},
   "source": "---\n## Summary\n\nStep 6 built the **provider registry** — the public API that ties everything together:\n\n```\nregistry.py  ->  load_model(), clear_cache(), _get_adapter_class()\n__init__.py  ->  exports load_model + clear_cache from registry\n```\n\n**Convention-based discovery:**\n```\nload_model(\"anthropic\")           load_model(\"openai\")\n         ↓                                  ↓\nproviders/anthropic.toml          providers/openai.toml\narcllm.adapters.anthropic         arcllm.adapters.openai\nAnthropicAdapter                  OpenaiAdapter\n```\n\n**Key design decisions:**\n- Convention over configuration — file structure is the registry, no mapping dict\n- `provider_name.title() + \"Adapter\"` — predictable class names (hence OpenAI -> Openai rename)\n- Module-level config caching — TOML parsed once, reused across `load_model()` calls\n- `clear_cache()` for test isolation\n- Lazy adapter loading via `importlib` — unused providers never imported\n- Lazy adapter imports in `__init__.py` via `__getattr__` — `import arcllm` doesn't load httpx\n- Provider name regex allows underscores but not hyphens (must be valid Python module names)\n- `load_model()` takes only `provider` and `model` — no `**kwargs` (adapters have fixed constructors)\n- Clear error messages for every failure mode (missing TOML, missing module, missing class, missing key, bad name)\n\n**The agent pattern:**\n```python\nmodel = load_model(\"anthropic\")\nasync with model:\n    resp = await model.invoke(messages, tools)\n```\n\nPhase 1 (Core Foundation) is **complete**. The library has:\n- Types (Step 1)\n- Config (Step 2)\n- Anthropic adapter (Step 3)\n- OpenAI adapter (Step 5)\n- Registry + `load_model()` (Step 6)\n- 139 tests passing"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}