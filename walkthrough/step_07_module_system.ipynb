{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ArcLLM Step 7: Module System + Retry + Fallback\n",
    "\n",
    "This notebook walks through everything built in Step 7 — the **module system** that adds opt-in behaviors (retry, fallback) by wrapping adapters in middleware layers.\n",
    "\n",
    "**What was built:**\n",
    "- `BaseModule` — transparent wrapper foundation all modules inherit from\n",
    "- `RetryModule` — exponential backoff with jitter on transient failures (429, 500, 502, 503, 529, connection errors)\n",
    "- `FallbackModule` — automatic provider chain switching on failure\n",
    "- `load_model()` updated — module kwargs (`retry=True`, `fallback={...}`) enable wrapping\n",
    "\n",
    "**Why it matters:** In production with thousands of concurrent agents, transient failures are guaranteed. Rate limiting (429), server errors (500), network blips — they all happen. Without retry/fallback, every agent crashes. With these modules, agents survive transparently:\n",
    "\n",
    "```python\n",
    "model = load_model(\"anthropic\", retry=True, fallback={\"chain\": [\"openai\"]})\n",
    "resp = await model.invoke(messages)  # retries and falls back automatically\n",
    "```\n",
    "\n",
    "The agent code doesn't change. The modules wrap `invoke()` and handle failures invisibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "from unittest.mock import AsyncMock, MagicMock, patch\n",
    "import httpx\n",
    "\n",
    "from arcllm import (\n",
    "    load_model, clear_cache,\n",
    "    AnthropicAdapter, OpenaiAdapter, BaseAdapter,\n",
    "    ArcLLMAPIError, ArcLLMConfigError,\n",
    "    Message, Tool, LLMResponse, LLMProvider, Usage, StopReason,\n",
    ")\n",
    "from arcllm.modules import BaseModule, RetryModule, FallbackModule\n",
    "\n",
    "print(\"All imports successful — including BaseModule, RetryModule, FallbackModule!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: API keys for adapter construction\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-test-key-for-walkthrough\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-openai-test-key-for-walkthrough\"\n",
    "clear_cache()\n",
    "\n",
    "# Helper: mock inner provider\n",
    "def make_inner(side_effects, name=\"test-provider\"):\n",
    "    \"\"\"Create a mock LLMProvider with specified invoke() side effects.\"\"\"\n",
    "    inner = MagicMock(spec=LLMProvider)\n",
    "    inner.name = name\n",
    "    inner.model_name = \"test-model\"\n",
    "    inner.validate_config.return_value = True\n",
    "    inner.invoke = AsyncMock(side_effect=side_effects)\n",
    "    return inner\n",
    "\n",
    "# Helper: standard OK response\n",
    "OK = LLMResponse(\n",
    "    content=\"Success!\",\n",
    "    usage=Usage(input_tokens=10, output_tokens=5, total_tokens=15),\n",
    "    model=\"test-model\",\n",
    "    stop_reason=\"end_turn\",\n",
    ")\n",
    "\n",
    "# Helper: create API errors\n",
    "def api_error(status_code, provider=\"anthropic\"):\n",
    "    return ArcLLMAPIError(status_code=status_code, body=f\"HTTP {status_code}\", provider=provider)\n",
    "\n",
    "print(\"Helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Module Pattern: Wrapper Classes (Middleware)\n",
    "\n",
    "Each module is a wrapper that implements `LLMProvider` and wraps an inner `LLMProvider`. This is the decorator pattern — modules stack around the adapter.\n",
    "\n",
    "```\n",
    "Agent calls invoke()\n",
    "       ↓\n",
    "RetryModule.invoke()      ← outermost: handles retries\n",
    "       ↓\n",
    "FallbackModule.invoke()   ← middle: handles provider switching\n",
    "       ↓\n",
    "AnthropicAdapter.invoke() ← innermost: actual HTTP call\n",
    "```\n",
    "\n",
    "The agent only sees `LLMProvider.invoke()`. It doesn't know modules exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaseModule: transparent wrapper — delegates everything to inner\n",
    "inner = make_inner([OK])\n",
    "module = BaseModule({}, inner)\n",
    "\n",
    "print(f\"BaseModule is LLMProvider: {isinstance(module, LLMProvider)}\")\n",
    "print(f\"module.name:              {module.name}  (delegates to inner)\")\n",
    "print(f\"module.validate_config(): {module.validate_config()}  (delegates to inner)\")\n",
    "\n",
    "# invoke() passes through\n",
    "messages = [Message(role=\"user\", content=\"hi\")]\n",
    "result = await module.invoke(messages)\n",
    "print(f\"\\ninvoke() result: {result.content!r}  (from inner, unchanged)\")\n",
    "print(f\"inner.invoke called: {inner.invoke.await_count} time(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules stack — each wraps the previous\n",
    "inner = make_inner([OK])\n",
    "fallback = FallbackModule({\"chain\": []}, inner)\n",
    "retry = RetryModule({}, fallback)\n",
    "\n",
    "print(\"Stacking: Retry -> Fallback -> Inner\")\n",
    "print(f\"  retry._inner         = {type(retry._inner).__name__}\")\n",
    "print(f\"  fallback._inner      = {type(fallback._inner).__name__}\")\n",
    "print(f\"  All are LLMProvider: {all(isinstance(x, LLMProvider) for x in [retry, fallback, inner])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. RetryModule — Transient Failure Recovery\n",
    "\n",
    "The `RetryModule` wraps `invoke()` with a retry loop. When a transient error occurs, it waits with exponential backoff + jitter and tries again.\n",
    "\n",
    "### What's retryable?\n",
    "\n",
    "| Error | Retryable? | Why |\n",
    "|-------|------------|-----|\n",
    "| HTTP 429 | Yes | Rate limited — wait and retry |\n",
    "| HTTP 500 | Yes | Server error — transient |\n",
    "| HTTP 502 | Yes | Bad gateway — transient |\n",
    "| HTTP 503 | Yes | Service unavailable — transient |\n",
    "| HTTP 529 | Yes | Anthropic overload — transient |\n",
    "| Connection error | Yes | Network blip |\n",
    "| Timeout | Yes | Server slow |\n",
    "| HTTP 400 | **No** | Bad request — won't fix itself |\n",
    "| HTTP 401 | **No** | Auth error — wrong key |\n",
    "| HTTP 403 | **No** | Forbidden — won't fix itself |\n",
    "| ValueError etc. | **No** | Programming error |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success on first try — no retry needed\n",
    "inner = make_inner([OK])\n",
    "module = RetryModule({\"backoff_base_seconds\": 0.01}, inner)\n",
    "\n",
    "result = await module.invoke(messages)\n",
    "print(f\"First try succeeded: {result.content!r}\")\n",
    "print(f\"Attempts: {inner.invoke.await_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry on 429 (rate limited) — fail once, succeed on retry\n",
    "inner = make_inner([api_error(429), OK])\n",
    "module = RetryModule({\"backoff_base_seconds\": 0.01}, inner)\n",
    "\n",
    "result = await module.invoke(messages)\n",
    "print(f\"429 -> retry -> success: {result.content!r}\")\n",
    "print(f\"Attempts: {inner.invoke.await_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry on all transient codes\n",
    "for code in [429, 500, 502, 503, 529]:\n",
    "    inner = make_inner([api_error(code), OK])\n",
    "    module = RetryModule({\"backoff_base_seconds\": 0.001}, inner)\n",
    "    result = await module.invoke(messages)\n",
    "    print(f\"  HTTP {code} -> retry -> {result.content!r}  (attempts: {inner.invoke.await_count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry on connection errors and timeouts\n",
    "for error in [httpx.ConnectError(\"refused\"), httpx.ReadTimeout(\"timed out\")]:\n",
    "    inner = make_inner([error, OK])\n",
    "    module = RetryModule({\"backoff_base_seconds\": 0.001}, inner)\n",
    "    result = await module.invoke(messages)\n",
    "    print(f\"  {type(error).__name__} -> retry -> {result.content!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO retry on non-transient errors — raised immediately\n",
    "for code in [400, 401, 403]:\n",
    "    inner = make_inner([api_error(code)])\n",
    "    module = RetryModule({\"backoff_base_seconds\": 0.001}, inner)\n",
    "    try:\n",
    "        await module.invoke(messages)\n",
    "    except ArcLLMAPIError as e:\n",
    "        print(f\"  HTTP {code} -> NOT retried (attempts: {inner.invoke.await_count})\")\n",
    "\n",
    "# Non-API errors pass through too\n",
    "inner = make_inner([ValueError(\"bad value\")])\n",
    "module = RetryModule({\"backoff_base_seconds\": 0.001}, inner)\n",
    "try:\n",
    "    await module.invoke(messages)\n",
    "except ValueError:\n",
    "    print(f\"  ValueError -> NOT retried (attempts: {inner.invoke.await_count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Retry Exhaustion — When All Retries Fail\n",
    "\n",
    "After `max_retries` attempts, the **last error** is raised. The original error type is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_retries=3 means 4 attempts total (1 initial + 3 retries)\n",
    "config = {\"max_retries\": 3, \"backoff_base_seconds\": 0.001}\n",
    "inner = make_inner([api_error(429)] * 4)  # all 4 fail\n",
    "module = RetryModule(config, inner)\n",
    "\n",
    "try:\n",
    "    await module.invoke(messages)\n",
    "except ArcLLMAPIError as e:\n",
    "    print(f\"All retries exhausted!\")\n",
    "    print(f\"  Attempts:     {inner.invoke.await_count}\")\n",
    "    print(f\"  Error type:   {type(e).__name__}\")\n",
    "    print(f\"  Status code:  {e.status_code}\")\n",
    "    print(f\"  Provider:     {e.provider}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error type preserved — connection error stays a connection error\n",
    "inner = make_inner([httpx.ConnectError(\"refused\")] * 4)\n",
    "module = RetryModule(config, inner)\n",
    "\n",
    "try:\n",
    "    await module.invoke(messages)\n",
    "except httpx.ConnectError as e:\n",
    "    print(f\"Original error type preserved: {type(e).__name__}\")\n",
    "    print(f\"Agents can catch the specific type for custom handling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exponential Backoff + Jitter\n",
    "\n",
    "The retry wait time uses **exponential backoff with proportional jitter**:\n",
    "\n",
    "```\n",
    "backoff = base_seconds * 2^attempt\n",
    "jitter  = random.uniform(0, backoff)  # proportional to backoff\n",
    "wait    = min(backoff + jitter, max_wait)\n",
    "```\n",
    "\n",
    "**Why jitter?** With thousands of agents hitting the same rate limit, they'd all retry at the exact same time (thundering herd). Jitter spreads them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate exponential backoff (with jitter=0 for clarity)\n",
    "config = {\n",
    "    \"max_retries\": 4,\n",
    "    \"backoff_base_seconds\": 1.0,\n",
    "    \"max_wait_seconds\": 100.0,\n",
    "}\n",
    "inner = make_inner([api_error(429)] * 4 + [OK])\n",
    "module = RetryModule(config, inner)\n",
    "\n",
    "with patch(\"arcllm.modules.retry.asyncio.sleep\", new_callable=AsyncMock) as mock_sleep:\n",
    "    with patch(\"arcllm.modules.retry.random.uniform\", return_value=0.0):  # no jitter\n",
    "        await module.invoke(messages)\n",
    "\n",
    "waits = [call.args[0] for call in mock_sleep.await_args_list]\n",
    "print(\"Exponential backoff (base=1.0, jitter=0):\")\n",
    "for i, wait in enumerate(waits):\n",
    "    formula = f\"1.0 * 2^{i}\"\n",
    "    print(f\"  Attempt {i+1}: wait {wait:.1f}s  ({formula} = {1.0 * 2**i:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backoff capped at max_wait_seconds\n",
    "config = {\n",
    "    \"max_retries\": 3,\n",
    "    \"backoff_base_seconds\": 10.0,\n",
    "    \"max_wait_seconds\": 15.0,  # cap\n",
    "}\n",
    "inner = make_inner([api_error(500)] * 3 + [OK])\n",
    "module = RetryModule(config, inner)\n",
    "\n",
    "with patch(\"arcllm.modules.retry.asyncio.sleep\", new_callable=AsyncMock) as mock_sleep:\n",
    "    with patch(\"arcllm.modules.retry.random.uniform\", return_value=0.0):\n",
    "        await module.invoke(messages)\n",
    "\n",
    "waits = [call.args[0] for call in mock_sleep.await_args_list]\n",
    "print(f\"Capped backoff (base=10, max_wait=15):\")\n",
    "for i, wait in enumerate(waits):\n",
    "    uncapped = 10.0 * 2**i\n",
    "    capped = \"(capped!)\" if uncapped > 15 else \"\"\n",
    "    print(f\"  Attempt {i+1}: wait {wait:.1f}s  (uncapped: {uncapped:.1f}s {capped})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jitter in action — proportional to backoff\n",
    "module2 = RetryModule(\n",
    "    {\"max_retries\": 3, \"backoff_base_seconds\": 1.0, \"max_wait_seconds\": 100.0},\n",
    "    make_inner([api_error(429)] * 3 + [OK]),\n",
    ")\n",
    "\n",
    "# Return 50% of backoff as jitter\n",
    "with patch(\"arcllm.modules.retry.asyncio.sleep\", new_callable=AsyncMock) as mock_sleep:\n",
    "    with patch(\"arcllm.modules.retry.random.uniform\", return_value=0.5):\n",
    "        await module2.invoke(messages)\n",
    "\n",
    "waits = [call.args[0] for call in mock_sleep.await_args_list]\n",
    "print(f\"Jitter = 50% of backoff:\")\n",
    "for i, wait in enumerate(waits):\n",
    "    base = 1.0 * 2**i\n",
    "    print(f\"  Attempt {i+1}: backoff={base:.1f} + jitter=0.5 = {wait:.1f}s\")\n",
    "\n",
    "print(f\"\\nJitter prevents thundering herd with thousands of concurrent agents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Retry-After Header Support\n",
    "\n",
    "When a provider sends a `Retry-After` header (via `ArcLLMAPIError.retry_after`), the module honors it instead of calculating backoff. Still capped at `max_wait_seconds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry-After honored\n",
    "config = {\"max_retries\": 1, \"backoff_base_seconds\": 1.0, \"max_wait_seconds\": 100.0}\n",
    "error_with_retry_after = ArcLLMAPIError(\n",
    "    status_code=429, body=\"rate limited\", provider=\"anthropic\", retry_after=5.0\n",
    ")\n",
    "inner = make_inner([error_with_retry_after, OK])\n",
    "module = RetryModule(config, inner)\n",
    "\n",
    "with patch(\"arcllm.modules.retry.asyncio.sleep\", new_callable=AsyncMock) as mock_sleep:\n",
    "    await module.invoke(messages)\n",
    "\n",
    "actual_wait = mock_sleep.await_args_list[0].args[0]\n",
    "print(f\"Retry-After: 5.0s\")\n",
    "print(f\"Actual wait: {actual_wait}s  (honored the header, not calculated)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry-After capped at max_wait_seconds\n",
    "config = {\"max_retries\": 1, \"backoff_base_seconds\": 1.0, \"max_wait_seconds\": 3.0}\n",
    "error = ArcLLMAPIError(status_code=429, body=\"rate limited\", provider=\"test\", retry_after=10.0)\n",
    "inner = make_inner([error, OK])\n",
    "module = RetryModule(config, inner)\n",
    "\n",
    "with patch(\"arcllm.modules.retry.asyncio.sleep\", new_callable=AsyncMock) as mock_sleep:\n",
    "    await module.invoke(messages)\n",
    "\n",
    "actual = mock_sleep.await_args_list[0].args[0]\n",
    "print(f\"Retry-After: 10.0s, max_wait: 3.0s\")\n",
    "print(f\"Actual wait: {actual}s  (capped at max_wait)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Retry Config Customization\n",
    "\n",
    "Every aspect of retry is configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom max_retries\n",
    "inner = make_inner([api_error(429)] * 5)\n",
    "module = RetryModule({\"max_retries\": 1, \"backoff_base_seconds\": 0.001}, inner)\n",
    "\n",
    "try:\n",
    "    await module.invoke(messages)\n",
    "except ArcLLMAPIError:\n",
    "    print(f\"max_retries=1 -> {inner.invoke.await_count} total attempts (1 initial + 1 retry)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom retryable codes — only retry 503\n",
    "config = {\n",
    "    \"max_retries\": 3,\n",
    "    \"backoff_base_seconds\": 0.001,\n",
    "    \"retryable_status_codes\": [503],  # ONLY 503\n",
    "}\n",
    "\n",
    "# 429 should NOT be retried\n",
    "inner = make_inner([api_error(429)])\n",
    "module = RetryModule(config, inner)\n",
    "try:\n",
    "    await module.invoke(messages)\n",
    "except ArcLLMAPIError:\n",
    "    print(f\"429 with custom codes: {inner.invoke.await_count} attempt (NOT retried)\")\n",
    "\n",
    "# 503 SHOULD be retried\n",
    "inner = make_inner([api_error(503), OK])\n",
    "module = RetryModule(config, inner)\n",
    "result = await module.invoke(messages)\n",
    "print(f\"503 with custom codes: {inner.invoke.await_count} attempts (retried -> success)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config validation — bad values rejected at construction\n",
    "inner = make_inner([OK])\n",
    "\n",
    "for config, expected_error in [\n",
    "    ({\"max_retries\": -1}, \"max_retries must be >= 0\"),\n",
    "    ({\"backoff_base_seconds\": 0}, \"backoff_base_seconds must be > 0\"),\n",
    "    ({\"max_wait_seconds\": 0}, \"max_wait_seconds must be > 0\"),\n",
    "]:\n",
    "    try:\n",
    "        RetryModule(config, inner)\n",
    "    except ArcLLMConfigError as e:\n",
    "        print(f\"  {config} -> REJECTED: {e}\")\n",
    "\n",
    "# max_retries=0 is valid (1 attempt, no retries)\n",
    "module = RetryModule({\"max_retries\": 0}, inner)\n",
    "print(f\"\\n  max_retries=0 -> ALLOWED (1 attempt, no retries)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. FallbackModule — Provider Chain Switching\n",
    "\n",
    "When the primary provider fails (any exception), the `FallbackModule` walks a **chain** of alternative providers, creating each on-demand via `load_model()`.\n",
    "\n",
    "```\n",
    "invoke() -> primary fails\n",
    "         -> try chain[0] (e.g., \"openai\") -> load_model(\"openai\") -> invoke()\n",
    "         -> chain[0] fails\n",
    "         -> try chain[1] (e.g., \"ollama\") -> load_model(\"ollama\") -> invoke()\n",
    "         -> all fail? raise PRIMARY error (not the last fallback error)\n",
    "```\n",
    "\n",
    "**Key:** Fallback adapters are created **on-demand** — no wasted memory for unused providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary succeeds — fallback never touched\n",
    "inner = make_inner([OK])\n",
    "module = FallbackModule({\"chain\": [\"openai\"]}, inner)\n",
    "\n",
    "with patch(\"arcllm.modules.fallback.load_model\") as mock_load:\n",
    "    result = await module.invoke(messages)\n",
    "    print(f\"Primary succeeded: {result.content!r}\")\n",
    "    print(f\"load_model called: {mock_load.call_count} times (fallback not needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary fails -> first fallback succeeds\n",
    "inner = make_inner([api_error(500)])\n",
    "fallback_response = LLMResponse(\n",
    "    content=\"Fallback response from OpenAI!\",\n",
    "    usage=Usage(input_tokens=10, output_tokens=5, total_tokens=15),\n",
    "    model=\"gpt-4o\",\n",
    "    stop_reason=\"end_turn\",\n",
    ")\n",
    "fallback_inner = make_inner([fallback_response])\n",
    "\n",
    "module = FallbackModule({\"chain\": [\"openai\"]}, inner)\n",
    "\n",
    "with patch(\"arcllm.modules.fallback.load_model\", return_value=fallback_inner) as mock_load:\n",
    "    result = await module.invoke(messages)\n",
    "    print(f\"Primary failed (500), fallback succeeded!\")\n",
    "    print(f\"  Response: {result.content!r}\")\n",
    "    print(f\"  Model:    {result.model}\")\n",
    "    print(f\"  load_model called with: {mock_load.call_args_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary fails -> first fallback fails -> second fallback succeeds\n",
    "inner = make_inner([api_error(500)])\n",
    "fallback_1 = make_inner([api_error(503)])\n",
    "fallback_2_response = LLMResponse(\n",
    "    content=\"Third time's the charm!\",\n",
    "    usage=Usage(input_tokens=10, output_tokens=5, total_tokens=15),\n",
    "    model=\"ollama-local\", stop_reason=\"end_turn\",\n",
    ")\n",
    "fallback_2 = make_inner([fallback_2_response])\n",
    "\n",
    "module = FallbackModule({\"chain\": [\"openai\", \"ollama\"]}, inner)\n",
    "\n",
    "with patch(\"arcllm.modules.fallback.load_model\", side_effect=[fallback_1, fallback_2]) as mock_load:\n",
    "    result = await module.invoke(messages)\n",
    "    print(f\"Primary: 500, Fallback 1 (openai): 503, Fallback 2 (ollama): success!\")\n",
    "    print(f\"  Response: {result.content!r}\")\n",
    "    print(f\"  Chain walked: {[call.args[0] for call in mock_load.call_args_list]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All fallbacks fail -> raises PRIMARY error (not the last fallback error)\n",
    "primary_error = ArcLLMAPIError(status_code=500, body=\"primary broke\", provider=\"anthropic\")\n",
    "inner = make_inner([primary_error])\n",
    "fallback_inner = make_inner([api_error(503)])\n",
    "\n",
    "module = FallbackModule({\"chain\": [\"openai\"]}, inner)\n",
    "\n",
    "with patch(\"arcllm.modules.fallback.load_model\", return_value=fallback_inner):\n",
    "    try:\n",
    "        await module.invoke(messages)\n",
    "    except ArcLLMAPIError as e:\n",
    "        print(f\"All fallbacks exhausted!\")\n",
    "        print(f\"  Raised error: HTTP {e.status_code} from {e.provider}\")\n",
    "        print(f\"  Body: {e.body!r}\")\n",
    "        print(f\"  This is the PRIMARY error, not the fallback error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty chain — error passes through immediately\n",
    "inner = make_inner([api_error(500)])\n",
    "module = FallbackModule({\"chain\": []}, inner)\n",
    "\n",
    "try:\n",
    "    await module.invoke(messages)\n",
    "except ArcLLMAPIError as e:\n",
    "    print(f\"Empty chain: error passes through (HTTP {e.status_code})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain length validated — max 10 providers\n",
    "inner = make_inner([OK])\n",
    "try:\n",
    "    FallbackModule({\"chain\": [f\"p{i}\" for i in range(11)]}, inner)\n",
    "except ArcLLMConfigError as e:\n",
    "    print(f\"Chain too long: {e}\")\n",
    "\n",
    "# 10 is allowed\n",
    "module = FallbackModule({\"chain\": [f\"p{i}\" for i in range(10)]}, inner)\n",
    "print(f\"10 providers: allowed (len={len(module._chain)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Fallback Cleanup — Adapters Closed After Use\n",
    "\n",
    "Fallback adapters are created on-demand and **closed after each use** (whether the fallback succeeded or failed). No connection pool leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify fallback adapter closed after success\n",
    "inner = make_inner([api_error(500)])\n",
    "fallback_inner = make_inner([fallback_response])\n",
    "fallback_inner.close = AsyncMock()\n",
    "\n",
    "module = FallbackModule({\"chain\": [\"openai\"]}, inner)\n",
    "\n",
    "with patch(\"arcllm.modules.fallback.load_model\", return_value=fallback_inner):\n",
    "    await module.invoke(messages)\n",
    "\n",
    "print(f\"Fallback adapter close() called: {fallback_inner.close.await_count} time(s)\")\n",
    "print(\"Cleanup happens in a 'finally' block — even on failure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Module Stacking via load_model()\n",
    "\n",
    "The updated `load_model()` reads module config and wraps the adapter automatically.\n",
    "\n",
    "```python\n",
    "# Simple — no modules\n",
    "model = load_model(\"anthropic\")\n",
    "\n",
    "# With retry\n",
    "model = load_model(\"anthropic\", retry=True)\n",
    "\n",
    "# With custom retry config\n",
    "model = load_model(\"anthropic\", retry={\"max_retries\": 5})\n",
    "\n",
    "# With fallback\n",
    "model = load_model(\"anthropic\", fallback={\"chain\": [\"openai\"]})\n",
    "\n",
    "# Both — stacking order: Retry(Fallback(Adapter))\n",
    "model = load_model(\"anthropic\", retry=True, fallback={\"chain\": [\"openai\"]})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cache()\n",
    "\n",
    "# No modules — returns adapter directly\n",
    "model = load_model(\"anthropic\")\n",
    "print(f\"No modules:        {type(model).__name__}\")\n",
    "\n",
    "# retry=True — wraps with RetryModule\n",
    "model = load_model(\"anthropic\", retry=True)\n",
    "print(f\"retry=True:        {type(model).__name__} -> {type(model._inner).__name__}\")\n",
    "\n",
    "# fallback=dict — wraps with FallbackModule\n",
    "model = load_model(\"anthropic\", fallback={\"chain\": [\"openai\"]})\n",
    "print(f\"fallback={{chain}}: {type(model).__name__} -> {type(model._inner).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both modules — stacking order: Retry(Fallback(Adapter))\n",
    "clear_cache()\n",
    "model = load_model(\"anthropic\", retry=True, fallback={\"chain\": [\"openai\"]})\n",
    "\n",
    "print(\"Stacking order (outermost first):\")\n",
    "print(f\"  Outermost: {type(model).__name__}\")\n",
    "print(f\"  Middle:    {type(model._inner).__name__}\")\n",
    "print(f\"  Innermost: {type(model._inner._inner).__name__}\")\n",
    "print(f\"\\nWhy this order?\")\n",
    "print(f\"  Retry wraps Fallback: retry a fallback that already tried alternatives.\")\n",
    "print(f\"  If Anthropic 429 -> retry. If Anthropic 500 -> retry.\")\n",
    "print(f\"  After retries exhausted -> Fallback tries OpenAI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom retry config via dict\n",
    "clear_cache()\n",
    "model = load_model(\"anthropic\", retry={\"max_retries\": 5, \"backoff_base_seconds\": 2.0})\n",
    "\n",
    "print(f\"Custom retry config:\")\n",
    "print(f\"  max_retries:        {model._max_retries}\")\n",
    "print(f\"  backoff_base:       {model._backoff_base}\")\n",
    "print(f\"  max_wait:           {model._max_wait}  (default, not overridden)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Config Resolution Priority\n",
    "\n",
    "Module config follows a 4-level priority chain:\n",
    "\n",
    "```\n",
    "1. kwarg=False  -> DISABLED (overrides everything)\n",
    "2. kwarg={dict} -> ENABLED with custom values (merged over config.toml)\n",
    "3. kwarg=True   -> ENABLED with config.toml defaults\n",
    "4. kwarg=None   -> check config.toml 'enabled' flag\n",
    "```\n",
    "\n",
    "The `config.toml` has defaults for all modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show config.toml module defaults\n",
    "from arcllm.config import load_global_config\n",
    "\n",
    "global_config = load_global_config()\n",
    "print(\"config.toml module settings:\")\n",
    "for name, cfg in global_config.modules.items():\n",
    "    settings = {k: v for k, v in cfg.model_dump().items()}\n",
    "    print(f\"  [{name}]: {settings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwarg=False overrides config.toml enabled=true\n",
    "# (Normally retry is disabled in config.toml, but let's test the logic)\n",
    "clear_cache()\n",
    "\n",
    "# retry=False -> always disabled, even if config.toml says enabled\n",
    "model = load_model(\"anthropic\", retry=False)\n",
    "print(f\"retry=False: {type(model).__name__}  (no RetryModule wrapping)\")\n",
    "\n",
    "# retry=True -> enabled with config.toml defaults\n",
    "model = load_model(\"anthropic\", retry=True)\n",
    "print(f\"retry=True:  {type(model).__name__}  (wrapped with config.toml defaults)\")\n",
    "print(f\"  max_retries from config.toml: {model._max_retries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Agent Code Is Unchanged\n",
    "\n",
    "The whole point of the module pattern — agent code stays exactly the same whether modules are enabled or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def agent_invoke(model, query):\n",
    "    \"\"\"Generic agent code — works with or without modules.\"\"\"\n",
    "    resp = await model.invoke(\n",
    "        [Message(role=\"user\", content=query)],\n",
    "        max_tokens=50, temperature=0.0,\n",
    "    )\n",
    "    return resp\n",
    "\n",
    "# Mock the HTTP layer for demonstration\n",
    "def mock_adapter(model):\n",
    "    \"\"\"Walk the wrapper chain and mock the innermost adapter's client.\"\"\"\n",
    "    target = model\n",
    "    while hasattr(target, '_inner'):\n",
    "        target = target._inner\n",
    "    target._client = AsyncMock()\n",
    "    target._client.post = AsyncMock(return_value=httpx.Response(\n",
    "        200, json={\n",
    "            \"id\": \"msg_test\", \"type\": \"message\", \"role\": \"assistant\",\n",
    "            \"model\": target._model_name,\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Hello from the adapter!\"}],\n",
    "            \"stop_reason\": \"end_turn\",\n",
    "            \"usage\": {\"input_tokens\": 10, \"output_tokens\": 5},\n",
    "        },\n",
    "        request=httpx.Request(\"POST\", \"https://api.anthropic.com/v1/messages\"),\n",
    "    ))\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "# Without modules\n",
    "model_plain = load_model(\"anthropic\")\n",
    "mock_adapter(model_plain)\n",
    "resp = await agent_invoke(model_plain, \"Hello!\")\n",
    "print(f\"Without modules: {resp.content!r}  (type: {type(model_plain).__name__})\")\n",
    "\n",
    "# With retry\n",
    "model_retry = load_model(\"anthropic\", retry=True)\n",
    "mock_adapter(model_retry)\n",
    "resp = await agent_invoke(model_retry, \"Hello!\")\n",
    "print(f\"With retry:      {resp.content!r}  (type: {type(model_retry).__name__})\")\n",
    "\n",
    "# With retry + fallback\n",
    "model_full = load_model(\"anthropic\", retry=True, fallback={\"chain\": [\"openai\"]})\n",
    "mock_adapter(model_full)\n",
    "resp = await agent_invoke(model_full, \"Hello!\")\n",
    "print(f\"Retry+Fallback:  {resp.content!r}  (type: {type(model_full).__name__})\")\n",
    "\n",
    "print(f\"\\nagent_invoke() is IDENTICAL in all three cases.\")\n",
    "print(f\"Modules are invisible to the agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Retry Logging\n",
    "\n",
    "Both modules log their activity — useful for observability without changing agent code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging to see retry/fallback activity\n",
    "import sys\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(logging.Formatter(\"%(name)s [%(levelname)s] %(message)s\"))\n",
    "\n",
    "retry_logger = logging.getLogger(\"arcllm.modules.retry\")\n",
    "retry_logger.addHandler(handler)\n",
    "retry_logger.setLevel(logging.WARNING)\n",
    "\n",
    "fallback_logger = logging.getLogger(\"arcllm.modules.fallback\")\n",
    "fallback_logger.addHandler(handler)\n",
    "fallback_logger.setLevel(logging.WARNING)\n",
    "\n",
    "print(\"Logging configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch retry logging\n",
    "inner = make_inner([api_error(429), api_error(429), OK])\n",
    "module = RetryModule({\"max_retries\": 3, \"backoff_base_seconds\": 0.001}, inner)\n",
    "\n",
    "print(\"Retry with 2 failures then success:\")\n",
    "print(\"-\" * 60)\n",
    "result = await module.invoke(messages)\n",
    "print(\"-\" * 60)\n",
    "print(f\"Result: {result.content!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch retry exhaustion logging\n",
    "inner = make_inner([api_error(500)] * 4)\n",
    "module = RetryModule({\"max_retries\": 3, \"backoff_base_seconds\": 0.001}, inner)\n",
    "\n",
    "print(\"Retry exhaustion:\")\n",
    "print(\"-\" * 60)\n",
    "try:\n",
    "    await module.invoke(messages)\n",
    "except ArcLLMAPIError:\n",
    "    pass\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Clean up loggers\n",
    "retry_logger.removeHandler(handler)\n",
    "fallback_logger.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-48",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Live API — Retry + Fallback in Action\n",
    "\n",
    "Let's use the real Anthropic API with retry enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "clear_cache()\n",
    "\n",
    "print(f\"API key: {os.environ.get('ANTHROPIC_API_KEY', 'NOT SET')[:12]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model with retry=True — production-ready with one kwarg\n",
    "async def live_retry_demo():\n",
    "    model = load_model(\"anthropic\", retry=True)\n",
    "    print(f\"Model type:  {type(model).__name__}\")\n",
    "    print(f\"Inner type:  {type(model._inner).__name__}\")\n",
    "    print(f\"Max retries: {model._max_retries}\")\n",
    "    print(f\"Backoff:     {model._backoff_base}s base\")\n",
    "\n",
    "    async with model:\n",
    "        resp = await model.invoke(\n",
    "            [Message(role=\"user\", content=\"What is 2 + 2? One word.\")],\n",
    "            max_tokens=10, temperature=0.0,\n",
    "        )\n",
    "\n",
    "    print(f\"\\nResponse:    {resp.content!r}\")\n",
    "    print(f\"Model:       {resp.model}\")\n",
    "    print(f\"Tokens:      {resp.usage.total_tokens}\")\n",
    "    print(f\"Stop reason: {resp.stop_reason}\")\n",
    "    print(f\"\\nIf this had gotten a 429, it would have automatically retried.\")\n",
    "\n",
    "await live_retry_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom retry config — more aggressive for critical tasks\n",
    "async def live_custom_retry():\n",
    "    model = load_model(\"anthropic\", retry={\"max_retries\": 5, \"backoff_base_seconds\": 0.5})\n",
    "    print(f\"Custom config: max_retries={model._max_retries}, backoff={model._backoff_base}s\")\n",
    "\n",
    "    async with model:\n",
    "        resp = await model.invoke(\n",
    "            [Message(role=\"user\", content=\"Say 'hello' and nothing else.\")],\n",
    "            max_tokens=10, temperature=0.0,\n",
    "        )\n",
    "\n",
    "    print(f\"Response: {resp.content!r}\")\n",
    "\n",
    "await live_custom_retry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full agentic loop with retry — same pattern as before, just add retry=True\n",
    "async def live_agentic_with_retry():\n",
    "    model = load_model(\"anthropic\", retry=True)\n",
    "\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"calculate\",\n",
    "            description=\"Evaluate a mathematical expression\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"expression\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"expression\"],\n",
    "            },\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    msgs = [Message(role=\"user\", content=\"What is 123 * 456? Use the calculator.\")]\n",
    "    turn = 0\n",
    "\n",
    "    async with model:\n",
    "        while True:\n",
    "            turn += 1\n",
    "            resp = await model.invoke(msgs, tools=tools, max_tokens=200, temperature=0.0)\n",
    "\n",
    "            if resp.stop_reason == \"end_turn\":\n",
    "                print(f\"Turn {turn}: {resp.content}\")\n",
    "                break\n",
    "\n",
    "            if resp.stop_reason == \"tool_use\":\n",
    "                for tc in resp.tool_calls:\n",
    "                    try:\n",
    "                        result = str(eval(tc.arguments[\"expression\"]))\n",
    "                    except Exception:\n",
    "                        result = \"Error\"\n",
    "                    print(f\"Turn {turn}: calculate({tc.arguments['expression']}) = {result}\")\n",
    "                    msgs.append(Message(role=\"assistant\", content=[\n",
    "                        ToolUseBlock(id=tc.id, name=tc.name, arguments=tc.arguments),\n",
    "                    ]))\n",
    "                    msgs.append(Message(role=\"tool\", content=[\n",
    "                        ToolResultBlock(tool_use_id=tc.id, content=result),\n",
    "                    ]))\n",
    "\n",
    "            if turn > 5:\n",
    "                break\n",
    "\n",
    "    expected = 123 * 456\n",
    "    print(f\"\\nExpected: {expected}\")\n",
    "    print(f\"Retry was active throughout — any transient error would have been handled.\")\n",
    "\n",
    "await live_agentic_with_retry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Step 7 built the **module system** and the first two modules:\n",
    "\n",
    "```\n",
    "modules/base.py       ->  BaseModule — transparent wrapper, delegates all calls\n",
    "modules/retry.py      ->  RetryModule — exponential backoff + jitter\n",
    "modules/fallback.py   ->  FallbackModule — provider chain on failure\n",
    "registry.py (updated) ->  load_model() now accepts retry= and fallback= kwargs\n",
    "```\n",
    "\n",
    "**Module stacking:**\n",
    "```\n",
    "load_model(\"anthropic\", retry=True, fallback={\"chain\": [\"openai\"]})\n",
    "  -> RetryModule(FallbackModule(AnthropicAdapter))\n",
    "```\n",
    "\n",
    "**Key design decisions:**\n",
    "- Wrapper pattern (middleware) — each module wraps `invoke()`, composable and testable\n",
    "- Retry: 429/500/502/503/529 + connection errors. Exponential backoff with jitter.\n",
    "- Retry-After header honored when present, capped at max_wait\n",
    "- Fallback: config-driven chain, adapters created on-demand, closed after use\n",
    "- Fallback raises primary error when chain exhausted (not last fallback error)\n",
    "- Config resolution: kwarg=False > kwarg={dict} > kwarg=True > config.toml\n",
    "- Agent code unchanged — `model.invoke()` just works\n",
    "- Circular import solved with lazy import in fallback.py\n",
    "\n",
    "**What's next:** Rate limiter (Step 8), Router (Step 9), then Observability (Steps 10-13)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}